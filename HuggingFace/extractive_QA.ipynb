{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVvf4gJoJxgM",
        "outputId": "60a0127a-af0a-4bd3-b230-49a64152feaf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, transformers, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 evaluate-0.4.3 fsspec-2025.3.0 transformers-4.52.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_s-FFD6vHs5"
      },
      "source": [
        "# Question answering\n",
        "\n",
        "Time to look at question answering! This task comes in many flavors, but the one we’ll focus on in this section is called **extractive question answering**. This involves posing questions about a document and identifying the answers as spans of text in the document itself.\n",
        "\n",
        "### Preparing the data\n",
        "\n",
        "The dataset that is used the most as an academic benchmark for extractive question answering is SQuAD, so that’s the one we’ll use here. There is also a harder SQuAD v2 benchmark, which includes questions that don’t have an answer. As long as your own dataset contains a column for contexts, a column for questions, and a column for answers, you should be able to adapt the steps below.\n",
        "\n",
        "### The SQuAD dataset\n",
        "\n",
        "As usual, we can download and cache the dataset in just one step thanks to load_dataset():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302,
          "referenced_widgets": [
            "65a373e5f84744d9b4b03f546119e788",
            "406e2095a14744e6836e37793febd269",
            "37876f308f494ffabe234fe588b0adbd",
            "5bae6abcf5a34fc9ac90bf4e79ee5996",
            "b89773e7cb2f49b3aa168a1ff7953c34",
            "02c188a577084984bee7edfedc2a32e4",
            "d1daa41dc65d4928a06f3f650bdc5a76",
            "6b47006d9dd24ded8907d5f146e3bef7",
            "a10546b2999446b195dde8ba6d19cda1",
            "2d30c56b90e04b83b3a4e437fcee82e0",
            "d17dce115e4346878dc7a3f6b03336d8",
            "82fc41ea5b504e2ea9192d0e80f7ae68",
            "31cea2d2e3db43cf8684b9f1b765fe82",
            "adb90bdcae434a7fb1498c508f858207",
            "fe9ad9131aee41d893eaca6cf590aa44",
            "cf9feba39aaa43d3b0c3f5213b59fe84",
            "28cdac0e3ea8489eb0836c3e028102a2",
            "965261321e0744328cc5d3db28012356",
            "90f162d5e4564cb18f591f12f7be577f",
            "657696f10bdb4922bb74a5fab27ee21d",
            "08ad6582823345739aa3e8cff88fe4a5",
            "9f46aefe73a04b14b845fe1519e524a2",
            "682e1e434bb846ee92c8c36f0616cb17",
            "5f812a95390d4da8822168b91f6d3153",
            "2d8ef51d0af5405b9a5e97005cedad59",
            "da79c5be5125411ab5275fda884995c2",
            "5593af1be0f5400faa06e10929f12afb",
            "76f84824478b4a66bdd3ab1bbbbdb801",
            "8630a36e815e459b998aab9a31c1b192",
            "86757ed4366d49ab8632fd4455133fcc",
            "4dbbd2bac8b64c30ba5e6f259b65c20e",
            "15d5a8cfe51c4e5d816b20af0a46b633",
            "719d1120231644fdb96bdaa4174a7412",
            "fb1e23fabef84554829e8f25fa76fdb5",
            "75398107c2914578b1c5a6bde4a3c537",
            "6539d500ef24457b937798dbc568894a",
            "4efb8237f47944d297005212599d490c",
            "a90410f21adb40518c80c235c13dd177",
            "a34209082f00434593b73eb09ec45c88",
            "6e107a89cdef47bfa5681bf52d3bca4e",
            "d9bd72e4c2844fb58f6d9e226a4444a1",
            "b783694309a546769832a50e61b8caac",
            "8180b355f8e849acbdb5e17c1c069b25",
            "b22508a61c514ab698348c442b3f8135",
            "f775f8eff767453a8edd9ff0db01ff45",
            "48734f19cb2646e7acf2e02c3b094d8e",
            "9f22d828dfad47f6a4dc1982c65bd9f6",
            "3aa17a74654d483bb7e634ec4ef55e43",
            "a00a1b00abad4d71908d29bb4cdeb33c",
            "5400876eafc64952905c49c57e3dfadf",
            "ff830c96792a4c6084409dcfc8c7f86f",
            "9bccdee2f10347eb9fecdcae287b48be",
            "7925884eeacd4f7e9c8d041937c65a22",
            "b1917dae1f56454eb1ffb5f7a83a7acd",
            "5e6af5ac9bc44c2bb801c9ca58e593e9"
          ]
        },
        "id": "LLQGzM-fvHs7",
        "outputId": "30b9ef05-a44a-4fec-dd00-c4fe3d3946ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65a373e5f84744d9b4b03f546119e788"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82fc41ea5b504e2ea9192d0e80f7ae68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "682e1e434bb846ee92c8c36f0616cb17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb1e23fabef84554829e8f25fa76fdb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f775f8eff767453a8edd9ff0db01ff45"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBMlb18svHs9"
      },
      "source": [
        "We can then have a look at this object to learn more about the SQuAD dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_7sAQFFvHs-",
        "outputId": "c115b5a1-8aca-491f-ca12-447a934f8382"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVcWN76sKMZP",
        "outputId": "ff236a51-6233-4fd6-f7ce-0b84da52ed30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661182',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcNbNg30vHs-"
      },
      "source": [
        "It looks like we have everything we need with the context, question, and answers fields, so let’s print those for the first element of our training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JOEJ4HXvHs_",
        "outputId": "836e256f-8354-4437-c364-f2da18bebe1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Answer:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
          ]
        }
      ],
      "source": [
        "print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n",
        "print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
        "print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2-ZQxKRvHs_"
      },
      "source": [
        "The context and question fields are very straightforward to use. The answers field is a bit trickier as it comports a dictionary with two fields that are both lists. This is the format that will be expected by the squad metric during evaluation; if you are using your own data, you don’t necessarily need to worry about putting the answers in the same format. The text field is rather obvious, and the answer_start field contains the starting character index of each answer in the context.\n",
        "\n",
        "During training, there is only one possible answer. We can double-check this by using the Dataset.filter() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139,
          "referenced_widgets": [
            "bfbff17a0cf147c3a947c2e341fccf66",
            "3751709413c34e129ce760f853531bf7",
            "271f77df40f44d8f902f3e4194e78e3b",
            "f8555a42a82f45c8814474c1c6c46d55",
            "fa3af498521d4388a28ca33320e13850",
            "282ff58b1f12426093f93bd33f4c0ca1",
            "a96146ec10144a7e83f41244914dd1f6",
            "43a9b0551c364cf98bff6036d899ee61",
            "877f307dd18041a3b2d3e4038e4e30a7",
            "03a926d96ed747afb305e44436c3f15f",
            "a8432936d73142a2b5e97105ab3bf065"
          ]
        },
        "id": "3NBdgbDsvHtA",
        "outputId": "1dde938c-bf0b-4f6c-be31-a02b35912939"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfbff17a0cf147c3a947c2e341fccf66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 0\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-6tw3fivHtA"
      },
      "source": [
        "For evaluation, however, there are several possible answers for each sample, which may be the same or different:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlubJrymvHtB",
        "outputId": "cf989f5c-711f-4fea-d64e-033f94c96255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}\n",
            "{'text': ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"], 'answer_start': [403, 355, 355]}\n"
          ]
        }
      ],
      "source": [
        "print(raw_datasets[\"validation\"][0][\"answers\"])\n",
        "print(raw_datasets[\"validation\"][2][\"answers\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqEksNpnvHtB"
      },
      "source": [
        "We won’t dive into the evaluation script as it will all be wrapped up by a 🤗 Datasets metric for us, but the short version is that some of the questions have several possible answers, and this script will _compare a predicted answer to all the acceptable answers and take the best score_. If we take a look at the sample at index 2, for instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_9GcdmbvHtC",
        "outputId": "8e673769-5dee-4f4f-e119-7eb9a6026eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "Where did Super Bowl 50 take place?\n"
          ]
        }
      ],
      "source": [
        "print(raw_datasets[\"validation\"][2][\"context\"])\n",
        "print(raw_datasets[\"validation\"][2][\"question\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmrFflvavHtC"
      },
      "source": [
        "We can see that the answer can indeed be one of the three possibilities we saw before.\n",
        "\n",
        "### Processing the training data\n",
        "\n",
        "Let’s start with preprocessing the training data. The hard part will be to generate labels for the question’s answer, which will be the start and end positions of the tokens corresponding to the answer inside the context.\n",
        "\n",
        "But let’s not get ahead of ourselves. First, we need to convert the text in the input into IDs the model can make sense of, using a tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "3aec7094db3f460e8c5da62f3f3fc8bc",
            "6060eb78cefb459c8f77616a43a7428a",
            "d91c3982c84743f0959b578e8654db69",
            "ba21870606d6446bbb59498aa8f9088b",
            "9cd96489a01645819b904deea5829e61",
            "6388a39b33224db5b70c787355f2f442",
            "34f861d5b8264092aae0d8ea8ee51ed3",
            "74bbad5968774515be196ae802a4888f",
            "726a702176e34051b9f80c02784b1d7b",
            "fc96376b94c24531b37461fee4eed830",
            "ef6651c09e4945ef9400f146a86504b7",
            "e7a2a69777584d3bbe192caacb3e0d37",
            "81b80e794ac84fb9878a2f6ed09e9110",
            "fb478b502058454ab09b0e0979d174d8",
            "2c4d48894af742a29e979da7f5f23d4b",
            "f65bc2393f0849e596e7b837ec70baa1",
            "39641d8afef144b0b0c15f1336fdb0e2",
            "19ead0bf6e8a45c3930247aac6212d84",
            "7e550542f1d94811904a59373c4f5d4a",
            "343600105f0b41848068175ad73cfd51",
            "6351f2540e3b4485bbcaa111ff4209f0",
            "74fb12513db247adad320390c5459f59",
            "ddc4d764e8b845aab2c469e2b776031b",
            "985ad9502aea488e8472e68526852428",
            "68c0c8ee9be94f00a64c38659e509284",
            "ac4e948938cd4e6c9c535504696b54c9",
            "94b589e1d06349fd841b400b6861abe9",
            "b101c336326d435a972fb4359083164a",
            "a399789d593f48a9bfbf07da2b168eeb",
            "d7161af87d0349e8bfadd24396a73de6",
            "0bcacbd0b3e74f48ab0c4ac6bce53c7d",
            "f074cae2b5dd4bebb82307ecb94ad309",
            "569c1d30b8a04afa971cbe37db4941ac",
            "1386d18f274844ee94c62802426f2f0e",
            "303773d2549c4647839af9408ee98b59",
            "f6769fe83424418bad18ed146f9780bc",
            "bab4fdd0535744608fb5316d0c03fa60",
            "a858a3a246db4d2c884630bcef2b5411",
            "b87f45aff6a4458baad8ad1bb7c522ba",
            "4875f916a8214f8a9a6ae818819b0693",
            "52c895d5945e442584acab6f840738f3",
            "522f15663db9454eb6ef348b83507e10",
            "e5416762a2d64c78950a606b19718f12",
            "82a4e810d2834d468cd25bb461e514e1"
          ]
        },
        "id": "vk4honi3vHtD",
        "outputId": "7e877bbf-ecfa-4819-cc6f-9c6808ea39c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3aec7094db3f460e8c5da62f3f3fc8bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7a2a69777584d3bbe192caacb3e0d37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddc4d764e8b845aab2c469e2b776031b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1386d18f274844ee94c62802426f2f0e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVkk_Ck-vHtE"
      },
      "source": [
        "As mentioned previously, we’ll be fine-tuning a BERT model, but you can use any other model type as long as it has a fast tokenizer implemented. You can see all the architectures that come with a fast version in this big table, and to check that the tokenizer object you’re using is indeed backed by 🤗 Tokenizers you can look at its is_fast attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sAH2weGvHtE"
      },
      "outputs": [],
      "source": [
        "tokenizer.is_fast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmYg6kffvHtE"
      },
      "source": [
        "We can pass to our tokenizer the question and the context together, and it will properly insert the special tokens to form a sentence like this:\n",
        "\n",
        "`[CLS] question [SEP] context [SEP]`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can directly call this tokenizer on two sentences (one for the answer, one for the context):"
      ],
      "metadata": {
        "id": "WTeDixCiKhBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8GSRXduKgBU",
        "outputId": "1f095a0a-da26-4bff-ad8a-86f257a79fe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1327, 1110, 1240, 1271, 136, 102, 1422, 1271, 1110, 156, 7777, 2497, 1394, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPM-1qiLvHtF"
      },
      "source": [
        "Let’s double-check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "o0--GLI2vHtF",
        "outputId": "0114bc3b-2fa6-4694-a38c-bad308e1db4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building \\' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "context = raw_datasets[\"train\"][0][\"context\"]\n",
        "question = raw_datasets[\"train\"][0][\"question\"]\n",
        "\n",
        "inputs = tokenizer(question, context)\n",
        "tokenizer.decode(inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URQEQ6ogvHtG"
      },
      "source": [
        "The labels will then be the index of the tokens starting and ending the answer, and the model will be tasked to predicted one start and end logit per token in the input, with the theoretical labels being as follow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0VE__abvHtG"
      },
      "source": [
        "In this case the context is not too long, but some of the examples in the dataset have very long contexts that will exceed the maximum length we set (which is 384 in this case). As we saw in Chapter 6 when we explored the internals of the question-answering pipeline, we will deal with long contexts by creating several training features (sub-examples) from one sample of our dataset, with a sliding window between them.\n",
        "\n",
        "To see how this works using the current example, we can limit the length to 100 and use a sliding window of 50 tokens. As a reminder, we use:\n",
        "\n",
        "max_length to set the maximum length (here 100)\n",
        "truncation=\"only_second\" to truncate the context (which is in the second position) when the question with its context is too long\n",
        "stride to set the number of overlapping tokens between two successive chunks (here 50)\n",
        "return_overflowing_tokens=True to let the tokenizer know we want the overflowing tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O351qfRRvHtH",
        "outputId": "5033d2ff-ccba-491d-deca-425a45caa348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building ' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
            "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
            "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
            "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length=100,\n",
        "    truncation=\"only_second\",\n",
        "    stride=50,\n",
        "    return_overflowing_tokens=True,\n",
        ")\n",
        "\n",
        "for ids in inputs[\"input_ids\"]:\n",
        "    print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R3WIEjHvHtI"
      },
      "source": [
        "As we can see, our example has been in split into four inputs, each of them containing the question and some part of the context. Note that the answer to the question (“Bernadette Soubirous”) only appears in the third and last inputs, so by dealing with long contexts in this way we will create some training examples where the answer is not included in the context. For those examples, the labels will be start_position = end_position = 0 (so we predict the [CLS] token). We will also set those labels in the unfortunate case where the answer has been truncated so that we only have the start (or end) of it. For the examples where the answer is fully in the context, the labels will be the index of the token where the answer starts and the index of the token where the answer ends.\n",
        "\n",
        "The dataset provides us with the start character of the answer in the context, and by adding the length of the answer, we can find the end character in the context. To map those to token indices, we will need to use the offset mappings we studied in Chapter 6. We can have our tokenizer return these by passing along return_offsets_mapping=True:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOiC33CcvHtI",
        "outputId": "311ad564-da8f-411b-b504-ecb63bdd7702"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length=100,\n",
        "    truncation=\"only_second\",\n",
        "    stride=50,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        ")\n",
        "\n",
        "inputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYpSjPH6vHtI"
      },
      "source": [
        "As we can see, we get back the usual input IDs, token type IDs, and attention mask, as well as the offset mapping we required and an extra key, overflow_to_sample_mapping. The corresponding value will be of use to us when we tokenize several texts at the same time (which we should do to benefit from the fact that our tokenizer is backed by Rust). Since one sample can give several features, it maps each feature to the example it originated from. Because here we only tokenized one example, we get a list of 0s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j34SWJ5jvHtJ",
        "outputId": "aef91fe6-2da5-4c4c-aae8-b42ae3b4ef56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "inputs[\"overflow_to_sample_mapping\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZxpEDLpvHtJ"
      },
      "source": [
        "But if we tokenize more examples, this will become more useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA6M6qtJvHtJ",
        "outputId": "8f31fed4-be9d-4384-eafc-9de9ac158dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 4 examples gave 19 features.\n",
            "Here is which example each feature comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].\n",
            "Here is the offset_mapping for the first feature from example 0: [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (0, 0)]\n",
            "Here is which sentence the token comes from in the first feature: question (0) or context (1)?: [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(\n",
        "    raw_datasets[\"train\"][2:6][\"question\"],\n",
        "    raw_datasets[\"train\"][2:6][\"context\"],\n",
        "    max_length=100,\n",
        "    truncation=\"only_second\",\n",
        "    stride=50,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        ")\n",
        "\n",
        "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
        "print(f\"Here is which example each feature comes from: {inputs['overflow_to_sample_mapping']}.\")\n",
        "print(f\"Here is the offset_mapping for the first feature from example {inputs['overflow_to_sample_mapping'][0]}: {inputs['offset_mapping'][0]}\")\n",
        "print(f\"Here is which sentence the token comes from in the first feature: question (0) or context (1)?: {inputs.sequence_ids(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6OYBIMgvHtK"
      },
      "source": [
        "As we can see, the first three examples were broken into 4 fetaures (sub-examples) and the last example broken into 7.\n",
        "\n",
        "This information will be useful when we create labels for features. As mentioned earlier, those labels are the pairs:\n",
        "\n",
        "(0, 0) if the answer is not in the corresponding span of the context, and\n",
        "(start_position, end_position) if the answer is in the corresponding span of the context, with start_position being the index of the token (in the input IDs) at the start of the answer and end_position being the index of the token (in the input IDs) where the answer ends.\n",
        "\n",
        "To determine which of these is the case and, if relevant, the positions of the tokens, we first find the indices that start and end tokens in the context in the input IDs. We could use the token type IDs to do this, but since those do not necessarily exist for all models (DistilBERT does not require them, for instance), we’ll instead use the `sequence_ids()` method of the BatchEncoding our tokenizer returns.\n",
        "\n",
        "Once we have those token indices, we look at the corresponding offsets, which for every token, it is a tuple of two integers indicating the start and the end index of the word in the context corresponding to this token. We can thus detect if the chunk of the context in this sub-example starts after the answer or ends before the answer begins (in which case the label is (0, 0)). If that’s not the case, we loop to find the first and last token of the answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjTUVP_3vHtK",
        "outputId": "edbe36fd-2346-44f8-f4ff-118459c3fdb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n",
              " [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "\n",
        "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
        "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
        "    answer = answers[sample_idx]\n",
        "    start_char = answer[\"answer_start\"][0]\n",
        "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "    sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "    # Find the start and end of the context\n",
        "    idx = 0\n",
        "    while sequence_ids[idx] != 1:\n",
        "        idx += 1\n",
        "    context_start = idx\n",
        "    while sequence_ids[idx] == 1:\n",
        "        idx += 1\n",
        "    context_end = idx - 1\n",
        "\n",
        "    # If the answer is not fully inside the context, label is (0, 0)\n",
        "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "        start_positions.append(0)\n",
        "        end_positions.append(0)\n",
        "    else:\n",
        "        # Otherwise it's the start and end token positions\n",
        "        idx = context_start\n",
        "        while idx <= context_end and offset[idx][0] <= start_char:\n",
        "            idx += 1\n",
        "        start_positions.append(idx - 1)\n",
        "\n",
        "        idx = context_end\n",
        "        while idx >= context_start and offset[idx][1] >= end_char:\n",
        "            idx -= 1\n",
        "        end_positions.append(idx + 1)\n",
        "\n",
        "start_positions, end_positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtaA2VOwvHtL"
      },
      "source": [
        "Let’s take a look at a few results to verify that our approach is correct. For the first feature we find (83, 85) as labels, so let’s compare the theoretical answer with the decoded span of tokens from 83 to 85 (inclusive):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApWP6Q3OvHtL",
        "outputId": "ef297074-0790-441a-af71-02cb755b0346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theoretical answer: the Main Building,\n",
            "Labels give: the Main Building\n"
          ]
        }
      ],
      "source": [
        "idx = 0\n",
        "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
        "# There might be more than one answer for each question-context pair. Take the first answer!\n",
        "answer = answers[sample_idx][\"text\"][0]\n",
        "\n",
        "start = start_positions[idx]\n",
        "end = end_positions[idx]\n",
        "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
        "\n",
        "print(f\"Theoretical answer: {answer},\\nLabels give: {labeled_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Is8mPSvHtL"
      },
      "source": [
        "So that’s a match! Now let’s check index 3 from the same first train example, where we set the labels to (0, 0), which means the answer is not in the context chunk of that feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdOX3WHcvHtM",
        "outputId": "56f8d530-439a-46cc-cd8b-f3d71bc2d6c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theoretical answer: the Main Building, \n",
            " decoded_context give: [CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n",
            "\n",
            "Labels give: [CLS]\n"
          ]
        }
      ],
      "source": [
        "idx = 3\n",
        "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
        "answer = answers[sample_idx][\"text\"][0]\n",
        "\n",
        "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
        "\n",
        "start = start_positions[idx]\n",
        "end = end_positions[idx]\n",
        "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
        "\n",
        "print(f\"Theoretical answer: {answer}, \\n decoded_context give: {decoded_example}\")\n",
        "print(f\"\\nLabels give: {labeled_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgMW9m8svHtM"
      },
      "source": [
        "Indeed, we don’t see the answer inside the context.\n",
        "\n",
        "✏️ Your turn! When using the XLNet architecture, padding is applied on the left and the question and context are switched. Adapt all the code we just saw to the XLNet architecture (and add padding=True). Be aware that the [CLS] token may not be at the 0 position with padding applied.\n",
        "\n",
        "Now that we have seen step by step how to preprocess our training data, we can group it in a function we will apply on the whole training dataset. We’ll pad every feature to the maximum length we set, as most of the contexts will be long (and the corresponding samples will be split into several features), so there is no real benefit to applying dynamic padding here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3wBNAB3RvHtN"
      },
      "outputs": [],
      "source": [
        "max_length = 384\n",
        "stride = 128\n",
        "\n",
        "\n",
        "def preprocess_training_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label is (0, 0)\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-PSKiWHvHtN"
      },
      "source": [
        "Note that we defined two constants to determine the maximum length used as well as the length of the sliding window, and that we added a tiny bit of cleanup before tokenizing: some of the questions in the SQuAD dataset have extra spaces at the beginning and the end that don’t add anything (and take up space when being tokenized if you use a model like RoBERTa), so we removed those extra spaces.\n",
        "\n",
        "To apply this function to the whole training set, we use the Dataset.map() method with the batched=True flag. It’s necessary here as we are changing the length of the dataset (since one example can give several training features):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "0740d6dc334f41c5b97be22df9441c89",
            "bccc095cf7164d3f8ec2f05e553b3b0c",
            "592697c487d14e24bc6dd466bf37d8aa",
            "33e4d08a39784556ba18349bbe95ce18",
            "97ef740b07b9478d859aa37c31488e96",
            "99d56f51ed01401590033f8d68d1bbea",
            "10c1d91f8b9940a2a583d2b94a652072",
            "f9305a64cc1a42b78f65c3c2dd581ad2",
            "11cdbcec06114bf586a84d244ab138ae",
            "86d70f665ef1467c8c6b1f239a197d2b",
            "8480d52b19c34c16af15693834b2bf3a"
          ]
        },
        "id": "GdO543JKvHtO",
        "outputId": "71ba92b8-bf04-4e13-8517-3e9ab34af77d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0740d6dc334f41c5b97be22df9441c89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(87599, 88729)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_dataset = raw_datasets[\"train\"].map(\n",
        "    preprocess_training_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")\n",
        "len(raw_datasets[\"train\"]), len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9UVgYJVuijO",
        "outputId": "2c0eb401-6751-48c3-c783-7ecc9b85e1a7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "    num_rows: 88729\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "\n",
        "Evaluating our model will require a bit more work, as we will need to map the predictions of our model back to parts of the original context. The model itself predicts logits for the start and end positions of our answers. To extract an answer span from the context, two learnable vectors are introduced:\n",
        "- One vector for the start position\n",
        "- One vector for the end position\n",
        "  \n",
        "The model computes a score for each token in the context by taking the dot product of these vectors with the token embeddings:\n",
        "```sh\n",
        "start_scores = dot(H_i, w_start)\n",
        "end_scores = dot(H_i, w_end)\n",
        "```\n",
        "Where:\n",
        "- H_i is the hidden state for token i from BERT.\n",
        "- `w_start` and `w_end` are trainable weights.\n",
        "\n",
        "These scores are turned into probabilities (via softmax). During training (e.g. on SQuAD), the model is trained with cross-entropy loss against the true start and end indices of the answer span.\n",
        "\n",
        "\n",
        "At inference, the best answer is the span with the highest joint start–end score such as sum of logits for start and end which correspond to product of probabilities ($\\log (ab) = \\log a+ \\log b$). For example, the most obvious thing to predict an answer for each feature is to take the index for the maximum of the start logits as a start position and the index of the maximum of the end logits as an end position.\n",
        "\n",
        "```python\n",
        "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)\n",
        "```\n",
        "\n",
        "But what if this prediction gives us something impossible: the start position could be greater than the end position, or point to a span of text in the question instead of the answer. In that case, we might want to look at the second best prediction to see if it gives a possible answer and select that instead. However, picking the second best answer is not as easy as picking the best one: is it the second best index in the start logits with the best index in the end logits? Or the best index in the start logits with the second best index in the end logits? And if that second best answer is not possible either, it gets even trickier for the third best answer.\n",
        "\n",
        "_To classify our answers, we will use the score obtained by adding the start and end logits_. We won't try to order all the possible answers and limit ourselves to with a hyper-parameter we call `n_best_size`. We'll pick the best indices in the start and end logits and gather all the answers this predicts. After checking if each one is valid, we will sort them by their score and keep the best one. Here is how we would do this on the first feature in the batch:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
        "                }\n",
        "            )\n",
        "```\n",
        "And then we can sort the valid_answers according to their score and only keep the best one. The only point left is how to _check a given span is inside the context_ (and not the question) and how to get back the text inside. To do this, we need to add two things to our validation features:\n",
        "\n",
        "- the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
        "- the offset mapping that will give us a map from token indices to character positions in the context.\n",
        "\n",
        "That's why we will re-process the validation set with the following function, slightly different from prepare_train_features:\n"
      ],
      "metadata": {
        "id": "j1FDtJFPrkDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fdFLKE_yvHtP"
      },
      "outputs": [],
      "source": [
        "def preprocess_validation_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i]\n",
        "        example_ids.append(examples[\"id\"][sample_idx])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwIoZqtkvHtQ"
      },
      "source": [
        "We can apply this function on the whole validation dataset like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "b14c82ab5c124313bd2970155035b094",
            "e8750ca888eb4cf58cdbed05550f4630",
            "e46c4026cffd4444b63ad1fca7ce351f",
            "ff95f308eeda4a119c87926aa2f27744",
            "0d96e3f707a742dab852c973274f5f80",
            "f37105dd2b9b4c4a82b9417924edf8c9",
            "498d11f01ae248e7a0734a37e6dae889",
            "2b906f65768d42b193ce4789020e3b30",
            "046758a8876e44138ceae31c9272500b",
            "0a2c83706fbe4f71821f6ce47dab83b5",
            "f28792b70c424fb9a0f34c1821837a93"
          ]
        },
        "id": "KLcTupOZvHtQ",
        "outputId": "d9409081-6faf-4404-9dcc-f5d27a97ed1d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b14c82ab5c124313bd2970155035b094"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10570, 10822)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "validation_dataset = raw_datasets[\"validation\"].map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
        ")\n",
        "len(raw_datasets[\"validation\"]), len(validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r2WXd7ItnTF",
        "outputId": "a65da121-1f35-4021-df1f-20d86b86084b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
              "    num_rows: 10822\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mW8KXs7vHtQ"
      },
      "source": [
        "In this case we’ve only added a couple of hundred samples, so it appears the contexts in the validation dataset are a bit shorter.\n",
        "\n",
        "Now that we have preprocessed all the data, we can get to the training.\n",
        "\n",
        "### Fine-tuning the model with the Trainer API\n",
        "\n",
        "The training code for this example will look a lot like the code in the previous sections — the hardest thing will be to write the compute_metrics() function. Since we padded all the samples to the maximum length we set, there is no data collator to define, so this metric computation is really the only thing we have to worry about. The difficult part will be to post-process the model predictions into spans of text in the original examples; once we have done that, the metric from the 🤗 Datasets library will do most of the work for us.\n",
        "\n",
        "### Post-processing\n",
        "\n",
        "The model will output logits for the start and end positions of the answer in the input IDs, as we saw during our exploration of the question-answering pipeline. The post-processing step will be similar to what we did there, so here’s a quick reminder of the actions we took:\n",
        "\n",
        "We masked the start and end logits corresponding to tokens outside of the context.\n",
        "We then converted the start and end logits into probabilities using a softmax.\n",
        "We attributed a score to each (start_token, end_token) pair by taking the product of the corresponding two probabilities.\n",
        "We looked for the pair with the maximum score that yielded a valid answer (e.g., a start_token lower than end_token).\n",
        "Here we will change this process slightly because we don’t need to compute actual scores (just the predicted answer). This means we can skip the softmax step. To go faster, we also won’t score all the possible (start_token, end_token) pairs, but only the ones corresponding to the highest n_best logits (with n_best=20). Since we will skip the softmax, those scores will be logit scores, and will be obtained by taking the sum of the start and end logits (instead of the product, because of the rule\n",
        "$\\log(ab)=log(a)+log(b)$.\n",
        "\n",
        "To demonstrate all of this, we will need some kind of predictions. Since we have not trained our model yet, we are going to use the default model for the QA pipeline to generate some predictions on a small part of the validation set. We can use the same processing function as before; because it relies on the global constant tokenizer, we just have to change that object to the tokenizer of the model we want to use temporarily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70,
          "referenced_widgets": [
            "74921813fe5648d1865fe5421b9af069",
            "8e1b182a730e4231a5dfa56b04ca8f8b",
            "86ceb78ba7a548bfa8a08be5f4ad8a65",
            "57a0991f48f54f5eb9f79261a79ffdb3",
            "91c4d926354141c1a6f8e3a0aaa87565",
            "d7dfc3a141294ba38fd2e690e6bd33f4",
            "1547333cc3854287a9183a0b818ca479",
            "66f25fb5e4694dfaadcaf8613e469129",
            "8c7ebd5258d644f888590cf072021dbf",
            "7f8ee292f22749d294d115b0c6d1778f",
            "9c8aa1db86dd47afbdf29243b215fa02"
          ]
        },
        "id": "QksIq277vHtR",
        "outputId": "e54848fc-d98d-42c2-95a2-327746935972"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74921813fe5648d1865fe5421b9af069"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "small_eval_set = raw_datasets[\"validation\"].select(range(100))\n",
        "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "eval_set = small_eval_set.map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5epN8RzqvHtR"
      },
      "source": [
        "Now that the preprocessing is done, we change the tokenizer back to the one we originally picked:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1cIvFlKmvHtR"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc6j4BrJvHtS"
      },
      "source": [
        "We then remove the columns of our eval_set that are not expected by the model, build a batch with all of that small validation set, and pass it through the model. If a GPU is available, we use it to go faster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "d82a472f837a4f83907eea456cece0af",
            "ad2fae1fb0b846aa9f50ab545fa5e010",
            "e32964bedd3043008d37e9e4897837e3",
            "efdeea85c7384c7aa13ef9d5e51c777c",
            "9042e3901b70493ca9800cb3534f5903",
            "0a63c820da2d4dda9a49552704be9750",
            "1f3d995df19644c7b72b452b08392fdc",
            "1289f1ec45ea400bbc014a1b71b5a06f",
            "0e3bb28bfdaf48c1a29a5410bf1155f4",
            "03d6089e48e84696bbfc0701d859905c",
            "5debc457f7dd47d499f9accedeeff176"
          ]
        },
        "id": "z5KiEJ-XvHtS",
        "outputId": "c92aac6f-8874-4690-a7e8-5e7e16595d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d82a472f837a4f83907eea456cece0af"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trained_model(**batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T3niNZzvHtS"
      },
      "source": [
        "Since the Trainer will give us predictions as NumPy arrays, we grab the start and end logits and convert them to that format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1HCm0EYCvHtT"
      },
      "outputs": [],
      "source": [
        "start_logits = outputs.start_logits.cpu().numpy()\n",
        "end_logits = outputs.end_logits.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohjC92oLvHtT"
      },
      "source": [
        "We will need a map between examples and their corresponding features. Also, since one example can give several features, we will need to gather together all the answers in all the features generated by a given example, then pick the best one. The following code builds a map from example index to its corresponding features indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "SIqLrwWrvHtT"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "example_to_features = collections.defaultdict(list)\n",
        "for idx, feature in enumerate(eval_set):\n",
        "    example_to_features[feature[\"example_id\"]].append(idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH8RNrBYvHtT"
      },
      "source": [
        "With this in hand, we can really get to work by looping through all the examples and, for each example, through all the associated features. As we said before, we’ll look at the logit scores for the n_best start logits and end logits, excluding positions that give:\n",
        "\n",
        "An answer that wouldn’t be inside the context\n",
        "An answer with negative length\n",
        "An answer that is too long (we limit the possibilities at max_answer_length=30)\n",
        "Once we have all the scored possible answers for one example, we just pick the one with the best logit score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QH9yN4_VvHtU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "n_best = 20\n",
        "max_answer_length = 30\n",
        "predicted_answers = []\n",
        "\n",
        "for example in small_eval_set:\n",
        "    example_id = example[\"id\"]\n",
        "    context = example[\"context\"]\n",
        "    answers = []\n",
        "\n",
        "    for feature_index in example_to_features[example_id]:\n",
        "        start_logit = start_logits[feature_index]\n",
        "        end_logit = end_logits[feature_index]\n",
        "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
        "\n",
        "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        for start_index in start_indexes:\n",
        "            for end_index in end_indexes:\n",
        "                # Skip answers that are not fully in the context\n",
        "                if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                    continue\n",
        "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
        "                if (\n",
        "                    end_index < start_index\n",
        "                    or end_index - start_index + 1 > max_answer_length\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "                answers.append(\n",
        "                    {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ew0zFulvHtU"
      },
      "source": [
        "The final format of the predicted answers is the one that will be expected by the metric we will use. As usual, we can load it with the help of the 🤗 Evaluate library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "d107ad34305e4591ae751e44f710ba09",
            "d1cb6f6c5a8e4172b787164f25fca475",
            "c6edf11bb50547f3bb7efe577ef765dd",
            "ae14d3540530474cafdd8702d376731e",
            "50375939e59b464b8a56a4464873c545",
            "05bd57cf012d41b9b0e2fd1904b00e62",
            "21852295df8e48758bde638c7e2b3259",
            "3c3d672c753d45c99cbd34acd449b235",
            "426d90baaf3c4164b707f912a903b8ba",
            "ffefad4f16154c25929cbef518405746",
            "94aa1b78d14046b6a7293f6ad88140ff",
            "605cb2d855d74fa7a06b70d27629d6bb",
            "64768bd752ba4c769b9c03f886dd9e14",
            "3756ffb5634d4bfa85a45813b37af4df",
            "673359b42ad54d92bd4b5c9a4a77ee84",
            "98befe5f157947f690b9aba42aef7f33",
            "6231abf667a1404d86ff565acfde69c8",
            "37e4f476fa9f42ff9395d676bd827a6f",
            "be9188ac73ee44f79979ddde35e0ca2a",
            "52d196d839cb44d6879ae88bd305d569",
            "7107eca4b4e64255a578e74178c3b2ab",
            "f546ac9add3d4d278825d32056e850c9"
          ]
        },
        "id": "ASMuExOwvHtU",
        "outputId": "3a393fce-d261-4ed1-e276-80d09ab9ba3a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d107ad34305e4591ae751e44f710ba09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "605cb2d855d74fa7a06b70d27629d6bb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKtSFcjivHtV"
      },
      "source": [
        "This metric expects the predicted answers in the format we saw above (a list of dictionaries with one key for the ID of the example and one key for the predicted text) and the theoretical answers in the format below (a list of dictionaries with one key for the ID of the example and one key for the possible answers):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DtwlALOevHtV"
      },
      "outputs": [],
      "source": [
        "theoretical_answers = [\n",
        "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBf5K38DvHtV"
      },
      "source": [
        "We can now check that we get sensible results by looking at the first element of both lists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2w07GdFvHtV",
        "outputId": "cd7f3549-60c2-4bb5-94ab-2bb066e5aee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\n",
            "{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
          ]
        }
      ],
      "source": [
        "print(predicted_answers[0])\n",
        "print(theoretical_answers[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJFwAfRNvHtW"
      },
      "source": [
        "Not too bad! Now let’s have a look at the score the metric gives us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KWmMcaDvHtW",
        "outputId": "906257fb-634a-4d77-ea57-366dc6425800"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 83.0, 'f1': 88.25000000000004}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQP0LznvHtW"
      },
      "source": [
        "Again, that’s rather good considering that according to its paper DistilBERT fine-tuned on SQuAD obtains 79.1 and 86.9 for those scores on the whole dataset.\n",
        "\n",
        "Now let’s put everything we just did in a compute_metrics() function that we will use in the Trainer. Normally, that compute_metrics() function only receives a tuple eval_preds with logits and labels. Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we won’t be able to use this function to get regular evaluation results during training. We will only use it at the end of training to check the results.\n",
        "\n",
        "The compute_metrics() function groups the same steps as before; we just add a small check in case we don’t come up with any valid answers (in which case we predict an empty string)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G8djDQ8DvHtX"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def compute_metrics(start_logits, end_logits, features, examples):\n",
        "    example_to_features = collections.defaultdict(list)\n",
        "    for idx, feature in enumerate(features):\n",
        "        example_to_features[feature[\"example_id\"]].append(idx)\n",
        "\n",
        "    predicted_answers = []\n",
        "    for example in tqdm(examples):\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "\n",
        "        # Loop through all features associated with that example\n",
        "        for feature_index in example_to_features[example_id]:\n",
        "            start_logit = start_logits[feature_index]\n",
        "            end_logit = end_logits[feature_index]\n",
        "            offsets = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Skip answers that are not fully in the context\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    answer = {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                    answers.append(answer)\n",
        "\n",
        "        # Select the answer with the best score\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "            predicted_answers.append(\n",
        "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
        "            )\n",
        "        else:\n",
        "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
        "\n",
        "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
        "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irKEs7k8vHtX"
      },
      "source": [
        "We can check it works on our predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "507bbb5312dc4fd9a129ff1768280f0c",
            "e91ea52e5b314b3db87aff15bc668ea5",
            "fb604a4a22a24d5883e93f0080723715",
            "9df7736eb20141d8b59154f5b169a084",
            "6b26347937a6462e8c699c8ee718a8b1",
            "faa29b58b6c5411694672d8f4d735715",
            "e902b55d267d4a66bd1c45ff04947fe7",
            "fda2d76850b2455496c37cf40d72c8d4",
            "682654b3996e4f18b99987c45dcf3dcc",
            "8c1f4df413c240319a87a942b3a5413b",
            "c5282cafd2cd444aba36c2b09f73e278"
          ]
        },
        "id": "y_ZbxyNBvHtX",
        "outputId": "3d11d963-67c9-4ef4-fbe3-e1523839625c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "507bbb5312dc4fd9a129ff1768280f0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 83.0, 'f1': 88.25000000000004}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNE9x-83vHtY"
      },
      "source": [
        "Looking good! Now let’s use this to fine-tune our model.\n",
        "\n",
        "### Fine-tuning the model\n",
        "\n",
        "We are now ready to train our model. Let’s create it first, using the AutoModelForQuestionAnswering class like before:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Ose8iyvHtY"
      },
      "source": [
        "### A custom training loop\n",
        "\n",
        "Let’s now have a look at the full training loop, so you can easily customize the parts you need. It will look a lot like the training loop in Chapter 3, with the exception of the evaluation loop. We will be able to evaluate the model regularly since we’re not constrained by the Trainer class anymore.\n",
        "\n",
        "#### Preparing everything for training\n",
        "\n",
        "First we need to build the DataLoaders from our datasets. We set the format of those datasets to \"torch\", and remove the columns in the validation set that are not used by the model. Then, we can use the default_data_collator provided by Transformers as a collate_fn and shuffle the training set, but not the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wkZA5xxwvHtY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "train_dataset.set_format(\"torch\")\n",
        "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "validation_set.set_format(\"torch\")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=default_data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    validation_set, collate_fn=default_data_collator, batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kf4titYvHtZ"
      },
      "source": [
        "Next we reinstantiate our model, to make sure we’re not continuing the fine-tuning from before but starting from the BERT pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139,
          "referenced_widgets": [
            "0799849865d04f759140fee566db0d64",
            "8f6d58d233304f51bb595615f26100f8",
            "96849789af3e41e48c18b4ff32f88614",
            "733a710c5bd64fd6841f51be6d4c10a5",
            "3ed88b84017f4adf859e5c7640897c2b",
            "244d17a5451d4e0cba69fda246bc08ea",
            "eca01b6d205441ceb1aa05c3d0cdf9df",
            "970f70a1ab4c4efea0f6217260e2f86e",
            "4f67663e338945f982bbf8d7be731bd0",
            "eb9008d19d00448aaceb2fe4dc15743b",
            "e6925ae89a604f6e957aaac29d4c9a5b"
          ]
        },
        "id": "GXwEsXSivHtZ",
        "outputId": "58ecc630-819b-4ec6-bb25-35a0d611097b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0799849865d04f759140fee566db0d64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJbBPt-YvHta"
      },
      "source": [
        "Then we will need an optimizer. As usual we use the classic AdamW, which is like Adam, but with a fix in the way weight decay is applied:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2F_ZFNtqvHta"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoRhIJuUvHta"
      },
      "source": [
        "Once we have all those objects, we can send them to the accelerator.prepare() method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn’t execute any cell that instantiates an Accelerator. We can force mixed-precision training by passing fp16=True to the Accelerator (or, if you are executing the code as a script, just make sure to fill in the 🤗 Accelerate config appropriately)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NhpK85iJvHtb"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRA5stu7vHtb"
      },
      "source": [
        "As you should know from the previous sections, we can only use the train_dataloader length to compute the number of training steps after it has gone through the accelerator.prepare() method. We use the same linear schedule as in the previous sections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UxcdQQ3bvHtb"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSf95nwkvHtc"
      },
      "source": [
        "To push our model to the Hub, we will need to create a Repository object in a working folder. First log in to the Hugging Face Hub, if you’re not logged in already. We’ll determine the repository name from the model ID we want to give our model (feel free to replace the repo_name with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9CVjnNGvHtc"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"bert-finetuned-squad-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPB0iPldvHtc"
      },
      "source": [
        "Then we can clone that repository in a local folder. If it already exists, this local folder should be a clone of the repository we are working with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pZLhOkvSvHtc"
      },
      "outputs": [],
      "source": [
        "output_dir = \"bert-finetuned-squad-accelerate\"\n",
        "# repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKjDO-otvHtd"
      },
      "source": [
        "We can now upload anything we save in output_dir by calling the repo.push_to_hub() method. This will help us upload the intermediate models at the end of each epoch.\n",
        "\n",
        "### Training loop\n",
        "\n",
        "We are now ready to write the full training loop. After defining a progress bar to follow how training goes, the loop has three parts:\n",
        "\n",
        "The training in itself, which is the classic iteration over the train_dataloader, forward pass through the model, then backward pass and optimizer step.\n",
        "The evaluation, in which we gather all the values for start_logits and end_logits before converting them to NumPy arrays. Once the evaluation loop is finished, we concatenate all the results. Note that we need to truncate because the Accelerator may have added a few samples at the end to ensure we have the same number of examples in each process.\n",
        "Saving and uploading, where we first save the model and the tokenizer, then call repo.push_to_hub(). As we did before, we use the argument blocking=False to tell the 🤗 Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background.\n",
        "Here’s the complete code for the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312,
          "referenced_widgets": [
            "0baed903505c457ebec7c74b04ceb60f",
            "212a2e2b425740e0bf045415510702d5",
            "450ff730b29348c48dff0ec21b89da21",
            "7c5f4fb727104c928ffe38b280bb0f64",
            "291f2e1a6a2d47339f199f09f9343c9a",
            "05940b4029fa49319bbb67993603daa8",
            "3272563cd56b4d7484c171c03a1fa57b",
            "ceff42f6227f4ab98056d38f53faf8b5",
            "b100d033fadc45369f6b4b258730997e",
            "29025554facc4b43a38356b1b8b90c35",
            "5733646378984b79959f8f2198554df7",
            "03bd15c5b1dd4e3caca248129663148e",
            "b8721045126444a7b8e2e9d310cee018",
            "a80a9dc22a634108abb3f5dfa1f9d39e",
            "f1eb2d54f08b42deb80a5b96c4010cc6",
            "d3a3567dc9764a64b07c360483444d2c",
            "84a3076be35f443d93ff8c213b7262bf",
            "9a2bdb4b02b549f7af54c5467f66ff09",
            "c716c351b2ca4027869d493157c2fcc8",
            "c8e1ef0408d44c5a984421a89d6f474b",
            "174cc32568ff494081e5b1121bcb5713",
            "3f30a54fe5974ee2b2069bb5f93ccb2d"
          ]
        },
        "id": "eGi6ZbsWvHtd",
        "outputId": "d0ed6da9-0988-4c9b-a49c-768555bc8848"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/33276 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0baed903505c457ebec7c74b04ceb60f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1353 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03bd15c5b1dd4e3caca248129663148e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7d39f9f5b0d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mend_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "num_train_epochs = 1\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    start_logits = []\n",
        "    end_logits = []\n",
        "    accelerator.print(\"Evaluation!\")\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
        "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
        "\n",
        "    start_logits = np.concatenate(start_logits)\n",
        "    end_logits = np.concatenate(end_logits)\n",
        "    start_logits = start_logits[: len(validation_dataset)]\n",
        "    end_logits = end_logits[: len(validation_dataset)]\n",
        "\n",
        "    metrics = compute_metrics(\n",
        "        start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]\n",
        "    )\n",
        "    print(f\"epoch {epoch}:\", metrics)\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        # repo.push_to_hub(\n",
        "        #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        # )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqrCOjBMvHte"
      },
      "source": [
        "In case this is the first time you’re seeing a model saved with 🤗 Accelerate, let’s take a moment to inspect the three lines of code that go with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEhphs2xvHte"
      },
      "outputs": [],
      "source": [
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM7PrWHPvHte"
      },
      "source": [
        "The first line is self-explanatory: it tells all the processes to wait until everyone is at that stage before continuing. This is to make sure we have the same model in every process before saving. Then we grab the unwrapped_model, which is the base model we defined. The accelerator.prepare() method changes the model to work in distributed training, so it won’t have the save_pretrained() method anymore; the accelerator.unwrap_model() method undoes that step. Lastly, we call save_pretrained() but tell that method to use accelerator.save() instead of torch.save().\n",
        "\n",
        "Once this is done, you should have a model that produces results pretty similar to the one trained with the Trainer. You can check the model we trained using this code at huggingface-course/bert-finetuned-squad-accelerate. And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above!\n",
        "\n",
        "Using the fine-tuned model\n",
        "\n",
        "We’ve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a pipeline, you just have to specify the model identifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "d1af9ac0c7b84ff79c3cf678da4e6036",
            "1059b0786110480591833055d0e3aa93",
            "230a04d2c03c4ad399351c48abb57b30",
            "6a9ad3f51f4b40759736f61941c48959",
            "8c91a089ba334f28ad94b7a0bff133bc",
            "21e1601ee80843efbd979a843a76a40a",
            "f0a493cc9700402d8dc73f730c16c16d",
            "4189d60ed87e40abab467fadae785fc4",
            "fa27474a44f94f2ebdd459f6793a1438",
            "862490adf5a54f5fbd04d673dd9719b6",
            "577f23bc60ea41c09a576f916b79fdb8",
            "1ac4f5a0a7794bec8b8213bcc39ac276",
            "865f21dc0b45402e811292c162d62cfd",
            "889c5419aba74e3794155802bfc66062",
            "751e2a7390c54c3683f08ea406c9fedd",
            "b066f5dc91614de889cfd910caa045c4",
            "ae617ea6f382466bb31409acc2002f0c",
            "4bd9a9401a4c4670bc29e39a0d3747b8",
            "95e1ca0b318a4ac8816f2fe6a5d4edf2",
            "565a0a422c0e4a45b5fb6da6b41d44c1",
            "59e29b9f79e34d1aace0aac2cfe9cfc0",
            "c2164fcddc244a4d8f97c215498df4cd",
            "f9fe005ef10e4f1995155f42158df4e1",
            "f2d823f32eaa4112aaf40d3385d5fb30",
            "75828be42dd049a195437b4acb00acee",
            "fadb96fb473d42ec864b9c7e5a0a49d5",
            "f012117b6a4e41f4a5c2058f6b67ed5d",
            "c39df3f8580c4447bc9e7cc900b0326c",
            "facd433b032d44d7aeb15a9f2e640944",
            "f7e867d26b4141a593e1716184b797af",
            "8272b12bb37a47cab69cd45272660b6a",
            "58a6e16af5dc4dc6b6e96980a9cf29c2",
            "9e6afa008fe641ba9d9952d151168017",
            "a4108bee2ac349de9253214e27e6d8c0",
            "3b1651a377ec4640b733d272f8502122",
            "240c98de74814c8b90e95e37754d3542",
            "07b0dcc6960d40738ecf55769e17cfd5",
            "ece74a020bfb4df3a6c2ba9444fdc012",
            "63276f30756844feaf5f81074a80cb91",
            "055d27fc5c9348ec8dd8bca7297a2750",
            "e36754f3afe6454789786fd544c319fa",
            "edb67e7aabaa4b23ac4554a4d84a1893",
            "edf65bec3886499da11e5c80ea8e9da5",
            "3a503a30e5b24f7c9d8e33c15adf3bd7",
            "556f396c458443e8bc784fcecaecbeb1",
            "5d1e352c0958432cb2ee2e83bbb6eb5c",
            "5c4c766ee5bd4b6c9880b341f5c9573f",
            "7861721024bd4e4b9d169578620c046c",
            "6e111a639c7c48519552f5f91c3dd0ce",
            "de1c94b0152b469b82dfec7f4632e216",
            "934ed3964a6e433aae6be57f464e4453",
            "b198a09369014569bb51e0a9be41df15",
            "e7736954e3834ca48f85e260f17131fe",
            "782250157072410ebbd8f123efa234bd",
            "4728da1a6a494c70b1d79ac71c8836c9",
            "fa48ae37ecbb453db02afed798444559",
            "c305dd8e437844d0a0ae942a27e8d856",
            "6cc6b51026544277b32408de34a50a98",
            "d96fb22b9ff24cca9fc801421bd5ee77",
            "21e0ad969e954ca1a282ab87356a9c8c",
            "77878af314114424814dacc4b27ed08e",
            "335cdd35a7f546bab577397b02a1eb8e",
            "a7830e752a6c4f12aa3b3b490218724a",
            "c35e1eb7fc3e4843a22e89589da112f7",
            "574327ed84314de998564fcb66bb2e5c",
            "b3edac69845944438596da889d0441cb",
            "ef273ee092644b84b2b8e4efa71a0921",
            "274b075977d245b7a8cd91f0fa0aa5a6",
            "205cb656c9cd48169f4a700e688a80a2",
            "8fcc55d61a954fae94145176c903ce5d",
            "5e171a2c450f47869fd6fcf7a999840a",
            "7d8c80a865da47519a0afdc9daaf4e46",
            "2ba68644d09046998aad82bb07940d18",
            "665cd7998a804903b02a6edc2353456a",
            "bdc33770227e4a77959a86e658c14b9c",
            "f994347f115b441e8c9be846db22a064",
            "ac3b95a5a9f64c5b938121edfaeb7ef9"
          ]
        },
        "id": "vDE-PZsJvHte",
        "outputId": "9edfa947-3b3f-4456-d93a-87ed4f3cd5d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1af9ac0c7b84ff79c3cf678da4e6036"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/431M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ac4f5a0a7794bec8b8213bcc39ac276"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9fe005ef10e4f1995155f42158df4e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4108bee2ac349de9253214e27e6d8c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "556f396c458443e8bc784fcecaecbeb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa48ae37ecbb453db02afed798444559"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef273ee092644b84b2b8e4efa71a0921"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.9978997707366943,\n",
              " 'start': 78,\n",
              " 'end': 105,\n",
              " 'answer': 'Jax, PyTorch and TensorFlow'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Replace this with your own checkpoint\n",
        "model_checkpoint = \"huggingface-course/bert-finetuned-squad\"\n",
        "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
        "\n",
        "context = \"\"\"\n",
        "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
        "question_answerer(question=question, context=context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hg98xDZvHtf"
      },
      "source": [
        "Great! Our model is working as well as the default one for this pipeline!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
