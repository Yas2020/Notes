<!DOCTYPE html><html><head>
      <title>kubernetes</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css">
      
      
      
      
      
      <style>
        body::after {
          content: "Created by Yas2020";
          position: fixed;
          bottom: 10px;
          right: 10px;
          font-size: 12px;
          color: gray;
          opacity: 0.7;
          pointer-events: none;
        }
        /* CSS to fix the button at the top-right corner */
        #backToHomeBtn {
            position: fixed;
            top: 20px;       /* Distance from the top */
            right: 20px;     /* Distance from the right */
            padding: 10px 20px;
            background-color: #4c74af; /* Green background */
            color: white;
            border: none;
            font-size: 16px;
            cursor: pointer;
            border-radius: 5px;
            z-index: 1000;  /* Ensures button stays on top */
        }

        #backToHomeBtn:hover {
            background-color: #45a049; /* Darker green on hover */
        }
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
/* Other interesting fonts are: Noto Sans, Roboto, Fira Sans, Sixtyfour*/
@font-face {
  font-family: 'Tangerine';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/tangerine/v17/IurY6Y5j_oScZZow4VOxCZZJ.ttf) format('truetype');
}
@font-face {
  font-family: 'Ubuntu Mono';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/ubuntumono/v17/KFOjCneDtsqEr0keqCMhbCc6CsE.ttf) format('truetype');
}
@font-face {
  font-family: 'Open Sans';
  font-style: normal;
  font-weight: 400;
  font-stretch: normal;
  src: url(https://fonts.gstatic.com/s/opensans/v43/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjZ0B4gaVc.ttf) format('truetype');
}
@font-face {
  font-family: 'Kode Mono';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/kodemono/v2/A2BLn5pb0QgtVEPFnlYkkaoBgw4qv9odq5myxD2ZbA.ttf) format('truetype');
}
@font-face {
  font-family: 'Cutive Mono';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/cutivemono/v21/m8JWjfRfY7WVjVi2E-K9H6RCTms.ttf) format('truetype');
}
.markdown-preview.markdown-preview {
  font-family: UbuntuMono, Open Sans, Fira Sans, serif;
  font-size: 16px;
}
.markdown-preview.markdown-preview.prince {
  font-family: UbuntuMono, Open Sans, Fira Sans, serif;
  line-height: 1.4;
  font-size: 10pt;
  color: #000;
}
@page {
  size: A4 portrait;
  margin-top: 0.5in;
  padding-top: 20px;
  color: black;
  margin-bottom: 1.5in;
  @bottom-center {
    content: "Page " counter(page) " of " counter(pages);
    font-size: 8pt;
    vertical-align: top;
    padding-top: 0.5in;
    font-family: UbuntuMono, Open Sans, Fira Sans, serif;
  }
  @top-center {
    content: "Yas Eftekhari - April 2025";
    text-align: left;
    border-bottom: 1px solid black;
    font-size: 8pt;
    font-weight: bold;
    margin: 0px, 50px;
    margin-left: inherit;
    margin-right: inherit;
    padding: 25px 0 5px 0;
    font-family: UbuntuMono, Open Sans, Fira Sans, serif;
  }
}
@page :first {
  counter-reset: page;
  /* Reset page numbering on first page */
}
.markdown-preview.markdown-preview p:not(:has(img)) {
  text-align: justify;
  page-break-inside: auto;
}
.markdown-preview.markdown-preview p img {
  margin-bottom: 5px;
  margin-top: 5px;
}
.markdown-preview.markdown-preview .title {
  font-size: 100pt;
  font-family: 'Tangerine';
  color: #305d8a;
  text-shadow: 3px 3px 0 #b08888;
  display: flex;
  justify-content: center;
  align-items: center;
  text-align: center;
  font-weight: bold;
  page-break-after: always;
  /* Ensure title is on its own page */
}
.markdown-preview.markdown-preview h1 {
  font-size: 30pt;
  color: #1a4fcc;
  border-bottom-width: 1px;
}
.markdown-preview.markdown-preview h2 {
  font-weight: 700;
  color: #3f77e0;
}
.markdown-preview.markdown-preview h3 {
  font-weight: 700;
  color: #6da2f2;
}
.markdown-preview.markdown-preview h4 {
  font-weight: 700;
  color: #a9c8f8;
}
.markdown-preview.markdown-preview ul li {
  /* list-style-type: ; */
  margin-top: 5px;
  margin-bottom: 5px;
  margin-left: 10px;
  text-align: justify;
}
.markdown-preview.markdown-preview ol {
  list-style-type: upper-roman;
  /* or decimal, lower-alpha, upper-alpha, etc. */
  list-style-position: outside;
  /* or inside */
}
.markdown-preview.markdown-preview table {
  justify-items: center;
}
.markdown-preview.markdown-preview table > thead > tr > th {
  text-align: center;
  border-bottom: 1px solid;
}
.markdown-preview.markdown-preview table > thead > tr > th,
.markdown-preview.markdown-preview table > thead > tr > td,
.markdown-preview.markdown-preview table > tbody > tr > th,
.markdown-preview.markdown-preview table > tbody > tr > td {
  padding: 5px 10px;
}
.markdown-preview.markdown-preview table > tbody > tr + tr > td {
  border-top: 1px solid;
}
.markdown-preview.markdown-preview th {
  background-color: #0430aa;
  color: white;
}
.markdown-preview.markdown-preview th,
.markdown-preview.markdown-preview td {
  border-bottom: 1px solid #ddd;
}
.markdown-preview.markdown-preview th,
.markdown-preview.markdown-preview td {
  padding: 10px;
  text-align: center;
}
.markdown-preview.markdown-preview blockquote {
  background: rgba(127, 127, 127, 0.1);
  border-color: rgba(0, 122, 204, 0.5);
  margin: 0 7px 0 5px;
  padding: 0 16px 0 10px;
  border-left-width: 5px;
  border-left-style: solid;
}
.markdown-preview.markdown-preview code {
  font-family: Monospace, Menlo, Monaco, Consolas;
  font-size: 12pt;
  color: #c15211;
}
.markdown-preview.markdown-preview pre.language-python.python code {
  font-family: Monospace, Menlo, Monaco, "Droid Sans Fallback";
  font-size: 12pt;
  line-height: 1.357em;
  color: #14c111;
  white-space: pre-wrap;
  word-wrap: break-word;
  overflow-wrap: break-word;
}
.markdown-preview.markdown-preview pre.language-text {
  background-color: #f7f7f5;
}
.markdown-preview.markdown-preview pre.language-text code {
  font-family: Monospace, Menlo, Monaco, "Droid Sans Fallback";
  font-size: 10pt;
  line-height: 1.357em;
  color: #14c111;
  white-space: pre-wrap;
  word-wrap: break-word;
  overflow-wrap: break-word;
  border-radius: 10px;
}
.markdown-preview.markdown-preview pre.language-o.o {
  background-color: #f7f7f5;
  margin-top: -25px;
}
.markdown-preview.markdown-preview pre.language-o.o code {
  font-family: Cutive Mono, Kode Mono, Monaco, "Droid Sans Fallback";
  font-size: 11pt;
  font-weight: 800;
  line-height: 1.357em;
  color: #93a293;
  white-space: pre-wrap;
  word-wrap: break-word;
  overflow-wrap: break-word;
}
@media screen and (max-width: 960px) {
  .markdown-preview.markdown-preview h1 {
    font-size: 30px;
  }
  .markdown-preview.markdown-preview h1.title {
    font-size: 100px;
  }
  .markdown-preview.markdown-preview button {
    font-size: 10px;
  }
}

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h1 class="title">Kubernetes</h1>
<h3 id="table-of-contents">Table of Contents </h3>
<ul>
<li><a href="#kubernetes">Kubernetes</a>
<ul>
<li><a href="#kubernetes-architecture">Kubernetes Architecture</a>
<ul>
<li><a href="#control-plane">Control Plane</a>
<ul>
<li><a href="#api-server">API Server</a></li>
<li><a href="#scheduler">Scheduler</a></li>
<li><a href="#controller-managers">Controller Managers</a></li>
<li><a href="#key-value-data-store">Key-Value Data Store</a></li>
</ul>
</li>
<li><a href="#data-plane">Data Plane</a>
<ul>
<li><a href="#container-runtime">Container Runtime</a></li>
<li><a href="#node-agent---kubelet">Node Agent - kubelet</a></li>
<li><a href="#proxy-kube-proxy">Proxy-Kube-proxy</a></li>
<li><a href="#add-ons">Add-ons</a></li>
</ul>
</li>
<li><a href="#networking-challenges">Networking Challenges</a></li>
<li><a href="#kubeadm">kubeadm</a>
<ul>
<li><a href="#kubectl">kubectl</a></li>
<li><a href="#namespaces">Namespaces</a></li>
</ul>
</li>
<li><a href="#pods">Pods</a></li>
<li><a href="#labels">Labels</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#service">Service</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#authorization">Authorization</a>
<ul>
<li><a href="#node">Node</a></li>
<li><a href="#webhook">Webhook</a></li>
<li><a href="#role-based-access-control-rbac">Role-Based Access Control (RBAC)</a></li>
<li><a href="#example-of-user-autherization-using-rbac">Example of User Autherization using RBAC</a></li>
<li><a href="#serviceaccount-permissions">ServiceAccount Permissions</a></li>
<li><a href="#use-cases-for-kubernetes-service-accounts">Use cases for Kubernetes Service Accounts</a>
<ul>
<li><a href="#grant-permissions-to-service-accounts">Grant Permissions to Service Accounts</a></li>
</ul>
</li>
<li><a href="#admission-control">Admission Control</a></li>
</ul>
</li>
<li><a href="#services">Services</a>
<ul>
<li><a href="#connecting-users-or-applications-to-pods">Connecting Users or Applications to Pods</a></li>
<li><a href="#kube-proxy">kube-proxy</a></li>
<li><a href="#traffic-policies">Traffic Policies</a>
<ul>
<li><a href="#servicetype-clusterip-and-nodeport">ServiceType: ClusterIP and NodePort</a></li>
<li><a href="#servicetype-loadbalancer">ServiceType: LoadBalancer</a></li>
</ul>
</li>
<li><a href="#liveness-and-readiness-probes">Liveness and Readiness Probes</a>
<ul>
<li><a href="#liveness">Liveness</a></li>
<li><a href="#liveness-http-request">Liveness HTTP Request</a></li>
<li><a href="#tcp-liveness-probe">TCP Liveness Probe</a></li>
<li><a href="#readiness-probes">Readiness Probes</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#kubernetes-volume-management">Kubernetes Volume Management</a>
<ul>
<li><a href="#persistentvolumes">PersistentVolumes</a>
<ul>
<li><a href="#persistent-volume-claims">Persistent Volume Claims</a></li>
<li><a href="#container-storage-interface-csi">Container Storage Interface (CSI)</a>
<ul>
<li><a href="#using-a-shared-hostpath-volume-type-demo-guide">Using a Shared hostPath Volume Type Demo Guide</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#configmaps-and-secrets">ConfigMaps and Secrets</a>
<ul>
<li><a href="#use-configmaps-inside-pods-as-environment-variables">Use ConfigMaps Inside Pods: As Environment Variables</a>
<ul>
<li><a href="#using-configmaps-as-volumes-demo-guide">Using ConfigMaps as Volumes Demo Guide</a></li>
</ul>
</li>
<li><a href="#secrets">Secrets</a>
<ul>
<li><a href="#use-secrets-inside-pods-as-environment-variables">Use Secrets Inside Pods: As Environment Variables</a></li>
<li><a href="#use-secrets-inside-pods-as-volumes">Use Secrets Inside Pods: As Volumes</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#ingress">Ingress</a>
<ul>
<li><a href="#ingress-controller">Ingress Controller</a>
<ul>
<li><a href="#deploy-an-ingress-resource">Deploy an Ingress Resource</a></li>
<li><a href="#access-services-using-ingress">Access Services Using Ingress</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#advanced-topics">Advanced Topics</a>
<ul>
<li><a href="#annotations">Annotations</a></li>
<li><a href="#quota-and-limits-management">Quota and Limits Management</a></li>
<li><a href="#autoscaling">Autoscaling</a></li>
<li><a href="#jobs-and-cronjobs">Jobs and CronJobs</a></li>
<li><a href="#statefulsets">StatefulSets</a></li>
<li><a href="#network-policies">Network Policies</a></li>
<li><a href="#monitoring-logging-and-troubleshooting">Monitoring, Logging, and Troubleshooting</a></li>
<li><a href="#helm">Helm</a></li>
<li><a href="#service-mesh">Service Mesh</a></li>
<li><a href="#application-deployment-strategies">Application Deployment Strategies</a></li>
</ul>
</li>
<li><a href="#introduction-to-istio">Introduction to Istio</a>
<ul>
<li><a href="#the-shift-to-cloud-native-applications">The Shift to Cloud-Native Applications</a></li>
<li><a href="#new-problems">New Problems</a></li>
<li><a href="#early-solutions">Early Solutions</a></li>
<li><a href="#service-meshes">Service Meshes</a>
<ul>
<li><a href="#features-of-istio-service-mesh">Features of Istio Service Mesh</a></li>
<li><a href="#security">Security</a></li>
<li><a href="#service-discovery">Service Discovery</a></li>
<li><a href="#traffic-management">Traffic Management</a></li>
<li><a href="#resilience">Resilience</a></li>
<li><a href="#observability">Observability</a>
<ul>
<li><a href="#advanced-deployment">Advanced Deployment</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#istio-architecture">Istio Architecture</a>
<ul>
<li><a href="#how-does-istio-work">How Does Istio Work?</a></li>
<li><a href="#sidecar-injection">Sidecar Injection</a>
<ul>
<li><a href="#routing-application-traffic-through-the-sidecar">Routing Application Traffic Through the Sidecar</a></li>
</ul>
</li>
<li><a href="#install-istio---commmand-line">Install Istio - Commmand Line</a></li>
</ul>
</li>
<li><a href="#observability-1">Observability</a>
<ul>
<li><a href="#how-service-meshes-simplify-and-improve-observability">How Service Meshes Simplify and Improve Observability</a></li>
<li><a href="#how-istio-exposes-workload-metrics-1">How Istio Exposes Workload Metrics (1)</a></li>
<li><a href="#grafana-dashboards-for-istio-1">Grafana Dashboards for Istio (1)</a></li>
<li><a href="#distributed-tracing">Distributed Tracing</a>
<ul>
<li><a href="#terms">Terms</a></li>
</ul>
</li>
<li><a href="#deploy-jaeger-and-review-some-traces">Deploy Jaeger and Review Some Traces</a></li>
</ul>
</li>
<li><a href="#traffic-management-1">Traffic Management</a>
<ul>
<li><a href="#gateways">Gateways</a></li>
<li><a href="#traffic-routing-in-istio">Traffic Routing in Istio</a>
<ul>
<li><a href="#where-to-route-the-traffic">Where to Route the Traffic?</a></li>
</ul>
</li>
<li><a href="#advanced-traffic-routing">Advanced Traffic Routing</a></li>
<li><a href="#rewriting-and-redirecting-traffic">Rewriting and Redirecting Traffic</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#security-1">Security</a>
<ul>
<li><a href="#access-control">Access Control</a>
<ul>
<li><a href="#authentication-authn">Authentication (authn)</a>
<ul>
<li><a href="#mutual-tls">Mutual TLS</a>
<ul>
<li><a href="#provisioning-identities-at-runtime">Provisioning Identities at Runtime</a></li>
<li><a href="#certificate-issuance-flow-in-istio">Certificate issuance flow in Istio</a></li>
<li><a href="#inbound-traffic-to-the-proxy">Inbound Traffic to the Proxy</a></li>
<li><a href="#outbound-traffic-from-the-proxy">Outbound Traffic from the Proxy</a></li>
</ul>
</li>
<li><a href="#gateways-and-tls">Gateways and TLS</a></li>
</ul>
</li>
<li><a href="#mutual-tls---lab">Mutual TLS - Lab</a>
<ul>
<li><a href="#disabling-sidecar-injection">Disabling Sidecar Injection</a></li>
<li><a href="#permissive-mode-in-action">Permissive Mode in Action</a></li>
<li><a href="#observing-the-permissive-mode-in-kiali">Observing the Permissive Mode in Kiali</a></li>
<li><a href="#exposing-the-customers-service">Exposing the "customers" Service</a></li>
<li><a href="#generating-traffic-to-the-customers-service">Generating Traffic to the "customers" Service</a></li>
<li><a href="#observing-the-traffic-in-kiali">Observing the Traffic in Kiali</a></li>
<li><a href="#enabling-strict-mode">Enabling "STRICT" mode</a>
<ul>
<li><a href="#cleanup">Cleanup</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#authenticating-users">Authenticating Users</a>
<ul>
<li><a href="#authorization-authz">Authorization (authz)</a>
<ul>
<li><a href="#authorization-policies">Authorization Policies</a>
<ul>
<li><a href="#sources-identities-from-field">Sources Identities ("from" field)</a></li>
<li><a href="#request-operation-to-field">Request Operation ("to" field)</a></li>
<li><a href="#conditions-when-field">Conditions ("when" field)</a></li>
<li><a href="#configure-the-action-field">Configure the "action" Field</a></li>
</ul>
</li>
<li><a href="#how-are-rules-evaluated">How Are Rules Evaluated?</a></li>
</ul>
</li>
<li><a href="#access-control---lab">Access Control - Lab</a>
<ul>
<li><a href="#enabling-requests-from-ingress-to-web-frontend">Enabling Requests from Ingress to “web-frontend"</a>
<ul>
<li><a href="#enabling-requests-from-web-frontend-to-customers">Enabling Requests from "web-frontend" to “customers"</a></li>
</ul>
</li>
<li><a href="#gateway-and-auth">Gateway and Auth</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#policy-storage">Policy storage</a></li>
</ul>
</li>
<li><a href="#advanced-topics-for-istio">Advanced Topics for Istio</a>
<ul>
<li><a href="#external-authorization">External Authorization</a></li>
<li><a href="#deployment-models-for-serice-mesh">Deployment Models for Serice Mesh</a>
<ul>
<li><a href="#multi-cluster-deployments">Multi-cluster Deployments</a>
<ul>
<li><a href="#network-deployment-models">Network Deployment Models</a></li>
<li><a href="#control-plane-deployment-models">Control Plane Deployment Models</a></li>
<li><a href="#mesh-deployment-models">Mesh Deployment Models</a></li>
</ul>
</li>
<li><a href="#tenancy-models">Tenancy Models</a></li>
<li><a href="#locality-failover">Locality Failover</a></li>
<li><a href="#onboarding-vms">Onboarding VMs</a>
<ul>
<li><a href="#connect-a-vm-workload-to-the-istio-mesh">Connect a VM Workload to the Istio Mesh</a>
<ul>
<li><a href="#create-a-kubernetes-cluster">Create a Kubernetes Cluster</a></li>
<li><a href="#install-the-ratings-service-on-the-vm">Install the Ratings Service on the VM</a></li>
<li><a href="#allow-pod-to-vm-traffic-on-port-9080">Allow POD-to-VM Traffic on Port 9080</a></li>
<li><a href="#install-the-east-west-gateway-and-expose-istiod">Install the East-West Gateway and Expose Istiod</a></li>
<li><a href="#create-the-workloadgroup">Create the WorkloadGroup</a></li>
<li><a href="#generate-vm-artifacts">Generate VM Artifacts</a></li>
<li><a href="#vm-configuration-recipe">VM Configuration Recipe</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#custom-ca-integration-using-kubernetes-csr">Custom CA Integration using Kubernetes CSR</a>
<ul>
<li><a href="#what-is-a-certificate">What Is a Certificate?</a></li>
<li><a href="#certificate-trust-chain">Certificate Trust Chain</a></li>
<li><a href="#how-to-incorporate-istio-into-the-pki-certificate-trust-chain">How to Incorporate Istio into the PKI Certificate Trust Chain</a></li>
<li><a href="#steps-for-using-a-custom-ca-in-istio">Steps for Using a Custom CA in Istio</a>
<ul>
<li><a href="#deploy-cert-manager-according-to-the-installation-doc">Deploy cert-manager according to the installation doc.</a></li>
<li><a href="#create-self-signed-cluster-issuers">Create self-signed cluster issuers</a></li>
<li><a href="#deploy-istio-on-the-cluster">Deploy Istio on the cluster</a></li>
<li><a href="#install-istio">Install Istio</a></li>
<li><a href="#congigure-sidecars-to-use-custom-ca-in-each-namespace">Congigure sidecars to use custom CA in each namespace</a></li>
<li><a href="#test">Test</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#secure-gatways-using-tls-self-signed-certificates">Secure Gatways using TLS self-signed certificates</a>
<ul>
<li><a href="#add-our-ca-to-a-secret">Add our CA to a secret</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#use-envoy-as-front-proxy-or-reverse-proxy">Use Envoy as Front Proxy (or Reverse Proxy)</a>
<ul>
<li><a href="#proxy-protocol-and-envoy">Proxy Protocol and Envoy</a>
<ul>
<li><a href="#what-is-the-proxy-protocol">What is the Proxy Protocol?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#gitops-fluxcd">GitOps: FluxCD</a>
<ul>
<li><a href="#control-and-feedback-loop">Control and feedback loop</a></li>
<li><a href="#key-gitops-benefits">Key GitOps benefits</a></li>
<li><a href="#gitops-delivery-pipeline">GitOps delivery pipeline</a></li>
<li><a href="#fluxcd">FluxCD</a>
<ul>
<li><a href="#bootstrap-flux">Bootstrap Flux:</a></li>
<li><a href="#create-a-k8s-secret-to-be-used-by-flux-to-access-repos-instead-of-pat">Create a K8s secret to be used by flux to access repos instead of PAT:</a></li>
<li><a href="#specify-the-source-repo-for-flux-to-monitor">Specify the source repo for flux to monitor</a></li>
<li><a href="#specify-the-location-of-manifests-in-the-source-repo">Specify the location of manifests in the source repo</a></li>
<li><a href="#create-a-kustomization-resource">Create a kustomization resource</a></li>
<li><a href="#automating-image-updates">Automating Image Updates</a></li>
</ul>
</li>
<li><a href="#progressive-deployment-flagger">Progressive Deployment: Flagger</a>
<ul>
<li><a href="#test-the-canary">Test the Canary:</a>
<ul>
<li><a href="#prometheus">Prometheus</a></li>
<li><a href="#workload-level-aggregation-via-recording-rules">Workload-level aggregation via recording rules</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#kubernetes-on-linux">Kubernetes on Linux</a><br>
- <a href="#metallb-installation">MetalLB Installation</a><br>
- <a href="#installation---manifest">Installation - Manifest</a><br>
- <a href="#configuration">Configuration</a><br>
- <a href="#install-helm">Install Helm</a><br>
- <a href="#useful-commands-for-kubernetes">Useful commands for Kubernetes:</a><br>
- <a href="#kubectl-exec-syntax">kubectl exec Syntax</a>
<ul>
<li><a href="#how-to-find-unused-ip-address-on-my-network">How to find unused IP address on my network?</a></li>
</ul>
</li>
</ul>
<h2 id="from-monolith-to-microservices">From Monolith to Microservices </h2>
<p>Most new companies today run their business processes in the cloud. Newer startups and enterprises which realized early enough the direction technology was headed developed their applications for the cloud.<br>
Not all companies were so fortunate. Some built their success decades ago on top of legacy technologies - monolithic applications with all components tightly coupled and almost impossible to separate, a nightmare to manage and deployed on super expensive hardware.</p>
<ul>
<li>A monolith has a rather <strong>expensive taste in hardware</strong>. Being a large, single piece of software which continuously grows, it has to run on a single system which has to satisfy its compute, memory, storage, and networking requirements.</li>
<li>The hardware of such capacity is not only complex and extremely pricey, but at times challenging to procure. Since the entire monolith application runs as a single process, the <strong>scaling of individual features of the monolith is almost impossible</strong>. It internally supports a hardcoded number of connections and operations.</li>
<li>However, scaling the entire application can be achieved by manually deploying a new instance of the monolith on another server, typically behind a load balancing appliance - another pricey solution. During upgrades, patches or migrations of the monolith application <strong>downtime is inevitable and maintenance windows have to be planned</strong> well in advance as disruptions in service are expected to impact clients.</li>
<li>While there are third party solutions to minimize downtime to customers by setting up monolith applications in a highly available active/passive configuration, they introduce new challenges for system engineers to keep all systems at the same patch level and may introduce new possible licensing costs.</li>
</ul>
<p>In contrast,</p>
<ul>
<li>Microservices can be deployed individually on separate servers provisioned with fewer resources - only what is required by each service and the host system itself, helping to lower compute resource expenses.</li>
<li>Microservices-based architecture is aligned with Event-driven Architecture&nbsp; and&nbsp; Service-Oriented Architecture (SOA) principles, where complex applications are composed of small independent processes which communicate with each other through Application Programming Interfaces (APIs) over a network.</li>
<li>APIs allow access by other internal services of the same application or external, third-party services and applications.</li>
<li>Each microservice is developed and written in a modern programming language, selected to be the best suitable for the type of service and its business function. This offers a great deal of flexibility when matching microservices with specific hardware when required, allowing deployments on inexpensive commodity hardware.</li>
<li>Although the distributed nature of microservices adds complexity to the architecture, one of the greatest benefits of microservices is scalability. With the overall application becoming modular, each microservice can be scaled individually, either manually or automated through demand-based autoscaling.</li>
<li>Seamless upgrades and patching processes are other benefits of microservices architecture. There is virtually no downtime and no service disruption to clients because upgrades are rolled out seamlessly - one service at a time, rather than having to recompile, rebuild and restart an entire monolithic application.</li>
<li>As a result, businesses are able to develop and roll-out new features and updates a lot faster, in an agile approach, having separate teams focusing on separate features, thus being more productive and cost-effective.</li>
</ul>
<p>Refactoring features out of monolith is an incremental approach that offers a gradual transition from a legacy monolith to modern microservices architecture and allows for phased migration of application features into the cloud. The refactoring phase slowly transforms the monolith into&nbsp;a cloud-native application which&nbsp;takes full advantage of cloud features, by coding in new programming languages and applying modern architectural patterns. Through refactoring, a legacy monolith application receives a second chance at life - to live on as a modular system&nbsp;adapted to fully integrate with today's fast-paced cloud automation tools and services.</p>
<h2 id="container-orchestration">Container Orchestration </h2>
<p>Container images allow us to confine&nbsp;the application code, its runtime, and all of its dependencies in a pre-defined format. The container runtimes&nbsp;like <code>runC</code>, <code>containerd</code>, or <code>cri-o</code>&nbsp;can use pre-packaged images as a source to create and run one or more containers. These runtimes are capable of running containers on a single host, however, in practice, we would like to have a fault-tolerant and scalable solution, achieved by building a single controller/management unit, a collection of multiple hosts connected together. This&nbsp;controller/management unit&nbsp;is generally referred to as a&nbsp;container orchestrator.</p>
<p>Microservices are lightweight applications written in various modern programming languages, with specific dependencies, libraries and environmental requirements. To ensure that an application has everything it needs to run successfully it is packaged together with its dependencies.</p>
<p>Containers encapsulate microservices and their dependencies but do not run them directly. Containers run container images. <em>A container image bundles the application along with its runtime, libraries, and dependencies, and it represents the source of a container deployed to offer an isolated executable environment for the application</em>. Containers can be deployed from a specific image on many platforms, such as workstations, Virtual Machines, public cloud, etc.</p>
<p>In Development (Dev) environments, running containers on a single host for development and testing of applications may be a suitable option. However, when migrating to Quality Assurance (QA) and Production (Prod) environments, that is no longer a viable option because the applications and services need to meet specific requirements:</p>
<ul>
<li>Fault-tolerance</li>
<li>On-demand scalability</li>
<li>Optimal resource usage</li>
<li>Auto-discovery to automatically discover and communicate with each other</li>
<li>Accessibility from the outside world</li>
<li>Seamless updates/rollbacks without any downtime.</li>
</ul>
<p>Container orchestrators are tools which group systems together to form clusters where containers' deployment and management&nbsp;is automated at scale while meeting the requirements mentioned above. Among some are Amazon Elastic Container Service (AWS ECS), Kubernetes (CNCF project), Azure Container Instances (Microsoft Azure) and Docker Swarm (part of Docker Engine).</p>
<p>Most container orchestrators can:</p>
<ul>
<li>Group hosts together while creating a cluster.</li>
<li>Schedule containers to run on hosts in the cluster based on resources availability.</li>
<li>Enable containers in a cluster to communicate with each other regardless of the host they are deployed to in the cluster.</li>
<li>Bind containers and storage resources.</li>
<li>Group sets of similar containers and bind them to load-balancing constructs to simplify access to containerized applications by creating an interface, a level of abstraction between the containers and the client.</li>
<li>Manage and optimize resource usage.</li>
<li>Allow for implementation of policies to secure access to applications running inside containers.</li>
</ul>
<p><strong>Kubernetes</strong> is one of the most in-demand container orchestration tools available today.<br>
Most container orchestrators can be deployed on the infrastructure of our choice - on bare metal, Virtual Machines, on-premises, on public and hybrid clouds. Kubernetes, for example, can be deployed on a workstation, with or without an isolation layer such as a local hypervisor or container runtime, inside a company's data center, in the cloud on AWS Elastic Compute Cloud (EC2) instances, Google Compute Engine (GCE) VMs, DigitalOcean Droplets, OpenStack, etc. There are turnkey solutions which allow Kubernetes clusters to be installed, with only a few commands, on top of cloud Infrastructures-as-a-Service, such as GCE, AWS EC2, IBM Cloud, Rancher, VMware Tanzu, and multi-cloud solutions through IBM Cloud Private or StackPointCloud.<br>
Last but not least, there is the managed container orchestration as-a-Service, more specifically the managed Kubernetes as-a-Service solution, offered and hosted by the major cloud providers, such as Amazon Elastic Kubernetes Service (Amazon EKS), Azure Kubernetes Service (AKS), DigitalOcean Kubernetes, Google Kubernetes Engine (GKE), IBM Cloud Kubernetes Service, Oracle Container Engine for Kubernetes, or VMware Tanzu Kubernetes Grid.</p>
<h1 id="kubernetes">Kubernetes </h1>
<p>"Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications". According to the abstract of Google's Borg paper, published in 2015, "Google's Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines".</p>
<p>For more than a decade, Borg has been Google's secret, running its worldwide containerized workloads in production. Services we use from Google, such as Gmail, Drive, Maps, Docs, etc., are all serviced using Borg.&nbsp;Among the initial authors of Kubernetes were Google employees who have used Borg and developed it in the past. They poured in their valuable knowledge and experience while designing Kubernetes. Several features/objects of Kubernetes that can be traced back to Borg, or to lessons learned from it, are:</p>
<ul>
<li>API servers</li>
<li>Pods</li>
<li>IP-per-Pod</li>
<li>Services</li>
<li>Labels</li>
</ul>
<p>Kubernetes offers a very rich set of features for container orchestration. Some of its fully supported features are:</p>
<ul>
<li><strong>Automatic bin packing</strong>: Kubernetes automatically schedules containers based on resource needs and constraints, to maximize utilization without sacrificing availability</li>
<li><strong>Designed for extensibility</strong>: A Kubernetes cluster can be extended with new custom features without modifying the upstream source code.</li>
<li><strong>Self-healing</strong>: Kubernetes automatically replaces and reschedules containers from failed nodes. It terminates and then restarts containers that become unresponsive to health checks, based on existing rules/policy. It also prevents traffic from being routed to unresponsive containers.</li>
<li><strong>Horizontal scaling</strong>: With Kubernetes applications are scaled manually or automatically based on CPU or custom metrics utilization.</li>
<li><strong>Service discovery and load balancing</strong>: Containers receive IP addresses from Kubernetes, while it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set.</li>
</ul>
<p>Additional fully supported Kubernetes features are:</p>
<ul>
<li><strong>Automated rollouts and rollbacks</strong>: Kubernetes seamlessly rolls out and rolls back application updates and configuration changes, constantly monitoring the application's health to prevent any downtime.</li>
<li><strong>Secret and configuration management</strong>: Kubernetes manages sensitive data and configuration details for an application separately from the container image, in order to avoid a rebuild of the respective image. Secrets consist of sensitive/confidential information passed to the application without revealing the sensitive content to the stack configuration, like on GitHub.</li>
<li>Storage orchestration: Kubernetes automatically mounts software-defined storage (SDS) solutions to containers from local storage, external cloud providers, distributed storage, or network storage systems.</li>
<li>Batch execution: Kubernetes supports batch execution, long-running jobs, and replaces failed containers.</li>
<li>IPv4/IPv6 dual-stack: Kubernetes supports both IPv4 and IPv6 addresses.</li>
</ul>
<p>There are many additional features currently in alpha or beta phase. They will add great value to any Kubernetes deployment once they become stable features. For example, support for role-based access control (RBAC) is stable only as of the Kubernetes 1.8 release.<br>
Another one of Kubernetes' strengths is portability. It can be deployed in many environments such as local or remote Virtual Machines, bare metal, or in public/private/hybrid/multi-cloud setups. Kubernetes extensibility allows it to support and to be supported by many 3rd party open source tools which enhance Kubernetes' capabilities and provide a feature-rich experience to its users. It's architecture is modular and pluggable. Not only does it orchestrate modular, decoupled microservices type applications, but also its architecture follows decoupled microservices patterns. <em>Kubernetes' functionality can be extended by writing custom resources, operators, custom APIs, scheduling rules or plugins</em>.</p>
<h2 id="kubernetes-architecture">Kubernetes Architecture </h2>
<p>At a very high level, Kubernetes is a cluster of compute systems categorized by their distinct roles:</p>
<ul>
<li>One or more control plane nodes</li>
<li>One or more worker nodes (optional, but recommended).</li>
</ul>
<p align="center">
<img src="./assets/k8s/k8s-architecture.png" alt="drawing" width="700" height="400" style="center">
</p>
<h3 id="control-plane">Control Plane </h3>
<ul>
<li>
<p>The control plane node provides a running environment for the control plane agents responsible for managing the state of a Kubernetes cluster, and it is the brain behind all operations inside the cluster.</p>
</li>
<li>
<p>In order to communicate with the Kubernetes cluster, users send requests to the control plane via a Command Line Interface (CLI) tool, a Web User-Interface (Web UI) Dashboard, or an Application Programming Interface (API)</p>
</li>
<li>
<p>It is important to keep the control plane running at all costs. Losing the control plane may introduce downtime, causing service disruption to clients, with possible loss of business.</p>
</li>
<li>
<p>To ensure the control plane's fault tolerance, control plane node replicas can be added to the cluster, configured in High-Availability (HA) mode. While only one of the control plane nodes is dedicated to actively managing the cluster, the control plane components stay in sync across the control plane node replicas. This type of configuration adds resiliency to the cluster's control plane, should the active control plane node fail.</p>
</li>
<li>
<p>To persist the Kubernetes cluster's state, all cluster configuration data is saved to a distributed key-value store which only holds cluster state related data, no client workload generated data. The key-value store may be configured on the control plane node (stacked topology), or on its dedicated host (external topology) to help reduce the chances of data store loss by decoupling it from the other control plane agents.</p>
<ul>
<li>In the stacked key-value store topology, HA control plane node replicas ensure the key-value store's resiliency as well. However, that is not the case with external key-value store topology, where the dedicated key-value store hosts have to be separately replicated for HA, a configuration that introduces the need for additional hardware, hence additional operational costs.</li>
</ul>
</li>
<li>
<p>A control plane node runs the following essential control plane components and agents:</p>
<ol>
<li><strong>API Server</strong></li>
<li><strong>Scheduler</strong></li>
<li><strong>Controller Managers</strong></li>
<li><strong>Key-Value Data Store</strong></li>
</ol>
<p>In addition, the control plane node runs: Container Runtime, Node Agent, Proxy, Optional add-ons for cluster-level monitoring and logging.</p>
</li>
</ul>
<h4 id="api-server">API Server </h4>
<p>All the administrative tasks are coordinated by the <em>kube-apiserver</em>, a central control plane component running on the control plane node. The API Server intercepts RESTful calls from users, administrators, developers, operators and external agents, then validates and processes them. During processing the API Server reads the Kubernetes cluster's current state from the key-value store, and after a call's execution, the resulting state of the Kubernetes cluster is saved in the key-value store for persistence. The API Server is the only control plane component to talk to the key-value store, both to read from and to save Kubernetes cluster state information - acting as a middle interface for any other control plane agent inquiring about the cluster's state.</p>
<h4 id="scheduler">Scheduler </h4>
<p>A scheduler watches for newly created Pods that have no Node assigned. For every Pod that the scheduler discovers, and becomes responsible for finding the best Node for that Pod to run on. The scheduler reaches this placement decision taking into account the scheduling principles described below.</p>
<p><em>kube-scheduler</em> is the default scheduler for Kubernetes and is designed so that, if you want and need to, you can write your own scheduling component and use that instead. Kube-scheduler selects an optimal node to run newly created or not yet scheduled (unscheduled) pods. Since containers in pods - and pods themselves - can have different requirements, the scheduler filters out any nodes that don't meet a Pod's specific scheduling needs.</p>
<p>In a cluster, Nodes that meet the scheduling requirements for a Pod are called feasible nodes. If none of the nodes are suitable, the pod remains unscheduled until the scheduler is able to place it.</p>
<p>The scheduler runs a set of functions to score the feasible Nodes and picks a Node with the highest score to run the Pod. The scheduler then notifies the API server about this decision in a process called binding.</p>
<p>Factors that need to be taken into account for scheduling decisions include individual and collective resource requirements, hardware, software, policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and so on.</p>
<h4 id="controller-managers">Controller Managers </h4>
<p>The controller managers are components of the control plane node running controllers or operator processes to regulate the state of the Kubernetes cluster. Controllers are watch-loop processes continuously running and comparing the cluster's desired state (provided by objects' configuration data) with its current state (obtained from the key-value store via the API Server). In case of a mismatch, corrective action is taken in the cluster until its current state matches the desired state.</p>
<p>The <em>kube-controller-manager</em> runs controllers or operators responsible to act when nodes become unavailable, to ensure container pod counts are as expected, to create endpoints, service accounts, and API access tokens.</p>
<p>The <em>cloud-controller-manager</em> runs controllers or operators responsible to interact with the underlying infrastructure of a cloud provider when nodes become unavailable, to manage storage volumes when provided by a cloud service, and to manage load balancing and routing.</p>
<h4 id="key-value-data-store">Key-Value Data Store </h4>
<p><strong><code>etcd</code></strong>&nbsp;is an open source project under the&nbsp;Cloud Native Computing Foundation (CNCF). <code>etcd</code> is a</p>
<ul>
<li>Strongly consistent,</li>
<li>Distributed key-value data store used to persist a Kubernetes cluster's state.</li>
<li>New data is written to the data store only by appending to it, data is never replaced in the data store. Obsolete data is compacted (or shredded) periodically to minimize the size of the data store.</li>
<li>Out of all the control plane components, only the API Server is able to communicate with the etcd data store.</li>
<li>In Kubernetes, besides storing the cluster state, etcd is also used to store configuration details such as subnets, ConfigMaps, Secrets, etc.</li>
</ul>
<h3 id="data-plane">Data Plane </h3>
<p>A <strong>worker node</strong> provides a running environment for client applications. These applications are microservices running as application containers. In Kubernetes the application containers are encapsulated in Pods, controlled by the cluster control plane agents running on the control plane node. Pods are scheduled on worker nodes, where they find required compute, memory and storage resources to run, and networking to talk to each other and the outside world.</p>
<p>A <strong>Pod</strong> is the smallest scheduling work unit in Kubernetes. It is a logical collection of one or more containers scheduled together, and the collection can be started, stopped, or rescheduled as a single unit of work.&nbsp;Also, in a multi-worker Kubernetes cluster, the network traffic between client users and the containerized applications deployed in Pods is handled directly by the worker nodes, and is not routed through the control plane node.</p>
<p>A worker node has the following components:</p>
<ul>
<li>Container Runtime</li>
<li>Node Agent - kubelet</li>
<li>Proxy - kube-proxy</li>
<li>Add-ons for DNS, Dashboard user interface, cluster-level monitoring and logging</li>
</ul>
<h4 id="container-runtime">Container Runtime </h4>
<p>Although Kubernetes is described as a "container orchestration engine", it lacks the capability to directly handle and run containers. In order to manage a container's lifecycle, Kubernetes requires a container runtime on the node where a Pod and its containers are to be scheduled. Runtimes are required on all nodes of a Kubernetes cluster, both control plane and worker.</p>
<p>Kubernetes supports several container runtimes:</p>
<ul>
<li>CRI-O: A lightweight container runtime for Kubernetes, supporting&nbsp;<a href="http://quay.io">quay.io</a> and&nbsp;Docker Hub image registries</li>
<li>containerd: A simple, robust, and portable container runtime</li>
<li>Docker Engine: A popular and complex container platform which uses containerd as a container runtime</li>
<li>Mirantis Container Runtime</li>
<li>Formerly known as the Docker Enterprise Edition.</li>
</ul>
<h4 id="node-agent---kubelet">Node Agent - kubelet </h4>
<p>The kubelet is an agent running on each node, control plane and workers, and communicates with the control plane. It receives Pod definitions, primarily from the API Server, and interacts with the container runtime on the node to run containers associated with the Pod. It also monitors the health and resources of Pods running containers.</p>
<p>The kubelet connects to container runtimes through a plugin based interface, the&nbsp;Container Runtime Interface (CRI). The CRI consists of protocol buffers, gRPC API, libraries, and additional specifications and tools.</p>
<p align="center">
<img src="./assets/k8s/kubelet.png" alt="drawing" width="600" height="200" style="center">
</p>
<p>In order to connect to interchangeable container runtimes, kubelet uses a CRI shim, an application which provides a clear abstraction layer between kubelet and the container runtime.</p>
<h4 id="proxy-kube-proxy">Proxy-Kube-proxy </h4>
<p>The kube-proxy is the network agent which runs on each node, control plane and workers, responsible for dynamic updates and maintenance of all networking rules on the node. It abstracts the details of Pods networking and forwards connection requests to the containers in the Pods.&nbsp;The kube-proxy is responsible for&nbsp;TCP, UDP, and SCTP stream forwarding or random forwarding across a set of Pod backends of an application, and it implements forwarding rules defined by users through Service API objects.</p>
<h4 id="add-ons">Add-ons </h4>
<p>Add-ons are cluster features and functionality not yet available in Kubernetes, therefore implemented through 3rd-party pods and services.</p>
<ul>
<li>DNS: Cluster DNS is a DNS server required to assign DNS records to Kubernetes objects and resources</li>
<li>Dashboard: A general purpose web-based user interface for cluster management</li>
<li>Monitoring:&nbsp;Collects cluster-level container metrics and saves them to a central data store</li>
<li>Logging: Collects cluster-level container logs and saves them to a central log store for analysis</li>
</ul>
<h3 id="networking-challenges">Networking Challenges </h3>
<p>Decoupled microservices based applications rely heavily on networking in order to mimic the tight-coupling once available in the monolithic era. Networking, in general, is not the easiest to understand and implement. Kubernetes is no exception - as a containerized microservices orchestrator it needs to address a few distinct networking challenges:</p>
<ul>
<li><strong>Container-to-Container</strong> communication inside Pods</li>
<li><strong>Pod-to-Pod</strong> communication on the same node and across cluster nodes</li>
<li><strong>Service-to-Pod</strong> communication within the same namespace and across cluster namespaces</li>
<li><strong>External-to-Service</strong> communication for clients to access applications in a cluster</li>
</ul>
<p>All these networking challenges must be addressed before deploying a Kubernetes cluster.</p>
<p>When a grouping of containers defined by a Pod is started, a special infrastructure <strong>Pause Container</strong> is initialized by the Container Runtime for the sole purpose of creating a network namespace for the Pod. All additional containers, created through user requests, running inside the Pod will share the pause container's network namespace so that they can all talk to each other via localhost.</p>
<p>In a Kubernetes cluster Pods, groups of containers, are scheduled on nodes in a nearly unpredictable fashion. Regardless of their host node, Pods are expected to be able to communicate with all other Pods in the cluster, all this <em>without the implementation of Network Address Translation (NAT)</em>. This is a fundamental requirement of any networking implementation in Kubernetes.</p>
<p>The Kubernetes network model aims to reduce complexity, and it treats Pods as VMs on a network, where each VM is equipped with a network interface - thus <em>each Pod receiving a unique IP address</em>. This model is called <strong>IP-per-Pod</strong> and ensures Pod-to-Pod communication, just as VMs are able to communicate with each other on the same network.</p>
<p>Let's not forget about containers though. They share the Pod's network namespace and must coordinate ports assignment inside the Pod just as applications would on a VM, all while being able to communicate with each other on <strong>localhost</strong> - inside the Pod. However, containers are integrated with the overall Kubernetes networking model through the use of the Container Network Interface (CNI) supported by CNI plugins. CNI is a set of specifications and libraries which allow plugins to configure the networking for containers. While there are a few core plugins, most CNI plugins are 3rd-party Software Defined Networking (SDN) solutions implementing the Kubernetes networking model.&nbsp;In addition to addressing the fundamental requirement of the networking model, some networking solutions offer support for Network Policies. <strong>Flannel</strong>, <strong>Weave</strong>, <strong>Calico</strong> are only a few of the SDN solutions available for Kubernetes clusters.</p>
<p>A successfully deployed containerized application running in Pods inside a Kubernetes cluster may require accessibility from the outside world. Kubernetes enables external accessibility through <strong>Services</strong>, complex encapsulations of network routing rule definitions stored in <strong>iptables</strong> on cluster nodes and implemented by <strong>kube-proxy</strong> agents. By exposing services to the external world with the aid of&nbsp;kube-proxy, applications become accessible from outside the cluster over a virtual IP address and a dedicated port number.</p>
<h3 id="kubeadm">kubeadm </h3>
<p><strong>kubeadm</strong> is a first-class citizen of the Kubernetes ecosystem. It is a secure and recommended method to bootstrap a multi-node production ready Highly Available Kubernetes cluster, on-premises or in the cloud. kubeadm can also bootstrap a single-node cluster for learning. It has a set of building blocks to set up the cluster, but it is easily extendable to add more features. Please note that kubeadm does not support the provisioning of hosts - they should be provisioned separately with a tool of our choice.</p>
<h4 id="kubectl">kubectl </h4>
<p><strong>kubectl</strong> allows us to manage local Kubernetes clusters, or remote clusters deployed in the cloud. To access the Kubernetes cluster, the&nbsp;kubectl&nbsp;client needs the control plane node endpoint and appropriate credentials to be able to securely interact with the API Server running on the control plane node.</p>
<p>When not using the&nbsp;kubectl proxy, we need to authenticate to the API Server when sending API requests. We can <strong>authenticate</strong> by providing a&nbsp;<strong>Bearer Token</strong>&nbsp;when issuing a&nbsp;curl&nbsp;command, or by providing a set of keys and certificates. A Bearer Token is an access token that can be generated by the authentication server (the API Server on the control plane node) at the client's request. Using that token, the client can securely communicate with the Kubernetes API Server without providing additional authentication details, and then, access resources. The token may need to be provided again for subsequent resource access requests.</p>
<h4 id="namespaces">Namespaces </h4>
<p>If multiple users and teams use the same Kubernetes cluster we can partition the cluster into virtual sub-clusters using <strong>Namespaces</strong>. The names of the resources/objects created inside a Namespace are unique, but not across Namespaces in the cluster. Generally, Kubernetes creates four Namespaces out of the box:&nbsp;kube-system, kube-public, kube-node-lease,&nbsp;and&nbsp;default.</p>
<ul>
<li>The kube-system Namespace contains the objects created by the Kubernetes system, mostly the control plane agents.</li>
<li>The&nbsp;default&nbsp;Namespace contains&nbsp;the objects and resources created by administrators and developers, and objects are assigned to it by default unless another Namespace name is provided by the user.</li>
<li>kube-public&nbsp;is a special Namespace, which is unsecured and readable by anyone, used for special purposes such as exposing public (non-sensitive) information about the cluster.</li>
<li>The newest Namespace is kube-node-lease which holds node lease objects used for node heartbeat data.</li>
</ul>
<p>Good practice, however, is to create additional Namespaces, as desired, to virtualize the cluster and isolate users, developer teams, applications, or tiers.</p>
<h3 id="pods">Pods </h3>
<p>A <strong>Pod</strong> is the smallest Kubernetes workload object. It is the unit of deployment in Kubernetes, which represents a single instance of the application.&nbsp;A Pod is&nbsp;a logical collection of one or more containers, enclosing and isolating them to ensure that they:</p>
<ul>
<li>Are scheduled together on the same host with the Pod</li>
<li>Share the same network namespace, meaning that they share a single IP address originally assigned to the Pod</li>
<li>Have access to mount the same external storage (volumes) and other common dependencies</li>
</ul>
<pre data-role="codeBlock" data-info="Yaml" class="language-yaml Yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>pod  
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>   
    <span class="token key atrule">run</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  containers<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx   
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.22.1 
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>    
    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre><p>The&nbsp;<code>apiVersion</code>&nbsp;field must specify <code>v1</code> for the Pod object definition. The second required field is kind&nbsp;specifying the Pod object type. The third required field&nbsp;metadata, holds the object's name and optional <code>labels</code> and <code>annotations</code>.&nbsp;The fourth required field&nbsp;<code>spec</code>&nbsp;marks the beginning of the block defining the&nbsp;desired state of the Pod object - also named the PodSpec. Our Pod creates a single container running the&nbsp;<code>nginx:1.22.1</code>&nbsp;image pulled from a container image registry, in this case from Docker Hub. The <code>containerPort</code> field specifies the container port to be exposed by Kubernetes resources for inter-application access or external client access - to be explored in the Services chapter. The contents of spec are evaluated for scheduling purposes, then the kubelet of the selected node becomes responsible for running the container image with the help of the container runtime of the node. The Pod's name and labels are used for workload accounting purposes.</p>
<h3 id="labels">Labels </h3>
<p><strong>Labels</strong>&nbsp;are key-value pairs attached to Kubernetes objects (e.g. Pods, ReplicaSets, Nodes, Namespaces, Persistent Volumes). Labels are used to organize and select a subset of objects, based on the requirements in place. Many&nbsp;objects can have the same Label(s). Labels do not provide uniqueness to objects. Controllers use Labels to logically group together decoupled objects, rather than using objects' names or IDs.<br>
Controllers, or operators, and Services, use&nbsp;label selectors to select a subset of objects. Kubernetes supports two types of Selectors:</p>
<ul>
<li>
<p>Equality-Based Selectors: allow filtering of objects based on label keys and values. Matching is achieved using the =,&nbsp;== (equals, used interchangeably), or !=&nbsp;(not equals) operators. For example, with <code>env==dev</code>&nbsp;or <code>env=dev</code>, we are selecting the objects where the env&nbsp;label key is set to value&nbsp;dev.</p>
</li>
<li>
<p>Set-Based Selectors: allow filtering of objects based on a set of values. We can use&nbsp;<code>in</code>, <code>notin</code>&nbsp;operators for Label values, and <code>exist</code>/<code>does not exist</code>&nbsp;operators for Label keys. For example, with <code>env in (dev,qa)</code>,&nbsp;we are selecting objects where the env&nbsp;Label is set to either&nbsp;<code>dev</code> or <code>qa</code>;&nbsp;with <code>!app</code> we select objects with no Label key app.</p>
</li>
</ul>
<p align="center">
    <img src="./assets/k8s/labels.png" alt="drawing" width="500" height="400" style="center">
</p>
<h3 id="deployment">Deployment </h3>
<p>Generally, we do not deploy a Pod independently, as it would not be able to re-start itself if terminated in error because a Pod misses the much desired self-healing feature that Kubernetes otherwise promises. The recommended method is to use some type of an operator to run and manage Pods. The default recommended controller is the&nbsp;<strong>Deployment</strong> which configures a&nbsp;<strong>ReplicaSet</strong>&nbsp;controller to manage application Pods' lifecycle. A ReplicaSet&nbsp;is, in part, the next-generation ReplicationController, as it implements the replication and self-healing aspects of the ReplicationController. ReplicaSets support both equality- and set-based Selectors, whereas ReplicationControllers only support equality-based Selectors.</p>
<p>The lifecycle of the application defined by a Pod will be overseen by the ReplicaSet. With the help of the ReplicaSet, we can scale the number of Pods running a specific application container image. Scaling can be accomplished manually or through the use of an autoscaler. Deployments&nbsp;manage the creation, deletion, and updates of Pods.</p>
<p>A Deployment automatically creates a ReplicaSet, which then creates a Pod. There is no need to manage ReplicaSets and Pods separately, the Deployment will manage them on our behalf.</p>
<p>Deployment&nbsp;objects provide declarative updates to Pods and ReplicaSets. The <strong>DeploymentController</strong> is part of the control plane node's controller manager, and as a controller it also ensures that the current state always matches the desired state of our running containerized application. It allows for seamless application updates and rollbacks, known as the default&nbsp;<strong>RollingUpdate</strong> strategy, through <strong>rollouts</strong> and <strong>rollbacks</strong>, and it directly manages its ReplicaSets for application scaling. It also supports a disruptive, less popular update strategy, known as Recreate.</p>
<pre data-role="codeBlock" data-info="Yaml" class="language-yaml Yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>deployment
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.14.2
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>
   <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate
   <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>
     <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">1</span>
</code></pre><p>The <code>apiVersion</code> field is the first required field, and it specifies the API endpoint on the API server which we want to connect to; it must match an existing version for the object type defined. The second required field is <code>kind</code>, specifying the object type - in our case it is&nbsp;Deployment, but it can be Pod, ReplicaSet, Namespace, Service, etc. The third required field&nbsp;<code>metadata</code>, holds the object's basic information, such as <code>name</code>, <code>annotations</code>, <code>labels</code>, <code>namespaces</code>, etc. Our example&nbsp;shows two&nbsp;<code>spec</code>&nbsp;fields (<code>spec</code> and&nbsp;<code>spec.template.spec</code>).&nbsp;The fourth required field&nbsp;<code>spec</code>&nbsp;marks the beginning of the block defining the&nbsp;desired state of the Deployment object. In our example, we are requesting that&nbsp;3 replicas, that is 3 instances of the Pod,&nbsp;are running at any given time. The Pods are created using the <strong>Pod Template</strong> defined in&nbsp;<code>spec.template</code>. A nested object, such as the Pod being part of a Deployment, retains its&nbsp;metadata and spec and loses its own&nbsp;apiVersion and kind - both being replaced by template. In&nbsp;<code>spec.template.spec</code>, we&nbsp;define the desired state of the Pod.</p>
<p>A <strong><code>rolling update</code></strong>&nbsp;is triggered when we update specific properties of the Pod Template for a deployment. While planned changes such as updating the container image, container port, volumes, and mounts would trigger a new Revision, other operations that are dynamic in nature, like scaling or labeling the deployment, do not trigger a rolling update, thus do not change the Revision number.<br>
Once the rolling update has completed, the Deployment will show both ReplicaSets A and B, where A is scaled to 0 (zero) Pods, and B is scaled to 3 Pods. This is how the Deployment records its prior state configuration settings, as Revisions.&nbsp;Once ReplicaSet B&nbsp;and its 3 Pods&nbsp;versioned 1.21.5 are ready, the Deployment starts actively managing them. However, the Deployment keeps its prior configuration states saved as Revisions which play a key factor in the <strong>rollback</strong> capability of the Deployment - returning to a prior known configuration state (<code>kubectl rollout undo deployment/nginx-deployment</code>).</p>
<h3 id="service">Service </h3>
<p>A containerized application deployed to a Kubernetes cluster may need to reach other such applications, or it may need to be accessible to other applications and possibly clients. This is problematic because the container does not expose its ports to the cluster's network, and it is not discoverable either. The solution would be a simple <strong>port mapping</strong>, as offered by a typical container host. However, due to the complexity of the Kubernetes framework, such a simple port mapping is not that "simple". The solution is much more sophisticated, with the involvement of the kube-proxy node agent, IP tables, routing rules, cluster DNS server, all collectively implementing a micro-load balancing mechanism that exposes a container's port to the cluster's network, even to the outside world if desired. This mechanism is called a <strong>Service</strong>, and it is the recommended method to expose any containerized application to the Kubernetes network. The benefits of the Kubernetes Service becomes more obvious when exposing a multi-replica application, when multiple containers running the same image need to expose the same port. This is where the simple port mapping of a container host would no longer work, but the Service would have no issue implementing such a complex requirement.</p>
<h1 id="authorization">Authorization </h1>
<p>After a successful authentication, users can send the API requests to perform different operations. Here, these API requests get&nbsp;authorized by Kubernetes using various authorization modules that allow or deny the requests.</p>
<p>Some of the API request attributes&nbsp;that are reviewed by Kubernetes include <strong>user</strong>, <strong>group</strong>, <strong>Resource</strong>, <strong>Namespace</strong>, or <strong>API group</strong>, to name a few. Next, these attributes are evaluated against policies. If the evaluation is successful, then the request is allowed,&nbsp;otherwise&nbsp;it is denied.</p>
<h3 id="node">Node </h3>
<p>Node authorization is a special-purpose authorization mode which specifically authorizes API requests made by kubelets. It authorizes the kubelet's read operations for services, endpoints, or nodes, and writes operations for nodes, pods, and events.</p>
<h3 id="webhook">Webhook </h3>
<p>In Webhook mode, Kubernetes can request authorization decisions to be made by third-party services, which would return true for successful authorization, and false for failure.&nbsp;In order to enable the Webhook authorizer, we need to start the API server with the&nbsp;<code>--authorization-webhook-config-file=SOME_FILENAME</code> option, where <code>SOME_FILENAME</code> is the configuration of the remote authorization service.</p>
<h3 id="role-based-access-control-rbac">Role-Based Access Control (RBAC) </h3>
<p>In general, with RBAC we regulate the access to resources based on the Roles of individual users.  While creating the Roles, we restrict resource access by specific operations, such as create, get, update, patch, etc. These operations are referred to as verbs that can appear in <code>kubectl</code> command for example (<code>kubectl get pods</code>, <code>kubectl patch deployment ...</code>).</p>
<p>In RBAC, we can create two kinds of Roles:</p>
<ul>
<li><strong>Role</strong>: A Role grants access to resources within a specific Namespace</li>
<li><strong>ClusterRole</strong>: A ClusterRole grants the same permissions as Role does, but its scope is cluster-wide</li>
</ul>
<p>An RBAC Role or ClusterRole contains rules that represent a set of permissions. Permissions are purely additive (there are no "deny" rules).</p>
<p>If you want to define a role within a namespace, use a Role; if you want to define a role cluster-wide, use a ClusterRole. For example, use ClusterRole for</p>
<ul>
<li>cluster-scoped resources (like nodes)</li>
<li>non-resource endpoints (like /healthz)</li>
<li>namespaced resources (like Pods), across all namespaces</li>
</ul>
<p>Example: you can use a ClusterRole to allow a particular user to run <code>kubectl get pods --all-namespaces</code>.</p>
<p>A <strong>RoleBinding</strong> grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted. A RoleBinding grants permissions within a specific namespace whereas a <strong>ClusterRoleBinding</strong> grants that access cluster-wide.</p>
<p>Below you will find an example:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Role
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  namespace<span class="token punctuation">:</span> lfs158
  name<span class="token punctuation">:</span> pod<span class="token punctuation">-</span>reader
<span class="token key atrule">rules</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span> <span class="token comment"># "" indicates the core API group</span>
  resources<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"pods"</span><span class="token punctuation">]</span>
  verbs<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">,</span> <span class="token string">"watch"</span><span class="token punctuation">,</span> <span class="token string">"list"</span><span class="token punctuation">]</span>
</code></pre><p>The manifest defines a pod-reader role, which has access only to read the Pods of <code>lfs158</code> namespace.</p>
<p>Once the role is created, we can <em>bind it to users with a RoleBinding object</em>. There are two kinds of RoleBindings:</p>
<ul>
<li><strong>RoleBinding</strong>: allows us to bind users to the same namespace as a Role. We could also refer to a ClusterRole in RoleBinding, which would grant permissions to Namespace resources defined in the ClusterRole within the RoleBinding’s Namespace.</li>
<li><strong>ClusterRoleBinding</strong>: allows us to grant access to resources at a cluster-level and to all Namespaces.</li>
</ul>
<p>Below, you will find an example of RoleBinding:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> RoleBinding
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> pod<span class="token punctuation">-</span>read<span class="token punctuation">-</span>access
  namespace<span class="token punctuation">:</span> lfs158
<span class="token key atrule">subjects</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> User
  name<span class="token punctuation">:</span> bob
  apiGroup<span class="token punctuation">:</span> rbac.authorization.k8s.io
<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>
  kind<span class="token punctuation">:</span> Role
  name<span class="token punctuation">:</span> pod<span class="token punctuation">-</span>reader
  apiGroup<span class="token punctuation">:</span> rbac.authorization.k8s.io
</code></pre><p>The manifest defines a bind between the pod-reader Role and user bob, to restrict the user to only read the Pods of the <code>lfs158</code> Namespace.</p>
<p>To enable the RBAC mode, we start the API server with the <code>--authorization-mode=RBAC</code> option, allowing us to dynamically configure policies. For more details, please review the <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC mode</a>.</p>
<h3 id="example-of-user-autherization-using-rbac">Example of User Autherization using RBAC </h3>
<p>You can find users or context by running command</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl config view
</code></pre><p>Create lfs158 namespace:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl create namespace lfs158
namespace/lfs158 created
</code></pre><p>Create the&nbsp;rbac directory and cd into it:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">mkdir</span> rbac
$ <span class="token builtin class-name">cd</span> rbac/
</code></pre><p>Create a new user bob on your workstation, and set bob's password as well (the system will prompt you to enter the password twice) :</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ <span class="token function">sudo</span> <span class="token function">useradd</span> <span class="token parameter variable">-s</span> /bin/bash bob
~/rbac$ <span class="token function">sudo</span> <span class="token function">passwd</span> bob
</code></pre><p>Create a private key for the new user&nbsp;bob&nbsp;with the&nbsp;<code>openssl</code> tool, then create a certificate signing request for&nbsp;bob&nbsp;with the same&nbsp;<code>openssl</code> tool:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ openssl genrsa <span class="token parameter variable">-out</span> bob.key <span class="token number">2048</span>
Generating RSA private key, <span class="token number">2048</span> bit long modulus <span class="token punctuation">(</span><span class="token number">2</span> primes<span class="token punctuation">)</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>.+++++<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>.+++++e is <span class="token number">65537</span> <span class="token punctuation">(</span>0x010001<span class="token punctuation">)</span>
~/rbac$ openssl req <span class="token parameter variable">-new</span> <span class="token parameter variable">-key</span> bob.key <span class="token punctuation">\</span>  <span class="token parameter variable">-out</span> bob.csr <span class="token parameter variable">-subj</span> <span class="token string">"/CN=bob/O=learner"</span>
</code></pre><p>Create a YAML definition manifest for a certificate signing request object, and save it with a blank value for the request field:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ <span class="token function">vim</span> signing-request.yaml
</code></pre><pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> certificates.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> CertificateSigningRequest
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> bob<span class="token punctuation">-</span>csr
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  groups<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> system<span class="token punctuation">:</span>authenticated
  request<span class="token punctuation">:</span> &lt;assign encoded value from next cat command<span class="token punctuation">&gt;</span>
  signerName<span class="token punctuation">:</span> kubernetes.io/kube<span class="token punctuation">-</span>apiserver<span class="token punctuation">-</span>client
  usages<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> digital signature
  <span class="token punctuation">-</span> key encipherment
  <span class="token punctuation">-</span> client auth
</code></pre><p>View the certificate, encode it in <code>base64</code>, and assign it to the request field in the <code>signing-request.yaml</code> file:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ <span class="token function">cat</span> bob.csr <span class="token operator">|</span> base64 <span class="token operator">|</span> <span class="token function">tr</span> <span class="token parameter variable">-d</span> <span class="token string">'\n'</span>,<span class="token string">'%'</span>
LS0tLS1CRUd<span class="token punctuation">..</span>.1QtLS0tLQo<span class="token operator">=</span>
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ <span class="token function">vim</span> signing-request.yaml
</code></pre><pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> certificates.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> CertificateSigningRequest
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> bob<span class="token punctuation">-</span>csr
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  groups<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> system<span class="token punctuation">:</span>authenticated
  request<span class="token punctuation">:</span> LS0tLS1CRUd<span class="token punctuation">...</span>1QtLS0tLQo=
  signerName<span class="token punctuation">:</span> kubernetes.io/kube<span class="token punctuation">-</span>apiserver<span class="token punctuation">-</span>client
  usages<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> digital signature
  <span class="token punctuation">-</span> key encipherment
  <span class="token punctuation">-</span> client auth
</code></pre><p>Create the certificate signing request object, then list the certificate signing request objects. It shows a pending state:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl create <span class="token parameter variable">-f</span> signing-request.yaml
certificatesigningrequest.certificates.k8s.io/bob-csr created
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl get csr
NAME      AGE   SIGNERNAME                            REQUESTOR       CONDITION
bob-csr   12s   kubernetes.io/kube-apiserver-client   minikube-user   Pending
</code></pre><p>Approve the certificate signing request object, then list the certificate signing request objects again. It shows both approved and issued states:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl certificate approve bob-csr
certificatesigningrequest.certificates.k8s.io/bob-csr approved
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl get csr
NAME      AGE   SIGNERNAME                            REQUESTOR       CONDITION
bob-csr   57s   kubernetes.io/kube-apiserver-client   minikube-user   Approved,Issued
</code></pre><p>Extract the approved certificate from the certificate signing request, decode it with base64 and save it as a certificate&nbsp;file. Then view the certificate in the newly created certificate file:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl get csr bob-csr <span class="token punctuation">\</span>  <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token string">'{.status.certificate}'</span> <span class="token operator">|</span> <span class="token punctuation">\</span>  base64 <span class="token parameter variable">-d</span> <span class="token operator">&gt;</span> bob.crt
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ <span class="token function">cat</span> bob.crt
-----BEGIN CERTIFICATE-----
MIIDGzCCA<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>.NOZRRZBVunTjK7A<span class="token operator">==</span>
-----END CERTIFICATE-----
</code></pre><p>Configure the&nbsp;kubectl&nbsp;client's configuration manifest with user bob's credentials by assigning his&nbsp;key and certificate:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl config set-credentials bob <span class="token punctuation">\</span>  --client-certificate<span class="token operator">=</span>bob.crt --client-key<span class="token operator">=</span>bob.key
User <span class="token string">"bob"</span> set.
</code></pre><p>Create a new context entry in the kubectl client's configuration manifest for user&nbsp;bob, associated with the&nbsp;<code>lfs158</code> namespace in the minikube cluster:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl config set-context bob-context <span class="token punctuation">\</span>  <span class="token parameter variable">--cluster</span><span class="token operator">=</span>minikube <span class="token parameter variable">--namespace</span><span class="token operator">=</span>lfs158 <span class="token parameter variable">--user</span><span class="token operator">=</span>bob
Context <span class="token string">"bob-context"</span> created.
</code></pre><p>View the contents of the&nbsp;kubectl client's configuration manifest again, observing the new context entry bob-context, and the new user entry bob (the output is redacted for readability):</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl config view

apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/student/.minikube/ca.crt
    <span class="token punctuation">..</span>.
    server: https://192.168.99.100:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    <span class="token punctuation">..</span>.
    user: minikube
  name: minikube
- context:
    cluster: minikube
    namespace: lfs158
    user: bob
  name: bob-context
current-context: minikube
kind: Config
preferences: <span class="token punctuation">{</span><span class="token punctuation">}</span>
users:
- name: minikube
  user:
    client-certificate: /home/student/.minikube/profiles/minikube/client.crt
    client-key: /home/student/.minikube/profiles/minikube/client.key
- name: bob
  user:
    client-certificate: /home/student/rbac/bob.crt
    client-key: /home/student/rbac/bob.key
</code></pre><p>While in the default minikube context, create a new deployment in the <code>lfs158</code> namespace:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl <span class="token parameter variable">-n</span> lfs158 create deployment nginx <span class="token parameter variable">--image</span><span class="token operator">=</span>nginx:alpine
deployment.apps/nginx created
</code></pre><p>From the new context <code>bob-context</code>, try to list pods. The attempt fails because user&nbsp;bob&nbsp;has no permissions configured for the bob-context:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl <span class="token parameter variable">--context</span><span class="token operator">=</span>bob-context get pods
Error from server <span class="token punctuation">(</span>Forbidden<span class="token punctuation">)</span>: pods is forbidden: User <span class="token string">"bob"</span> cannot list resource <span class="token string">"pods"</span> <span class="token keyword keyword-in">in</span> API group <span class="token string">""</span> <span class="token keyword keyword-in">in</span> the namespace <span class="token string">"lfs158"</span>
</code></pre><p>The following steps will assign a limited set of permissions to user&nbsp;bob&nbsp;in the <code>bob-context</code>.<br>
&nbsp;<br>
Create a YAML configuration manifest for a pod-reader&nbsp;Role object, which allows only get,&nbsp;watch,&nbsp;list actions/verbs in the <code>lfs158</code> namespace against pod resources. Then create the role object and list it from the default minikube context, but from the <code>lfs158</code> namespace:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ <span class="token function">vim</span> role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: lfs158
rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>
  resources: <span class="token punctuation">[</span><span class="token string">"pods"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"watch"</span>, <span class="token string">"list"</span><span class="token punctuation">]</span>
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl create <span class="token parameter variable">-f</span> role.yaml
role.rbac.authorization.k8s.io/pod-reader created
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl <span class="token parameter variable">-n</span> lfs158 get roles
NAME         CREATED AT
pod-reader   <span class="token number">2022</span>-04-11T03:47:45Z
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ <span class="token function">vim</span> rolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pod-read-access
  namespace: lfs158
subjects:
- kind: User
  name: bob
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>~/rbac$ kubectl create <span class="token parameter variable">-f</span> rolebinding.yaml
role.rbac.authorization.k8s.io/pod-read-access created
</code></pre><p>Now bob can list the pods by runing the command <code>kubectl --context=bob-context get pods</code> to see the pod <code>nginx</code> without getting error.</p>
<h3 id="serviceaccount-permissions">ServiceAccount Permissions </h3>
<p>A <a href="https://kubernetes.io/docs/concepts/security/service-accounts/">service account</a> is a type of non-human account that, in Kubernetes, provides a distinct identity in a Kubernetes cluster. Application Pods, system components, and entities inside and outside the cluster can use a specific ServiceAccount's credentials to identify as that ServiceAccount. This identity is useful in various situations, including authenticating to the API server or implementing identity-based security policies.</p>
<p>Service accounts exist as ServiceAccount objects in the API server. Service accounts have the following properties:</p>
<ul>
<li>
<p>Namespaced: Each service account is bound to a Kubernetes namespace. Every namespace gets a default ServiceAccount upon creation.</p>
</li>
<li>
<p>Lightweight: Service accounts exist in the cluster and are defined in the Kubernetes API. You can quickly create service accounts to enable specific tasks.</p>
</li>
<li>
<p>Portable: A configuration bundle for a complex containerized workload might include service account definitions for the system's components. The lightweight nature of service accounts and the namespaced identities make the configurations portable.</p>
</li>
</ul>
<p>Service accounts are different from user accounts, which are authenticated human users in the cluster. By default, user accounts don't exist in the Kubernetes API server; instead, the API server treats user identities as opaque data. You can authenticate as a user account using multiple methods. Some Kubernetes distributions might add custom extension APIs to represent user accounts in the API server.</p>
<h3 id="use-cases-for-kubernetes-service-accounts">Use cases for Kubernetes Service Accounts </h3>
<p>As a general guideline, you can use service accounts to provide identities in the following scenarios:</p>
<ul>
<li>Your Pods need to communicate with the Kubernetes API server, for example in situations such as the following:
<ul>
<li>Providing read-only access to sensitive information stored in Secrets.</li>
<li>Granting cross-namespace access, such as allowing a Pod in namespace example to read, list, and watch for Lease objects in the kube-node-lease namespace.</li>
</ul>
</li>
<li>Your Pods need to communicate with an external service. For example, a workload Pod requires an identity for a commercially available cloud API, and the commercial provider allows configuring a suitable trust relationship</li>
<li>Authenticating to a private image registry using an imagePullSecret</li>
<li>An external service needs to communicate with the Kubernetes API server. For example, authenticating to the cluster as part of a CI/CD pipeline.</li>
<li>You use third-party security software in your cluster that relies on the ServiceAccount identity of different Pods to group those Pods into different contexts.</li>
</ul>
<h4 id="grant-permissions-to-service-accounts">Grant Permissions to Service Accounts </h4>
<p>You can use RBAC mechanism to grant the minimum permissions required by each service account. You create a role, which grants access, and then bind the role to your ServiceAccount. RBAC lets you define a minimum set of permissions so that the service account permissions follow the principle of least privilege. Pods that use that service account don't get more permissions than are required to function correctly.</p>
<p>For instructions, refer to <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions">ServiceAccount permissions</a>.</p>
<p>To assign a ServiceAccount to a Pod, you set the <code>spec.serviceAccountName</code> field in the Pod specification. Kubernetes then automatically provides the credentials for that ServiceAccount to the Pod. In v1.22 and later, Kubernetes gets a short-lived, automatically rotating token using the TokenRequest API and mounts the token as a projected volume.</p>
<p>By default, Kubernetes provides the Pod with the credentials for an assigned ServiceAccount, whether that is the default ServiceAccount or a custom ServiceAccount that you specify.</p>
<h3 id="admission-control">Admission Control </h3>
<p><strong>Admission controllers</strong>&nbsp;are used to specify granular access control policies, which include allowing privileged containers, checking on resource quota, etc. We force these policies using different admission controllers, like ResourceQuota, DefaultStorageClass, AlwaysPullImages, etc. They come into effect only after API requests are authenticated and authorized.</p>
<p>To use admission controls, we must start the Kubernetes API server with the <code>--enable-admission-plugins</code>, which takes a comma-delimited, ordered list of controller names:</p>
<p><code>--enable-admission-plugins=NamespaceLifecycle,ResourceQuota,PodSecurity,DefaultStorageClass</code></p>
<p>Kubernetes has some admission controllers enabled by default. For more details, please review the&nbsp;<a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do">list of admission controllers</a>.</p>
<p>Kubernetes admission control can also be implemented though custom plugins, for a&nbsp;<a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">dynamic admission control method</a>. These plugins are developed as extensions and run as admission webhooks.</p>
<h1 id="services">Services </h1>
<h3 id="connecting-users-or-applications-to-pods">Connecting Users or Applications to Pods </h3>
<p>To access the application, a user or another application need to connect to the Pods. As Pods</p>
<ul>
<li>are ephemeral in nature, resources like IP addresses allocated to them cannot be static</li>
<li>could be terminated abruptly or be rescheduled based on existing requirements.</li>
</ul>
<p>To overcome this situation, Kubernetes provides a higher-level abstraction called Service, which logically groups Pods and defines a policy to access them. This grouping is achieved via Labels and Selectors. Labels and Selectors use a key-value pair format. In the following graphical representation, app&nbsp;is the Label key,&nbsp;frontend and db are Label values for different Pods.</p>
<p align="center">
    <img src="./assets/k8s/services.png" alt="drawing" width="600" height="400" style="center">
</p>
<p>Services can expose single Pods, ReplicaSets, Deployments, DaemonSets, and StatefulSets. When exposing the Pods managed by an operator, the Service's Selector may use the same label(s) as the operator.</p>
<p>The following is an example of a Service object definition:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> frontend<span class="token punctuation">-</span>svc
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> frontend
  ports<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
    port<span class="token punctuation">:</span> <span class="token number">80</span>
    targetPort<span class="token punctuation">:</span> <span class="token number">5000</span>
</code></pre><p>In this example,&nbsp;we are creating a <code>frontend-svc</code>&nbsp;Service by selecting all the Pods that have the label key=<code>app</code> set to value=<code>frontend</code>. By default, each Service receives an IP address routable only inside the cluster, known as <strong>ClusterIP</strong>. In our example, we have 172.17.0.4 and 172.17.0.5&nbsp;as ClusterIPs assigned to our frontend-svc and db-svc&nbsp;Services, respectively. The user/client now connects to a Service via its ClusterIP, which forwards traffic to one of the Pods attached to it. A Service provides <strong>load balancing</strong> by default while selecting the Pods for traffic forwarding.</p>
<p>While the Service forwards traffic to Pods, we can select the targetPort on the Pod which receives the traffic. In our example, the&nbsp;frontend-svc Service&nbsp;receives requests from the user/client on port:&nbsp;80&nbsp;and then forwards these requests to one of the attached Pods on the targetPort:&nbsp;5000. If the targetPort is not defined explicitly, then traffic will be forwarded to Pods on the port on which the Service receives traffic. It is very important to ensure that the value of the targetPort, which is 5000 in this example, matches the value of the containerPort property of the Pod spec section.<br>
&nbsp;<br>
A logical set of a Pod's IP address, along with the targetPort is referred to as a <strong>Service endpoint</strong>. In our example, the&nbsp;frontend-svc&nbsp;Service has 3 endpoints: <code>10.0.1.3:5000</code>, <code>10.0.1.4:5000</code>, and <code>10.0.1.5:5000</code>. Endpoints are created and managed automatically by the Service, not by the Kubernetes cluster administrator.</p>
<h3 id="kube-proxy">kube-proxy </h3>
<p>Each cluster node runs a daemon called kube-proxy, a node agent that watches the API server on the master node for the addition, updates, and removal of Services and endpoints. kube-proxy is responsible for implementing the Service configuration on behalf of an administrator or developer, in order to enable traffic routing to an exposed application running in Pods.</p>
<p>In the example below, for each new Service, on each node, kube-proxy configures&nbsp;iptables rules to capture the traffic for its ClusterIP and forwards it to one of the Service's endpoints. Therefore any node can receive the external traffic and then route it internally in the cluster based on the iptables rules. When the Service is removed, kube-proxy removes the corresponding&nbsp;iptables rules on all nodes as well.</p>
<p align="center">
    <img src="./assets/k8s/service-ip.png" alt="drawing" width="600" height="400" style="center">
</p>
<h2 id="traffic-policies">Traffic Policies </h2>
<p>The kube-proxy node agent together with the iptables implement the load-balancing mechanism of the Service when traffic is being routed to the application Endpoints. Due to restricting characteristics of the iptables this load-balancing is random by default.&nbsp;This means that the Endpoint Pod to receive the request forwarded by the Service will be randomly selected out of many replicas. This mechanism does not guarantee that the selected receiving Pod is the closest or even on the same node as the requester, therefore not the most efficient mechanism. Since this is the iptables supported load-balancing mechanism, if we desire better outcomes, we would need to take advantage of traffic policies.</p>
<p>Traffic policies&nbsp;allow users to instruct the kube-proxy on the context of the traffic routing. The two options are <strong>Cluster</strong> and <strong>Local</strong>:</p>
<ul>
<li>The Cluster option allows kube-proxy to target all ready Endpoints of the Service in the load-balancing process</li>
<li>The Local option, however, isolates the load-balancing process to only include the Endpoints of the Service located on the same node as the requester Pod. While this sounds like an ideal option, it does have a shortcoming - if the Service does not have a ready Endpoint on the node where the requester Pod is running, the Service will not route the request to Endpoints on other nodes to satisfy the request<br>
&nbsp;<br>
Both the Cluster and Local options are available for requests generated internally from within the cluster, or externally from applications and clients running outside the cluster.</li>
</ul>
<p>As Services are&nbsp;the primary mode of communication between containerized applications managed by Kubernetes, it is helpful to be able to discover them at runtime. Kubernetes supports two methods for discovering Services:</p>
<ul>
<li>
<p>Environment Variables:<br>
As soon as the Pod starts on any worker node, the kubelet daemon running on that node adds a set of environment variables in the Pod for all active Services. For example, if we have an active Service called redis-master, which exposes port 6379, and its ClusterIP is 172.17.0.6, then, on a newly created Pod, we can see the following environment variables:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token assign-left variable">REDIS_MASTER_SERVICE_HOST</span><span class="token operator">=</span><span class="token number">172.17</span>.0.6
<span class="token assign-left variable">REDIS_MASTER_SERVICE_PORT</span><span class="token operator">=</span><span class="token number">6379</span>
<span class="token assign-left variable">REDIS_MASTER_PORT</span><span class="token operator">=</span>tcp://172.17.0.6:6379
<span class="token assign-left variable">REDIS_MASTER_PORT_6379_TCP</span><span class="token operator">=</span>tcp://172.17.0.6:6379
<span class="token assign-left variable">REDIS_MASTER_PORT_6379_TCP_PROTO</span><span class="token operator">=</span>tcp
<span class="token assign-left variable">REDIS_MASTER_PORT_6379_TCP_PORT</span><span class="token operator">=</span><span class="token number">6379</span>
<span class="token assign-left variable">REDIS_MASTER_PORT_6379_TCP_ADDR</span><span class="token operator">=</span><span class="token number">172.17</span>.0.6
</code></pre><p>With this solution, we need to be careful while ordering our Services, as the Pods will not have the environment variables set for Services which are created after the Pods are created. This is not the recommended method to used for service discovery though.</p>
</li>
<li>
<p><strong>DNS</strong>: Kubernetes has an add-on&nbsp;for DNS, which creates a DNS record for each Service and its format is&nbsp;<code>my-svc.my-namespace.svc.cluster.local</code>.&nbsp;Services within the same Namespace find other Services just by their names. If we add a Service redis-master&nbsp;in&nbsp;<code>my-ns</code>&nbsp;Namespace, all Pods in the same <code>my-ns</code> Namespace lookup the&nbsp;Service just by its name, <code>redis-master</code>. Pods from other Namespaces, such as <code>test-ns</code>, lookup the same Service by adding the respective Namespace as a suffix, such as&nbsp;<code>redis-master.my-ns</code>&nbsp;or providing the FQDN of the service as <code>redis-master.my-ns.svc.cluster.local</code>.</p>
<p>This&nbsp;is the most common and highly recommended solution. For example, in the previous section's image, we have&nbsp;seen that an internal DNS is configured, which maps our Services <code>frontend-svc</code>&nbsp;and&nbsp;<code>db-svc</code>&nbsp;to 172.17.0.4&nbsp;and 172.17.0.5&nbsp;IP addresses&nbsp;respectively.</p>
</li>
</ul>
<h3 id="servicetype-clusterip-and-nodeport">ServiceType: ClusterIP and NodePort </h3>
<p>While defining a Service, we can also choose its access scope. We can decide whether the Service:</p>
<ul>
<li>Is only accessible within the cluster</li>
<li>Is accessible from within the cluster and the external world</li>
<li>Maps to an entity which resides either inside or outside the cluster.</li>
</ul>
<p>Access scope is decided by ServiceType property, defined when creating the Service.</p>
<p>ClusterIP is the default ServiceType. A Service receives a Virtual IP address, known as its ClusterIP. This Virtual IP address&nbsp;is used for communicating with the Service and is accessible only from within the cluster.</p>
<p>With the NodePort&nbsp;ServiceType, in addition to a ClusterIP, a high-port, dynamically picked from the default range 30000-32767, is mapped to the respective Service, from all the worker nodes. For example, if the mapped NodePort is 32233&nbsp;for the service <code>frontend-svc</code>, then, if we connect to any worker node on port 32233, the node&nbsp;would redirect all the traffic to the assigned ClusterIP - 172.17.0.4.&nbsp;If we prefer a specific high-port number instead,&nbsp;then we can assign that high-port number to the NodePort from the default range when creating the Service.</p>
<p align="center">
    <img src="./assets/k8s/nodeport.png" alt="drawing" width="700" height="400" style="center">
</p>
<p>The NodePort ServiceType is useful when we want to make our Services accessible from the external world. The end-user connects to any worker node on the specified high-port, which proxies the request internally to the ClusterIP of the Service, then the request is forwarded to the applications&nbsp;running inside the cluster. Let's not forget that the Service is load balancing such requests, and only forwards the request to one of the Pods running the desired application.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> web<span class="token punctuation">-</span>service
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  type<span class="token punctuation">:</span> NodePort
  ports<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
    protocol<span class="token punctuation">:</span> TCP
  selector<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> nginx 
</code></pre><p>To manage access to multiple application Services from the external world, administrators can configure a reverse proxy - an ingress,&nbsp;and define rules that target specific Services within the cluster.</p>
<h3 id="servicetype-loadbalancer">ServiceType: LoadBalancer </h3>
<p>With the <a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer">LoadBalancer&nbsp;ServiceType</a> NodePort and ClusterIP are automatically created, and the external load balancer will route to them. The Service is exposed at a static port on each worker node. The Service is exposed externally using the underlying cloud provider's load balancer feature.</p>
<p align="center">
    <img src="./assets/k8s/loadbalancer.png" alt="drawing" width="700" height="400" style="center">
</p>
<p>NOTE: The LoadBalancer ServiceType will only work if the underlying infrastructure supports the automatic creation of Load Balancers and have the respective support in Kubernetes, as is the case with the Google Cloud Platform and AWS. If no such feature is configured, the LoadBalancer IP address field is not populated, it remains in Pending state, but the Service will still work as a typical NodePort type Service.</p>
<h2 id="liveness-and-readiness-probes">Liveness and Readiness Probes </h2>
<p>While containerized applications are scheduled to run in pods on nodes across our cluster, at times the applications may become unresponsive or may be delayed during startup. Implementing&nbsp;<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Liveness and Readiness Probes</a>&nbsp;allows the kubelet to control the health of the application running inside a Pod's container and force a container restart of an unresponsive application. When defining both Readiness and Liveness Probes, it is recommended to allow enough time for the Readiness Probe to possibly fail a few times before a pass, and only then check the Liveness Probe. If Readiness and Liveness Probes overlap there may be a risk that the container never reaches ready state, being stuck in an infinite re-create - fail loop.</p>
<h3 id="liveness">Liveness </h3>
<p>If a container in the Pod has been running successfully for a while, but the application running inside this container suddenly stopped responding to our requests, then that container is no longer useful to us. This kind of situation can occur, for example, due to application deadlock or memory pressure. In such a case, it is recommended to restart the container to make the application available.</p>
<p>Rather than restarting it manually, we can use a&nbsp;<strong>Liveness Probe</strong>. Liveness Probe checks on an application's health, and if the health check fails, kubelet restarts the affected container automatically.</p>
<p>Liveness Probes can be set by defining:</p>
<ul>
<li>Liveness command</li>
<li>Liveness HTTP request</li>
<li>TCP Liveness probe</li>
</ul>
<p>Liveness Command<br>
In the following example, the liveness command is checking the existence of a file <code>/tmp/healthy</code>:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  labels<span class="token punctuation">:</span>
    test<span class="token punctuation">:</span> liveness
  name<span class="token punctuation">:</span> liveness<span class="token punctuation">-</span>exec
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  containers<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> liveness
    image<span class="token punctuation">:</span> k8s.gcr.io/busybox
    args<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> /bin/sh
    <span class="token punctuation">-</span> <span class="token punctuation">-</span>c
    <span class="token punctuation">-</span> touch /tmp/healthy; sleep 30; rm <span class="token punctuation">-</span>rf /tmp/healthy; sleep 600
    livenessProbe<span class="token punctuation">:</span>
      exec<span class="token punctuation">:</span>
        command<span class="token punctuation">:</span>
        <span class="token punctuation">-</span> cat
        <span class="token punctuation">-</span> /tmp/healthy
      initialDelaySeconds<span class="token punctuation">:</span> <span class="token number">15</span>
      failureThreshold<span class="token punctuation">:</span> <span class="token number">1</span>
      periodSeconds<span class="token punctuation">:</span> <span class="token number">5</span>
</code></pre><p>The existence of the&nbsp;<code>/tmp/healthy</code> file is configured to be checked every 5 seconds using the&nbsp;<code>periodSeconds</code> parameter. The <code>initialDelaySeconds</code> parameter requests the kubelet to wait for 15 seconds before the first probe. When running the command line argument to the container, we will first create the <code>/tmp/healthy</code> file, and then we will remove it after 30 seconds. The removal of the file would trigger a probe failure, while the failureThreshold parameter set to 1 instructs kubelet to declare the container unhealthy after a single probe failure and trigger a container restart as a result.</p>
<h3 id="liveness-http-request">Liveness HTTP Request </h3>
<p>In the following example, the kubelet sends the HTTP GET&nbsp;request to the&nbsp;/healthz endpoint of the application, on port 8080. If that returns a failure, then the kubelet will restart the affected container; otherwise, it would consider the application to be alive:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token punctuation">...</span>
     livenessProbe<span class="token punctuation">:</span>
       httpGet<span class="token punctuation">:</span>
         path<span class="token punctuation">:</span> /healthz
         port<span class="token punctuation">:</span> <span class="token number">8080</span>
         httpHeaders<span class="token punctuation">:</span>
         <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> X<span class="token punctuation">-</span>Custom<span class="token punctuation">-</span>Header
           value<span class="token punctuation">:</span> Awesome
       initialDelaySeconds<span class="token punctuation">:</span> <span class="token number">15</span>
       periodSeconds<span class="token punctuation">:</span> <span class="token number">5</span>
</code></pre><h3 id="tcp-liveness-probe">TCP Liveness Probe </h3>
<p>With TCP Liveness Probe, the kubelet attempts to open the&nbsp;TCP Socket to the container which is running the application. If it succeeds, the application is considered healthy, otherwise the kubelet would mark it as unhealthy and restart the affected container.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token punctuation">...</span>
    livenessProbe<span class="token punctuation">:</span>
      tcpSocket<span class="token punctuation">:</span>
        port<span class="token punctuation">:</span> <span class="token number">8080</span>
      initialDelaySeconds<span class="token punctuation">:</span> <span class="token number">15</span>
      periodSeconds<span class="token punctuation">:</span> <span class="token number">5</span>
</code></pre><h3 id="readiness-probes">Readiness Probes </h3>
<p>Sometimes, while initializing, applications have to meet certain conditions before they become ready to serve traffic. These conditions include ensuring that the dependent service is ready, or acknowledging that a large dataset needs to be loaded, etc. In such cases, we use <strong>Readiness Probes</strong>&nbsp;and wait for a certain condition to occur. Only then, the application can serve traffic.<br>
A Pod with containers that do not report ready status will not receive traffic from Kubernetes Services.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token punctuation">...</span>
    readinessProbe<span class="token punctuation">:</span>
          exec<span class="token punctuation">:</span>
            command<span class="token punctuation">:</span>
            <span class="token punctuation">-</span> cat
            <span class="token punctuation">-</span> /tmp/healthy
          initialDelaySeconds<span class="token punctuation">:</span> 5 
          periodSeconds<span class="token punctuation">:</span> <span class="token number">5</span>
<span class="token punctuation">...</span>
</code></pre><p>Readiness Probes are configured similarly to Liveness Probes. Their configuration also remains the same.</p>
<h1 id="kubernetes-volume-management">Kubernetes Volume Management </h1>
<p>As we know, containers running in Pods are ephemeral in nature. All data stored inside a container is deleted if the container crashes. However, the&nbsp;kubelet will restart it with a clean state, which means that it will not have any of the old data.</p>
<p>To overcome this problem, Kubernetes uses <strong>Volumes</strong>, storage abstractions that allow various storage technologies to be used by Kubernetes and offered to containers in Pods as storage media. A Volume is essentially a mount point on the container's file system backed by a storage medium. The storage medium, content and access mode are determined by the Volume Type. In Kubernetes, a Volume is linked to a Pod and can be shared among the containers of that Pod. Although the Volume has the same life span as the Pod, meaning that it is deleted together with the Pod, the Volume outlives the containers of the Pod - this allows data to be preserved across container restarts.</p>
<h2 id="persistentvolumes">PersistentVolumes </h2>
<p>In a typical IT environment, storage is managed by the storage/system administrators. The end user will just receive instructions to use the storage but is not involved with the underlying storage management.</p>
<p>In the containerized world, we would like to follow similar rules, but it becomes challenging, given the many Volume Types we have seen earlier. Kubernetes resolves this problem with the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolume</a> (PV)&nbsp;subsystem, which provides APIs for users and administrators to manage and consume persistent storage. To manage the Volume, it uses the PersistentVolume&nbsp;API resource type, and to consume it, it uses the PersistentVolumeClaim API resource type.</p>
<p>A Persistent Volume is a storage abstraction backed by several storage technologies, which could be local to the host where the Pod is deployed with its application container(s), network attached storage, cloud storage, or a distributed storage solution. A Persistent Volume&nbsp;is statically provisioned by the cluster administrator.</p>
<p align="center">
    <img src="./assets/k8s/persistant-vol.png" alt="drawing" width="600" height="300" style="center">
</p>
<p>PersistentVolumes can be&nbsp;<a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/">dynamically</a> provisioned&nbsp;based on&nbsp;the StorageClass resource. A StorageClass contains predefined provisioners and parameters to create a PersistentVolume. Using PersistentVolumeClaims, a user sends the request for dynamic PV creation, which gets wired to the StorageClass resource.</p>
<h3 id="persistent-volume-claims">Persistent Volume Claims </h3>
<p>A PersistentVolumeClaim (PVC)&nbsp;is a request for storage by a user. Users request for PersistentVolume resources based on storage class, access mode, size, and optionally volume mode. There are four access modes: <code>ReadWriteOnce</code> (read-write by a single node), <code>ReadOnlyMany</code> (read-only by many nodes), <code>ReadWriteMany</code> (read-write by many nodes), and <code>ReadWriteOncePod</code> (read-write by a single pod). The optional volume modes, filesystem or block device, allow volumes to be mounted into a pod's directory or as a raw block device respectively. Once a suitable PersistentVolume is found, it is bound to a PersistentVolumeClaim.</p>
<p>After a successful bound, the PersistentVolumeClaim resource can be used by the containers of the Pod.</p>
<p>Once a user finishes its work, the attached PersistentVolumes can be released. The underlying PersistentVolumes can then be reclaimed (for an admin to verify and/or aggregate data), deleted (both data and volume are deleted), or recycled for future usage (only data is deleted), based on the configured persistentVolumeReclaimPolicy property.</p>
<h3 id="container-storage-interface-csi">Container Storage Interface (CSI) </h3>
<p>Container orchestrators like Kubernetes, Mesos, Docker or Cloud Foundry used to have their own methods of managing external storage using Volumes. For storage vendors, it was challenging to manage different Volume plugins for different orchestrators. A maintainability challenge for Kubernetes as well, it involved in-tree storage plugins integrated into the orchestrator's source code. Storage vendors and community members from different orchestrators started working together to standardize the Volume interface - a volume plugin built using a standardized Container Storage Interface (CSI) designed to work on different container orchestrators with a variety of storage providers. Explore the CSI specifications&nbsp;for more details.</p>
<p>Between Kubernetes releases v1.9 and v1.13 CSI matured from alpha to stable support, which makes installing new CSI-compliant Volume drivers very easy. With CSI, third-party storage providers can develop solutions without the need to add them into the core Kubernetes codebase. These solutions are CSI drivers installed only when required by cluster administrators.</p>
<h4 id="using-a-shared-hostpath-volume-type-demo-guide">Using a Shared hostPath Volume Type Demo Guide </h4>
<p>This exercise guide includes a Deployment definition manifest that can be used as a template to define other similar objects as needed. In addition to the volumes and volume mounts specified for each container, a command stanza allows us to run a desired command in one of the containers. The debian container's shell command line interpreter (sh) is invoked to run the echo and sleep commands (-c).</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">vim</span> app-blue-shared-vol.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: blue-app
  name: blue-app
spec:
  replicas: <span class="token number">1</span>
  selector:
    matchLabels:
      app: blue-app
  strategy: <span class="token punctuation">{</span><span class="token punctuation">}</span>
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: blue-app
        type: canary
    spec:
      volumes:
      - name: host-volume
        hostPath:
          path: /home/docker/blue-shared-volume
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: <span class="token number">80</span>
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: host-volume
      - image: debian
        name: debian
        volumeMounts:
        - mountPath: /host-vol
          name: host-volume
        command: <span class="token punctuation">[</span><span class="token string">"/bin/sh"</span>, <span class="token string">"-c"</span>, <span class="token string">"echo Welcome to BLUE App! &gt; /host-vol/index.html ; sleep infinity"</span><span class="token punctuation">]</span>
status: <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre><h1 id="configmaps-and-secrets">ConfigMaps and Secrets </h1>
<p>ConfigMaps allow us to decouple the configuration details from the container image. Using ConfigMaps, we pass configuration data as key-value pairs, which are consumed by Pods or any other system components and controllers, in the form of environment variables, sets of commands and arguments, or volumes. We can create ConfigMaps from literal values, from configuration files, from one or more files or directories.</p>
<p>Create the ConfigMap:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl create configmap my-config <span class="token punctuation">\</span>
  --from-literal<span class="token operator">=</span>key1<span class="token operator">=</span>value1 <span class="token punctuation">\</span>
  --from-literal<span class="token operator">=</span>key2<span class="token operator">=</span>value2
configmap/my-config created 
</code></pre><p>Display the ConfigMap details for my-config:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get configmaps my-config <span class="token parameter variable">-o</span> yaml

apiVersion: v1
data:
  key1: value1
  key2: value2
kind: ConfigMap
metadata:
  creationTimestamp: <span class="token number">2022</span>-04-02T07:21:55Z
  name: my-config
  namespace: default
  resourceVersion: <span class="token string">"241345"</span>
  selfLink: /api/v1/namespaces/default/configmaps/my-config
  uid: d35f0a3d-45d1-11e7-9e62-080027a46057
</code></pre><p>With the <code>-o</code> yaml option, we are requesting the kubectl command to produce the output in the YAML format. As we can see, the object has the&nbsp;ConfigMap kind, and it has the key-value pairs inside the data field. The name of ConfigMap and other details are part of the metadata field.</p>
<h3 id="use-configmaps-inside-pods-as-environment-variables">Use ConfigMaps Inside Pods: As Environment Variables </h3>
<p>Inside a Container, we can retrieve the key-value data of an entire ConfigMap or the values of specific ConfigMap keys as environment variables.</p>
<p>In the following example all the myapp-full-container Container's environment variables receive the values of the <code>full-config-map</code>&nbsp;ConfigMap keys:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token punctuation">...</span>
  containers<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp<span class="token punctuation">-</span>full<span class="token punctuation">-</span>container
    image<span class="token punctuation">:</span> myapp
    envFrom<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">configMapRef</span><span class="token punctuation">:</span>
      name<span class="token punctuation">:</span> full<span class="token punctuation">-</span>config<span class="token punctuation">-</span>map
<span class="token punctuation">...</span>
</code></pre><p>In the following example the myapp-specific-container Container's environment variables receive their values from specific key-value pairs from two separate ConfigMaps, config-map-1 and config-map-2:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token punctuation">...</span>
  containers<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp<span class="token punctuation">-</span>specific<span class="token punctuation">-</span>container
    image<span class="token punctuation">:</span> myapp
    env<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SPECIFIC_ENV_VAR1
      valueFrom<span class="token punctuation">:</span>
        configMapKeyRef<span class="token punctuation">:</span>
          name<span class="token punctuation">:</span> config<span class="token punctuation">-</span>map<span class="token punctuation">-</span><span class="token number">1</span>
          key<span class="token punctuation">:</span> SPECIFIC_DATA
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SPECIFIC_ENV_VAR2
      valueFrom<span class="token punctuation">:</span>
        configMapKeyRef<span class="token punctuation">:</span>
          name<span class="token punctuation">:</span> config<span class="token punctuation">-</span>map<span class="token punctuation">-</span><span class="token number">2</span>
          key<span class="token punctuation">:</span> SPECIFIC_INFO
<span class="token punctuation">...</span>
</code></pre><p>With the configuration presented above, we will get the <code>SPECIFIC_ENV_VAR1</code>&nbsp;environment variable set to the value of <code>SPECIFIC_DATA</code> key from <code>config-map-1</code> ConfigMap, and&nbsp;<code>SPECIFIC_ENV_VAR2</code>&nbsp;environment variable set to the value of <code>SPECIFIC_INFO</code> key from <code>config-map-2</code> ConfigMap.</p>
<h4 id="using-configmaps-as-volumes-demo-guide">Using ConfigMaps as Volumes Demo Guide </h4>
<p>This exercise includes an <code>index.html</code> file and a Deployment definition manifest that can be used as templates to define other similar objects as needed. The goal of the demo is to store the custom webserver <code>index.html</code> file in a ConfigMap object, which is mounted by the nginx container specified by the Pod template nested in the Deployment definition manifest.</p>
<p>The webserver index file:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">vim</span> index.html

<span class="token operator">&lt;</span><span class="token operator">!</span>DOCTYPE html<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>html<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>head<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>title<span class="token operator">&gt;</span>Welcome to GREEN App<span class="token operator">!</span><span class="token operator">&lt;</span>/title<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>style<span class="token operator">&gt;</span>
    body <span class="token punctuation">{</span>
        width: 35em<span class="token punctuation">;</span>
        margin: <span class="token number">0</span> auto<span class="token punctuation">;</span>
        font-family: Tahoma, Verdana, Arial, sans-serif<span class="token punctuation">;</span>
        background-color: GREEN<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token operator">&lt;</span>/style<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/head<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>body<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>h1 <span class="token assign-left variable">style</span><span class="token operator">=</span><span class="token punctuation">\</span>"text-align: center<span class="token punctuation">;</span><span class="token punctuation">\</span>"<span class="token operator">&gt;</span>Welcome to GREEN App<span class="token operator">!</span><span class="token operator">&lt;</span>/h<span class="token operator"><span class="token file-descriptor important">1</span>&gt;</span>
<span class="token operator">&lt;</span>/body<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/html<span class="token operator">&gt;</span>
</code></pre><p>The Deployment definition file:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">vim</span> web-green-with-cm.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: green-web
  name: green-web
spec:
  replicas: <span class="token number">1</span>
  selector:
    matchLabels:
      app: green-web
  strategy: <span class="token punctuation">{</span><span class="token punctuation">}</span>
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: green-web
    spec:
      volumes:
      - name: web-config
        configMap:
          name: green-web-cm
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: <span class="token number">80</span>
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: web-config
status: <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre><h2 id="secrets">Secrets </h2>
<p>Let's assume that we have a Wordpress blog application, in which our wordpress frontend connects to the MySQL database backend using a password. While creating the Deployment for wordpress, we can include the MySQL password in the Deployment's YAML file, but the password would not be protected. The password would be available to anyone who has access to the configuration file.</p>
<p>In this scenario, the Secret&nbsp;object can help by allowing us to encode the sensitive information before sharing it. With Secrets, we can share sensitive information like passwords, tokens, or keys in the form of key-value pairs, similar to ConfigMaps; thus, we can control how the information in a Secret is used, reducing the risk for accidental exposures. In Deployments or other resources, the Secret object is&nbsp;referenced, without exposing its content.</p>
<p>It is important to keep in mind that by default, the Secret data is stored as plain text inside&nbsp;etcd, therefore administrators must limit access to the API server and&nbsp;etcd. However, Secret data can be encrypted at rest while it is stored in etcd, but this feature needs to be enabled at the API server level.</p>
<p>To create a Secret, we can use the&nbsp;kubectl create secret command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl create secret generic my-password <span class="token punctuation">\</span>
  --from-literal<span class="token operator">=</span>password<span class="token operator">=</span>mysqlpassword
</code></pre><p>The above&nbsp;command would create a&nbsp;secret called&nbsp;<code>my-password</code>, which&nbsp;has the value of the&nbsp;password&nbsp;key set to <code>mysqlpassword</code>. After successfully creating a secret, we can analyze it with the get and describe commands. They do not reveal the content of the Secret. The type is listed as Opaque.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get secret my-password
NAME          TYPE     DATA   AGE
my-password   Opaque   <span class="token number">1</span>      8m
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl describe secret my-password
Name:          my-password
Namespace:     default
Labels:        <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Annotations:   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Type Opaque
Data
<span class="token operator">==</span><span class="token operator">==</span>
password: <span class="token number">13</span> bytes
</code></pre><p>We can create a Secret manually from a YAML definition manifest. The example manifest below is named <code>mypass.yaml</code>. There are two types of maps for sensitive information inside a Secret: data and stringData. With data maps, each value of a sensitive information field must be encoded using <code>base64</code>. If we want to have a definition manifest for our Secret, we must first create the <code>base64</code> encoding of our password:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token builtin class-name">echo</span> mysqlpassword <span class="token operator">|</span> base64
<span class="token assign-left variable">bXlzcWxwYXNzd29yZAo</span><span class="token operator">=</span>
</code></pre><p>and then use it in the definition manifest:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> my<span class="token punctuation">-</span>password
<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque
<span class="token key atrule">data</span><span class="token punctuation">:</span>
  password<span class="token punctuation">:</span> bXlzcWxwYXNzd29yZAo=
</code></pre><p>Please note that base64 encoding does not mean encryption, and anyone can easily decode our encoded data:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token builtin class-name">echo</span> <span class="token string">"bXlzcWxwYXNzd29yZAo="</span> <span class="token operator">|</span> base64 <span class="token parameter variable">--decode</span>
mysqlpassword
</code></pre><p>Therefore, make sure you do not commit a Secret's definition file in the source code.</p>
<p>With stringData maps, there is no need to encode the value of each sensitive information field. The value of the sensitive field will be encoded when the my-password Secret is created:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> my<span class="token punctuation">-</span>password
<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque
<span class="token key atrule">stringData</span><span class="token punctuation">:</span>
  password<span class="token punctuation">:</span> mysqlpassword
</code></pre><p>Using the <code>mypass.yaml</code>&nbsp;definition file we can now create a secret with kubectl create command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl create <span class="token parameter variable">-f</span> mypass.yaml
secret/my-password created
</code></pre><p>To create a Secret from a File, we can use the&nbsp;kubectl create secret&nbsp;command.&nbsp;<br>
First, we encode the sensitive data and then we write the encoded data to a text file:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token builtin class-name">echo</span> mysqlpassword <span class="token operator">|</span> base64
<span class="token assign-left variable">bXlzcWxwYXNzd29yZAo</span><span class="token operator">=</span>
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token builtin class-name">echo</span> <span class="token parameter variable">-n</span> <span class="token string">'bXlzcWxwYXNzd29yZAo='</span> <span class="token operator">&gt;</span> password.txt
</code></pre><p>Now we can create the Secret from the password.txt file:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl create secret generic my-file-password <span class="token punctuation">\</span>
  --from-file<span class="token operator">=</span>password.txt
secret/my-file-password created
</code></pre><p>After successfully creating a secret we can analyze it with the get and describe&nbsp;commands. They do not reveal the content of the Secret. The type is listed as&nbsp;Opaque.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get secret my-file-password
NAME               TYPE     DATA   AGE 
my-file-password   Opaque   <span class="token number">1</span>      8m
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl describe secret my-file-password
Name:          my-file-password
Namespace:     defaultLabels:        <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Annotations:   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Type  Opaque
Data
<span class="token operator">==</span><span class="token operator">==</span>
password.txt:  <span class="token number">13</span> bytes 
</code></pre><h4 id="use-secrets-inside-pods-as-environment-variables">Use Secrets Inside Pods: As Environment Variables </h4>
<p>Secrets are consumed by Containers in Pods as mounted data volumes, or as environment variables, and are referenced in their entirety or specific key-values.<br>
Below we reference only the password key of the my-password Secret and assign its value to the&nbsp;WORDPRESS_DB_PASSWORD&nbsp;environment variable:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token punctuation">...</span>.
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  containers<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> wordpress<span class="token punctuation">:</span>4.7.3<span class="token punctuation">-</span>apache
    name<span class="token punctuation">:</span> wordpress
    env<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> WORDPRESS_DB_PASSWORD
      valueFrom<span class="token punctuation">:</span>
        secretKeyRef<span class="token punctuation">:</span>
          name<span class="token punctuation">:</span> my<span class="token punctuation">-</span>password
          key<span class="token punctuation">:</span> password
….
</code></pre><h3 id="use-secrets-inside-pods-as-volumes">Use Secrets Inside Pods: As Volumes </h3>
<p>We can also mount a Secret as a Volume inside a Pod. The following example creates a file for each my-password Secret key (where the files are named after the names of the keys), the files containing the values of the respective Secret keys:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token punctuation">..</span><span class="token punctuation">..</span>
spec:
  containers:
  - image: wordpress:4.7.3-apache
    name: wordpress
    volumeMounts:
    - name: secret-volume
      mountPath: <span class="token string">"/etc/secret-data"</span>
      readOnly: <span class="token boolean">true</span>
  volumes:
  - name: secret-volume
    secret:
      secretName: my-password
<span class="token punctuation">..</span><span class="token punctuation">..</span>
</code></pre><h1 id="ingress">Ingress </h1>
<p>In an earlier chapter, we saw how we can access our deployed containerized application from the external world via Services. Among the ServiceTypes&nbsp;the NodePort and LoadBalancer are the most often used. For the LoadBalancer ServiceType, we need to have support from the underlying infrastructure. Even after having the support, we may not want to use it for every Service, as LoadBalancer resources are&nbsp;limited and they can increase costs significantly. Managing the NodePort ServiceType can also be tricky at times, as we need to keep updating our proxy settings and keep track of the assigned ports.</p>
<p>In this chapter, we will explore the Ingress&nbsp;API resource, which represents another layer of abstraction, deployed in front of the Service API resources, offering a unified method of managing access to our applications from the external world.</p>
<p>With Services, routing rules are associated with a given Service. They exist for as long as the Service exists, and there are many rules because there are many Services in the cluster. If we can somehow decouple the routing rules from the application and centralize the rules management, we can then update our application without worrying about its external access. This can be done using the Ingress resource.</p>
<p>According to <a href="http://kubernetes.io">kubernetes.io</a>, "An Ingress is a collection of rules that allow inbound connections to reach the cluster Services". To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following:</p>
<ul>
<li>TLS (Transport Layer Security)</li>
<li>Name-based virtual hosting</li>
<li>Fanout routing</li>
<li>Loadbalancing</li>
<li>Custom rules</li>
</ul>
<p>With Ingress, users do not connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the desired Service. You can see an example of a Name-Based Virtual Hosting Ingress definition below:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.k8s.io/v1 
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress 
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  annotations<span class="token punctuation">:</span>
    kubernetes.io/ingress.class<span class="token punctuation">:</span> <span class="token string">"nginx"</span>
  name<span class="token punctuation">:</span> virtual<span class="token punctuation">-</span>host<span class="token punctuation">-</span>ingress 
  namespace<span class="token punctuation">:</span> default 
<span class="token key atrule">spec</span><span class="token punctuation">:</span> 
  rules<span class="token punctuation">:</span> 
  <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> blue.example.com 
    http<span class="token punctuation">:</span> 
      paths<span class="token punctuation">:</span> 
      <span class="token punctuation">-</span> <span class="token key atrule">backend</span><span class="token punctuation">:</span> 
          service<span class="token punctuation">:</span> 
            name<span class="token punctuation">:</span> webserver<span class="token punctuation">-</span>blue<span class="token punctuation">-</span>svc 
            port<span class="token punctuation">:</span> 
              number<span class="token punctuation">:</span> <span class="token number">80</span> 
        path<span class="token punctuation">:</span> / 
        pathType<span class="token punctuation">:</span> ImplementationSpecific 
  <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> green.example.com 
    http<span class="token punctuation">:</span> 
      paths<span class="token punctuation">:</span> 
      <span class="token punctuation">-</span> <span class="token key atrule">backend</span><span class="token punctuation">:</span> 
          service<span class="token punctuation">:</span> 
            name<span class="token punctuation">:</span> webserver<span class="token punctuation">-</span>green<span class="token punctuation">-</span>svc 
            port<span class="token punctuation">:</span> 
              number<span class="token punctuation">:</span> <span class="token number">80</span> 
        path<span class="token punctuation">:</span> / 
        pathType<span class="token punctuation">:</span> ImplementationSpecific
</code></pre><p>In the example above, user requests to both <code>blue.example.com</code> and <code>green.example.com</code> would go to the same Ingress endpoint, and, from there, they would be forwarded to <code>webserver-blue-svc</code>, and <code>webserver-green-svc</code>, respectively.<br>
This diagram presents a Name-Based Virtual Hosting Ingress rule:</p>
<p align="center">
    <img src="./assets/k8s/ingress-namebase.png" alt="drawing" width="500" height="200" style="center">
</p>
<p>We can also&nbsp;define&nbsp;Fanout Ingress rules, presented in the example definition and the diagram below, when requests to&nbsp;<a href="http://example.com/blue">example.com/blue</a> and <a href="http://example.com/green">example.com/green</a>&nbsp;would be forwarded to&nbsp;webserver-blue-svc&nbsp;and&nbsp;webserver-green-svc, respectively:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.k8s.io/v1 
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress 
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  annotations<span class="token punctuation">:</span>
    kubernetes.io/ingress.class<span class="token punctuation">:</span> <span class="token string">"nginx"</span>
  name<span class="token punctuation">:</span> fan<span class="token punctuation">-</span>out<span class="token punctuation">-</span>ingress 
  namespace<span class="token punctuation">:</span> default 
<span class="token key atrule">spec</span><span class="token punctuation">:</span> 
  rules<span class="token punctuation">:</span> 
  <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> example.com 
    http<span class="token punctuation">:</span> 
      paths<span class="token punctuation">:</span> 
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /blue
        backend<span class="token punctuation">:</span> 
          service<span class="token punctuation">:</span> 
            name<span class="token punctuation">:</span> webserver<span class="token punctuation">-</span>blue<span class="token punctuation">-</span>svc 
            port<span class="token punctuation">:</span> 
              number<span class="token punctuation">:</span> <span class="token number">80</span> 
        pathType<span class="token punctuation">:</span> ImplementationSpecific
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /green
        backend<span class="token punctuation">:</span> 
          service<span class="token punctuation">:</span> 
            name<span class="token punctuation">:</span> webserver<span class="token punctuation">-</span>green<span class="token punctuation">-</span>svc 
            port<span class="token punctuation">:</span> 
              number<span class="token punctuation">:</span> <span class="token number">80</span> 
        pathType<span class="token punctuation">:</span> ImplementationSpecific
</code></pre><p align="center">
    <img src="./assets/k8s/ingress-fanout.png" alt="drawing" width="500" height="200" style="center">
</p>
<p>The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an <strong>Ingress Controller</strong>, which is a reverse proxy responsible for traffic routing based on rules defined in the Ingress resource.</p>
<h3 id="ingress-controller">Ingress Controller </h3>
<p>An Ingress Controller is an application watching the Control Plane Node's API server for changes in the Ingress resources and updates the Layer 7 Load Balancer accordingly. Ingress Controllers are also known as Controllers, <strong>Ingress Proxy</strong>, <strong>Service Proxy</strong>, <strong>Reverse Proxy</strong>, etc. Kubernetes supports an array of Ingress Controllers, and, if needed, we can also build our own. GCE L7 Load Balancer Controller&nbsp;and Nginx Ingress Controller&nbsp;are commonly used Ingress Controllers. Other controllers are Contour, HAProxy Ingress, Istio Ingress, Kong, Traefik, etc. In order to ensure that the ingress controller is watching its corresponding ingress resource, the ingress resource definition manifest needs to include an ingress class annotation with the name of the desired controller <code>kubernetes.io/ingress.class: "nginx"</code> (for an nginx ingress controller).</p>
<h4 id="deploy-an-ingress-resource">Deploy an Ingress Resource </h4>
<p>Once the Ingress Controller is deployed, we can create an Ingress resource using the kubectl create command. For example, if we create a <code>virtual-host-ingress.yaml</code> file with the Name-Based Virtual Hosting Ingress rule definition that we saw in the Ingress (2)&nbsp;section, then we use the following command to create an Ingress resource:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl create <span class="token parameter variable">-f</span> virtual-host-ingress.yaml
</code></pre><h4 id="access-services-using-ingress">Access Services Using Ingress </h4>
<p>With the Ingress resource we just created, we should now be able to access the&nbsp;<code>webserver-blue-svc</code>&nbsp;or <code>webserver-green-svc</code>&nbsp;services using the&nbsp;<code>blue.example.com</code> and <code>green.example.com</code> URLs. As our current setup is on Minikube, we will need to update the host configuration file (<code>/etc/hosts</code> on Mac and Linux) on our workstation to the Minikube IP for those URLs. After the update, the file should look similar to:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">vim</span> /etc/hosts
<span class="token number">127.0</span>.0.1        localhost
::1              localhost
<span class="token number">192.168</span>.99.100   blue.example.com green.example.com 
</code></pre><p>Now we can open <code>blue.example.com</code> and <code>green.example.com</code> on the browser and access each application.</p>
<h1 id="advanced-topics">Advanced Topics </h1>
<h3 id="annotations">Annotations </h3>
<p>With Annotations, we can attach arbitrary non-identifying metadata to any objects, in a key-value format:</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>"annotations": {
&nbsp; "key1" : "value1",
&nbsp; "key2" : "value2"
}
</code></pre><p>Unlike Labels, annotations are not used to identify and select objects. Annotations can be used to:</p>
<ul>
<li>Store build/release IDs, PR numbers, git branch, etc.</li>
<li>Phone/pager numbers of people responsible, or directory entries specifying where such information can be found</li>
<li>Pointers to logging, monitoring, analytics, audit repositories, debugging tools, etc</li>
<li>Ingress controller information</li>
<li>Deployment state and revision information</li>
</ul>
<p>For example, while creating a Deployment, we can add a description as seen below:</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>apiVersion: apps/v1
kind: Deployment
metadata:
&nbsp; name: webserver
&nbsp; annotations:
&nbsp; &nbsp; description: Deployment based PoC dates 2nd Mar'2022
....
</code></pre><p>Annotations are displayed while describing an object:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl describe deployment webserver
Name:                webserver
Namespace:           default
CreationTimestamp:   Fri, <span class="token number">25</span> Mar <span class="token number">2022</span> 05:10:38 +0530
Labels:              <span class="token assign-left variable">app</span><span class="token operator">=</span>webserver
Annotations:         deployment.kubernetes.io/revision<span class="token operator">=</span><span class="token number">1</span>
                     <span class="token assign-left variable">description</span><span class="token operator">=</span>Deployment based PoC dates 2nd Mar'2022
<span class="token punctuation">..</span>.
</code></pre><p>As another example, we use annotation in an ingress resource to specify route matching:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code>  <span class="token key atrule">annotations</span><span class="token punctuation">:</span>
    <span class="token key atrule">nginx.ingress.kubernetes.io/rewrite-target</span><span class="token punctuation">:</span> /$1
</code></pre><p>If you haven't defined any capture groups in your rules,  this directive is rewriting all requests to /. If a client asks for <code>/foo/tea/darjeeling</code>, you request /. If a client requests <code>/foo/this/does/not/exist</code>, you request <code>/.</code></p>
<h3 id="quota-and-limits-management">Quota and Limits Management </h3>
<p>When there are many users sharing&nbsp;a given Kubernetes cluster, there is always a concern for fair usage. A user should not take undue advantage. To address this concern, administrators can use the <strong>ResourceQuota</strong>&nbsp;API resource, which provides constraints that limit aggregate resource consumption per Namespace.</p>
<p>We can set the following types of quotas per Namespace:</p>
<ul>
<li>Compute Resource Quota: we can limit the total sum of compute resources (CPU, memory, etc.) that can be requested in a given&nbsp;Namespace</li>
<li>Storage Resource Quota: we can limit the&nbsp;total sum of storage resources (PersistentVolumeClaims, requests.storage, etc.) that can be requested</li>
<li>Object Count Quota: we can restrict the&nbsp;number of objects of a given type (pods, ConfigMaps, PersistentVolumeClaims, ReplicationControllers, Services, Secrets, etc.)</li>
</ul>
<p>An additional resource that helps limit resources allocation to pods and containers in a namespace, is the <strong>LimitRange</strong>, used in conjunction with the ResourceQuota API resource. A LimitRange&nbsp;can:</p>
<ul>
<li>Set compute resources usage limits per Pod or Container in a namespace.</li>
<li>Set&nbsp;storage request limits per PersistentVolumeClaim in a namespace.</li>
<li>Set a request to limit ratio for a resource in a namespace.</li>
<li>Set default requests and limits and automatically inject them into Containers' environments at runtime.</li>
</ul>
<h3 id="autoscaling">Autoscaling </h3>
<p>While it is fairly easy to manually scale a few Kubernetes objects, this may not be a practical solution for a production-ready cluster where hundreds or thousands of objects are deployed. We need a dynamic scaling solution which adds or removes objects from the cluster based on resource utilization, availability, and requirements.</p>
<p>Autoscaling can be implemented in a Kubernetes cluster via controllers which periodically adjust the number of running objects based on single, multiple, or custom metrics. There are various types of autoscalers available in Kubernetes which can be implemented individually or combined for a more robust autoscaling solution:</p>
<ul>
<li><strong>Horizontal Pod Autoscaler</strong> (HPA): HPA is an algorithm-based controller&nbsp;API resource which automatically adjusts the number of replicas in a ReplicaSet, Deployment or Replication Controller based on CPU utilization</li>
<li><strong>Vertical Pod Autoscaler</strong> (VPA): VPA automatically sets Container resource requirements (CPU and memory) in a Pod and dynamically adjusts them in runtime, based on historical utilization data, current resource availability and real-time events</li>
<li><strong>Cluster Autoscaler</strong>: Cluster Autoscaler automatically&nbsp;re-sizes the Kubernetes cluster when there are insufficient resources available for new Pods expecting to be scheduled or when there are underutilized nodes in the cluster</li>
</ul>
<h3 id="jobs-and-cronjobs">Jobs and CronJobs </h3>
<p>A Job&nbsp;creates one or more Pods to perform a given task. The Job object takes the responsibility of Pod failures. It makes sure that the given task is completed successfully. Once the task is complete, all the Pods have terminated automatically.</p>
<p>Job configuration options include:</p>
<ul>
<li>parallelism - to set the number of pods allowed to run in parallel</li>
<li>completions - to set the number of expected completions</li>
<li>activeDeadlineSeconds - to set the duration of the Job</li>
<li>backoffLimit - to set the number of retries before Job is marked as failed</li>
<li>ttlSecondsAfterFinished - to delay the cleanup of the finished Jobs</li>
</ul>
<p>Starting with the Kubernetes 1.4 release, we can also perform Jobs at scheduled times/dates with&nbsp;<code>CronJobs</code>,&nbsp;where a new Job object is created about once per each execution cycle. The <code>CronJob</code> configuration options include:</p>
<ul>
<li>startingDeadlineSeconds - to set the deadline to start a Job if scheduled time was missed</li>
<li>concurrencyPolicy - to allow or forbid concurrent Jobs or to replace old Jobs with new ones</li>
</ul>
<h3 id="statefulsets">StatefulSets </h3>
<p>The&nbsp;<a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> controller is used for stateful applications which require a unique identity, such as name, network identifications, or strict ordering. For example, MySQL cluster, etcd cluster.</p>
<p>The StatefulSet controller provides identity and guaranteed ordering of deployment and scaling to Pods. However, the StatefulSet controller has very strict Service and Storage Volume dependencies that make it challenging to configure and deploy. It also supports scaling, rolling updates, and rollbacks.</p>
<h3 id="network-policies">Network Policies </h3>
<p>Kubernetes was designed to allow all Pods to communicate freely, without restrictions, with all other Pods in cluster Namespaces. In time it became clear that it was not an ideal design, and mechanisms needed to be put in place in order to restrict communication between certain Pods and applications in the cluster Namespace.&nbsp;Network Policies&nbsp;are sets of rules which define how Pods are allowed to talk to other Pods and resources inside and outside the cluster.</p>
<p>Pods not covered by any Network Policy will continue to receive unrestricted traffic from any endpoint.</p>
<p>Network Policies are very similar to typical Firewalls. They are designed to protect mostly assets located inside the Firewall but can restrict outgoing traffic as well based on sets of rules and policies.<br>
&nbsp;<br>
The Network Policy API resource specifies podSelectors, Ingress and/or Egress policyTypes, and rules based on source and destination ipBlocks and ports. Very simplistic default allow or default deny policies can be defined as well. As a good practice, it is recommended to define a default deny policy to block all traffic to and from the Namespace, and then define sets of rules for specific traffic to be allowed in and out of the Namespace.</p>
<p>Let's keep in mind that not all the networking solutions available for Kubernetes support Network Policies. Review the Pod-to-Pod Communication section from the Kubernetes Architecture chapter if needed. By default, Network Policies are namespaced API resources, but certain network plugins provide additional features so that Network Policies can be applied cluster-wide.</p>
<h3 id="monitoring-logging-and-troubleshooting">Monitoring, Logging, and Troubleshooting </h3>
<p>In Kubernetes, we have to collect resource usage data by Pods, Services, nodes, etc., to understand the overall resource consumption and to make decisions for scaling a given application. Two popular Kubernetes monitoring solutions are the Kubernetes <strong>Metrics Server</strong> and <strong>Prometheus</strong>.</p>
<p>Metrics Server: is a cluster-wide aggregator of resource usage data - a relatively new feature in Kubernetes<br>
Prometheus: now part of CNCF&nbsp;(Cloud Native Computing Foundation), can also be used to scrape the resource usage from different Kubernetes components and objects. Using its client libraries, we can also instrument the code of our application.</p>
<p>Another important aspect for troubleshooting and debugging is <strong>logging</strong>, in which we collect the logs from different components of a given system. In Kubernetes, we can collect logs from different cluster components, objects, nodes, etc. Unfortunately, however, Kubernetes does not provide cluster-wide logging by default, therefore third party tools are required to centralize and aggregate cluster logs. A popular method to collect logs is using <strong>Elasticsearch</strong>&nbsp;together with&nbsp;<strong>Fluentd</strong>&nbsp;with custom configuration as an agent on the nodes. Fluentd is an open source data collector, which is also part of CNCF.</p>
<p>The third-party troubleshooting tools are addressing a shortcoming of Kubernetes with regards to its logging capability. Although we can extract container logs from the cluster, we are limited only to logs of currently running containers, and in the case of several consecutive container restarts due to failures - the logs of the very last failed container (using the -p or --previous flags). The logs can be displayed for a single container pod or a specific container of a multi-container pod (using the -c flag):</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>$ kubectl logs &lt;pod-name&gt;
$ kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;
$ kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt; -p
</code></pre><p>In addition, a user can run a custom command in a running container of a pod, or interact with the running container from the terminal (using the -it flag and invoking the shell command line interpreter of the container):</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>$ kubectl exec &lt;pod-name&gt; -- ls -la /
$ kubectl exec &lt;pod-name&gt; -c &lt;container-name&gt; -- env
$ kubectl exec &lt;pod-name&gt; -c &lt;container-name&gt; -it -- /bin/sh
</code></pre><p>There are scenarios when the pods of an application do not reach the expected running state, visible in the output of the kubectl get pods command. In order to discover what prevents the pod from running - whether a missing dependency, container image or runtime issue, we can view the events for the entire cluster or for a specific pod in the output of the describe command:</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>$ kubectl get events
$ kubectl describe pod &lt;pod-name&gt;
</code></pre><h3 id="helm">Helm </h3>
<p>To deploy a complex application, we use a large number of Kubernetes manifests to define API resources such as Deployments, Services, PersistentVolumes, PersistentVolumeClaims, Ingress, or ServiceAccounts. It can become counter productive to deploy them one by one. We can bundle all those manifests after templatizing them into a well-defined format, along with other metadata. Such a bundle is referred to as Chart. These Charts can then be served via repositories, such as those that we have for rpm and deb packages.</p>
<p><strong>Helm</strong>&nbsp;is a package manager (analogous to yum and apt for Linux) for Kubernetes, which can install/update/delete those Charts in the Kubernetes cluster. Helm is a CLI client that may run side-by-side with kubectl on our workstation, that also uses kubeconfig to securely communicate with the Kubernetes API server.</p>
<p>The helm client queries the Chart repositories for Charts based on search parameters, downloads a desired Chart, and then it requests the API server to deploy in the cluster the resources defined in the Chart. Charts submitted for Kubernetes are available <a href="https://artifacthub.io">here</a>.</p>
<h3 id="service-mesh">Service Mesh </h3>
<p>Service Mesh is a third party solution alternative to the Kubernetes native application connectivity and exposure achieved with Services paired with Ingress Controllers. Service Mesh tools are gaining popularity especially with larger organizations managing larger, more dynamic Kubernetes clusters. These third party solutions introduce features such as service discovery, multi-cloud routing, and traffic telemetry.</p>
<p>A Service Mesh is an implementation that relies on a proxy component part of the Data Plane, which is then managed through a Control Plane. The Control Plane runs agents responsible for the service discovery, telemetry, load balancing, network policy, and gateway. The Data Plane proxy component is typically injected into Pods, and it is responsible for handling all Pod-to-Pod communication, while maintaining a constant communication with the Control Plane of the Service Mesh.</p>
<p>Istio&nbsp;is&nbsp;one of the most popular service mesh solutions, backed by Google, IBM and Lyft</p>
<h3 id="application-deployment-strategies">Application Deployment Strategies </h3>
<p>A method presented earlier for new application release rollouts was the Rolling Update mechanism supported by the Deployment operator. The Rolling Update mechanism, and its reverse - the Rollback, are practical methods to manage application updates by allowing one single controller, the Deployment, to handle all the work it involves. However, while transitioning between the old and the new versions of the application replicas, the Service exposing the Deployment eventually forwards traffic to all replicas, old and new, without any possibility for the default Service to isolate a subset of the Deployment's replicas. Because of the traffic routing challenges these update mechanisms introduce, many users may steer away from the one Deployment and one Service model, and embrace more complex deployment mechanism alternatives.</p>
<p>The <strong>Canary</strong> strategy runs two application releases simultaneously managed by two independent Deployment controllers, both exposed by the same Service.&nbsp;The users can manage the amount of traffic each Deployment is exposed to by separately scaling up or down the two Deployment controllers, thus increasing or decreasing the number of their replicas receiving traffic.<br>
&nbsp;<br>
The <strong>Blue/Green</strong> strategy runs the same application release or two releases of the application on two isolated environments, but only one of the two environments is actively receiving traffic, while the second environment is idle, or may undergo rigorous testing prior to shifting traffic to it. This strategy would also require two independent Deployment controllers, each exposed by their dedicated Services, however, a traffic shifting mechanism is also required. Typically, the traffic shifting can be implemented with the use of an Ingress.</p>
<h1 id="introduction-to-istio">Introduction to Istio </h1>
<p><a href="https://learning.edx.org/course/course-v1:LinuxFoundationX+LFS144x+3T2022/home">Reference: learning.edx.org</a></p>
<p>Service meshes are becoming a vital component of a company’s infrastructure. <strong>Istio</strong> is an open-source project and the leading service mesh implementation that is gaining rapid adoption. To understand why service meshes are so important and why they are becoming increasingly common, we must look back at the shift from monoliths to microservices and cloud-native applications, and understand the problems that stemmed from this shift.</p>
<p>Service meshes elegantly solve problems in the areas of <strong>security</strong>, <strong>observability</strong>, <strong>high availability</strong>, and <strong>scalability</strong>. They enable enterprises to respond to situations quickly without requiring development teams to modify and redeploy their applications to effect a policy change. A service mesh serves as the foundation for running cloud-native applications.</p>
<p>By the end of this chapter, you should be able to explain:</p>
<ul>
<li>What problems stemmed from the shift to cloud-native applications.</li>
<li>How these problems were mitigated before service meshes existed.</li>
<li>How service meshes address these problems.</li>
<li>The design and architecture of Istio.</li>
</ul>
<h3 id="the-shift-to-cloud-native-applications">The Shift to Cloud-Native Applications </h3>
<p>The term "cloud-native" represents a list of characteristics that are desirable in a software system, traits such as:</p>
<ul>
<li>High availability</li>
<li>Horizontal scalability</li>
<li>Zero-downtime deployments and upgrades</li>
<li>Security built-in</li>
</ul>
<p>Enterprises' bottom lines became increasingly dependent on system uptime and availability, and the old model of large monolithic applications presented a variety of obstacles to becoming cloud-native. Too many developers in a single codebase complicates continuous integration.</p>
<p>The landscape is littered with business cases of enterprises' difficult journeys toward microservices. In the end, these journeys took the enterprise to a better place, one where the use of automation had increased, where continuous delivery was taking place with increasing frequency. Development and operations became less siloed from one another, allowing developers to self-serve the deployment of their applications, and to become more involved with operating their applications in production. Teams got better at monitoring their systems and were able to lower <strong>mean time to detection</strong> and improved their <strong>mean time to recover</strong>y.</p>
<p>Teams shared their experiences, and before long, many were striving to emulate these successes in their own organizations. Platform companies such as Heroku shared their experience and provided guidance for moving to cloud-native with the publication of the <strong>Twelve-Factor App</strong>. Others built on this foundation, and added to the wisdom by raising the importance of <strong>API-first development</strong>, having <strong>security built-in</strong>, and the importance of <strong>telemetry</strong>. To read more about this topic, check out the book <a href="https://www.vmware.com/docs/ebook-beyond-the-12-factor-app">Beyond the Twelve-Factor App</a>.</p>
<p>These transitions took a long time and required significant effort, primarily because the new microservices architecture, also known as distributed applications, brought its own challenges.</p>
<h3 id="new-problems">New Problems </h3>
<p>Microservices brought several benefits:</p>
<ol>
<li>Organizations were able to organize into smaller teams.</li>
<li>Codebases didn't all have to be written in the same language. Individual codebases shrank and consequently became simpler and easier to maintain and deploy</li>
<li>The deployment of a single microservice entailed less risk. Continuous integration got easier</li>
<li>There now existed contracts in the form of APIs for accessing other services, and developers had fewer dependencies to contend with</li>
</ol>
<p>But the new architecture also brought with it a new set of challenges. Certain operations that used to be simple became difficult. What used to be a method call away became a call over the network</p>
<p>Here are some of the challenges that the new architecture posed:</p>
<ul>
<li><strong>Service discovery</strong>:<br>
How does a service discover the network address of other services?</li>
<li><strong>Load balancing</strong>:<br>
Given that each service is scaled horizontally, the problem of load-balancing was no longer an ingress-only problem</li>
<li><strong>Service call handling</strong>:<br>
How to deal with situations where calls to other services fail or take an inordinately long time? An increasing portion of developers' codebases had to be dedicated to handling the failures and dealing with long response times, by sprinkling in retries and network timeouts</li>
<li><strong>Resilience</strong>:<br>
Developers had to learn (perhaps the hard way) to build distributed applications that are resilient and that prevent cascading failures</li>
<li><strong>Security</strong>:<br>
How do we secure our systems given this new architecture has a much larger attack surface? Can a service trust calls from other services? How does a service identify its caller?</li>
<li><strong>Programming models</strong>:&nbsp;<br>
Developers began exploring alternative models to traditional multithreading to deal more efficiently with network IO (input and output, see <a href="https://reactivex.io">ReactiveX</a>)</li>
<li><strong>Diagnosis and troubleshooting</strong>:&nbsp;<br>
Stack traces no longer provided complete context for diagnosing an issue. Logs were now distributed. How does a developer diagnose issues that span multiple microservices?</li>
<li><strong>Resource utilization</strong>:&nbsp;<br>
Managing resource utilization efficiently became a challenge, given the larger deployment footprint of a system made up of numerous smaller services.</li>
<li><strong>Automated testing</strong>:&nbsp;<br>
End-to-end testing became more difficult</li>
<li><strong>Traffic management</strong>:&nbsp;<br>
The ability to route requests flexibly to different services under different conditions started becoming a necessity</li>
</ul>
<h3 id="early-solutions">Early Solutions </h3>
<p>Netflix was one such company that, out of necessity, had made the move to cloud-native. Netflix was growing at such a rapid pace that they had but a few months to migrate their systems to AWS before they ran out of capacity in their data center.<br>
In the process of that transition, they ran into many of the above-described problems. Netflix teams began addressing their issues by developing several projects, which they chose to open-source. Netflix teams built the <em>Eureka</em> service registry to solve the problems of service discovery. They wrote <em>Ribbon</em> to support client-side load balancing. They developed the <em>Hystrix</em> library (and dashboards) to deal with cascading failures, and the <em>Zuul</em> proxy was designed to give them the routing flexibility they needed for a variety of needs from blue-green deployments to failover, troubleshooting, and chaos engineering.</p>
<p>Shortly thereafter, the <em>Spring</em> engineering team adapted these projects to the popular Spring framework under the umbrella name Spring cloud. These new services became accessible to any Spring developer by adding client libraries as dependencies to their Spring projects. Many enterprises adopted these projects. But their use implied certain constraints. To participate in this ecosystem, services had to be written for the JVM and had to use the Spring framework. A growing list of third-party dependencies had to be added to the footprint of each application. Using these services was not a completely transparent operation either. Developers often had to annotate their applications to enable features and configure them. Specific methods required custom annotations, and in other cases, specific client APIs were required to use a feature.</p>
<p>The use of these infrastructure services represented an important improvement over the previous state of affairs. For example, development teams no longer had to write their own retry logic. At the same time, use of these infrastructure services was not transparent to participating applications and represented an added burden on development teams. Dependencies had to be kept up to date and versions in sync, adding a certain measure of configuration fragility. What if these infrastructural concerns could be removed from these individual microservice applications, and "pushed down" into the fabric of the underlying platform?</p>
<p>While there are a number of advantages for adopting microservices architecture on a distributed system like Kubernetes, it has its fair share of complexities. Since distributed services have to communicate with each other, we have to think about discovery, routing, retries, and fail-over.</p>
<p>Now, building these communication capabilities within each service can be quite tedious — even more so when the service landscape grows and communication becomes complex. This is precisely where a service mesh can help us. Basically, <em>a service mesh takes away the responsibility of managing all service-to-service communication within a distributed software system</em>. The way service mesh is able to do that is through an array of <strong>network proxies</strong>. Essentially, requests between services are routed through proxies that run alongside the services but sit outside in the infrastructure layer. These proxies basically create a <em>mesh network</em> for the services — hence the name, service mesh! Through these proxies, a service mesh is able to control every aspect of service-to-service communication. As such, we can use it to address the <a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">eight fallacies of distributed computing</a>, a set of assertions that describe false assumptions we often make about a distributed application.</p>
<p>Fundamentally, Istio works by deploying an extended version of <a href="https://www.envoyproxy.io"><strong>Envoy</strong></a> as proxies to every microservice as a sidecar. This network of proxies constitutes the <strong>data plane</strong> of the Istio architecture. The configuration and management of these proxies are done from the <strong>control plane</strong>. The control plane is basically the brain of the service mesh. It provides <strong>discovery</strong>, <strong>configuration</strong>, and <strong>certificate management</strong> to Envoy proxies in the data plane at runtime.</p>
<p align="center">
    <img src="./assets/k8s/istio-arch1.png" alt="drawing" width="600" height="300" style="center">
</p>
<p>Of course, we can only realize the benefit of Istio when we have a large number of microservices that communicate with each other. Here, the sidecar proxies form a complex service mesh in a dedicated infrastructure layer:</p>
<p align="center">
    <img src="./assets/k8s/istio2.png" alt="drawing" width="600" height="400" style="center">
</p>
<p>Istio is quite flexible in terms of integrating with external libraries and platforms. For instance, we can integrate Istio with an external logging platform, telemetry, or policy system. See <a href="https://www.baeldung.com/ops/istio-service-mesh">here</a> for more.</p>
<h2 id="service-meshes">Service Meshes </h2>
<p>Engineers struggled with similar problems at Lyft, moving away from monoliths toward a cloud-native architecture. At Lyft, Matt Klein and others proposed a different approach to solving these same problems: to bundle infrastructural capabilities completely out of process, in a manner separate from the main running application. Essentially every service in a distributed system would be accompanied by its own dedicated proxy running out-of-process. By routing requests in and out of a given application through its proxy, the proxy would have the opportunity to perform services on behalf of its application in a transparent fashion. This proxy could</p>
<ul>
<li>retry failed requests</li>
<li>be configured with specific network timeouts, and circuit-breaking logic</li>
<li>could encapsulate the logic of performing client-side load balancing requests to other services</li>
<li>could act as a security gateway, also known as a Policy Enforcement Point. Connections can be upgraded from plain HTTP to encrypted traffic with mutual TLS</li>
<li>could collect metrics such as request counts, durations, response codes and more, and expose those metrics to a monitoring system, thereby removing the burden of managing metrics collection and publishing from the development teams</li>
</ul>
<p>The application of this proxy at Lyft helped solve many of the problems that its development teams were running into, and helped make their migration away from monoliths a success. Matt Klein subsequently open-sourced the project and named it Envoy. At the same time, the advent of containerization (Docker) and container orchestrators (Kubernetes) began addressing problems of resource utilization and freed operators from the mundane task of determining where to run workloads. Kubernetes Pods provided an important intermediary construct that, on the one hand, allowed for isolation within the container but on the other, for multiple loosely coupled containers to be bundled together as a single unit. Kubernetes came from Google, and Google was looking to build this same out-of-process infrastructural capability on top of Kubernetes. The Istio project started at Google, and as it turns out, Istio saw in Envoy the perfect building block for its service mesh. The Istio control plane would automate the configuration and synchronization of proxies deployed onto Kubernetes as sidecars inside each Pod. The Kubernetes API server's capabilities could be leveraged to automate service discovery and communicate the locations of service endpoints directly to each proxy. Fine-grained configuration of proxies could be performed by exposing Kubernetes Custom Resource Definitions (CRDs).</p>
<p>The Istio project was born as one of the implementations of service mesh. Istio is the first and most widely used open source project to make service mesh accessible to a wider audience. Istio service mesh</p>
<ul>
<li>serves as a networking layer, automating and securing communications between applications</li>
<li>is independent of any specific programming language</li>
<li>provides a control plane to define and implement the way microservices communicate with each other</li>
<li>based on a foundation layer of lightweight network proxy instances derived from the Envoy proxy. Envoy is responsible for all service interaction in Kubernetes or virtual machines (VMs)</li>
<li>services do not know they are working alongside Envoy instances or using a sidecar proxy.</li>
</ul>
<p>Using Istio service mesh, platform teams can address needs for</p>
<ul>
<li><strong>Traffic Management</strong></li>
<li><strong>Service Security</strong></li>
<li><strong>Application Monitoring</strong></li>
</ul>
<p>Istio is designed to run in various environments like on-premises, multi-cloud, Kubernetes containers, and virtual machines (VMs), so Istio helps platform teams manage and monitor all the service traffic across clusters and data centers.</p>
<h3 id="features-of-istio-service-mesh">Features of Istio Service Mesh </h3>
<p>Istio provides numerous features that make software development and delivery faster, easier, and more secure. Istio offers</p>
<ul>
<li>Authentication</li>
<li>Authorization</li>
<li>Load balancing</li>
<li>Circuit breaker</li>
<li>Time outs, retries, and deployment strategies</li>
<li>Service discovery</li>
<li>Observability</li>
</ul>
<p>Following is a brief description of key capabilities that you can expect Istio + Envoy software to provide:</p>
<h3 id="security">Security </h3>
<p>Istio helps application teams to achieve zero trust security with the ability to define and implement <strong>authentication</strong>, <strong>authorization</strong>, and <strong>access control policies</strong>. All your data communicated among the services, in and outside of the cluster or data center, will be encrypted based on <strong>mTLS protocols</strong> provided by Istio resources. You can also ensure <strong>authentication of apps from internal and external users using JSON Web Tokens (JWT)</strong> provided by Istio.</p>
<h3 id="service-discovery">Service Discovery </h3>
<p>One of the primary needs of an application running in a production environment is to be highly available. This requires one to scale up the number of service instances with increasing load and scale down when needed to save costs. Istio’s service discovery capability keeps track of all the available nodes ready to pick up new tasks. In case of node unavailability, service discovery removes a node from the list of available nodes and stops sending new requests to the node.</p>
<h3 id="traffic-management">Traffic Management </h3>
<p>Using Envoy proxies, Istio provides flexibility to finely control the traffic among the available services. Istio provides features like <strong>load balancing</strong>, <strong>health checks</strong>, and <strong>deployment strategies</strong>. Istio allows load balancing based on algorithms that include round robin, random selection, weighted algorithms, etc. Istio performs constant health checks of service instances to ensure they are available before routing the traffic request. And based on the deployment type used in the configuration, Istio drives traffic to new nodes in a weighted pattern.</p>
<h3 id="resilience">Resilience </h3>
<p>Istio removes the need for coding <strong>circuit breakers</strong> within an application. Istio helps platform architects to define mechanisms such as <strong>timeouts</strong> to a service, number of <strong>retries</strong> to be made and planned <strong>automatic failover</strong> of high availability (HA) systems, without the application knowing about them.</p>
<h3 id="observability">Observability </h3>
<p>Istio keeps <em>track of network requests and traces</em> each call across multiple services. Istio provides the <strong>telemetry</strong> (such as latency, saturation, traffic health, and errors) that helps SREs to understand service behavior and troubleshoot, maintain, and optimize their applications.</p>
<h4 id="advanced-deployment">Advanced Deployment </h4>
<p>Istio provides <strong>visibility</strong> and <strong>fine-grained network controls</strong> for traditional and modern workloads, including containers and virtual machines. Istio helps to achieve <strong>canary</strong> and <strong>blue-green deployment</strong> by providing the capability to route specific user groups to newly deployed applications.</p>
<h2 id="istio-architecture">Istio Architecture </h2>
<p>As shown in the illustration in the previous section, the basic idea behind Istio is to push microservices concerns into the infrastructure by leveraging Kubernetes. This is implemented by bundling the Envoy proxy as a sidecar container directly inside every Pod.</p>
<p>Note: in the Advanced Topics chapter, we show how a service mesh can be extended to include workloads running on VMs, outside Kubernetes.<br>
In terms of implementation, Istio's main concerns are, therefore, solving the following problems:</p>
<ul>
<li>Ensuring that each time a workload is deployed, an Envoy sidecar is deployed alongside it.</li>
<li>Ensuring traffic into and out of the application is transparently diverted through the proxy.</li>
<li>Assigning each workload a cryptographic identity as the basis for a more secure computing environment.</li>
<li>Configuring the proxies with all the information they need to handle incoming and outgoing traffic.</li>
</ul>
<p>We will explore each of these concerns more in-depth in the following sections.</p>
<h3 id="how-does-istio-work">How Does Istio Work? </h3>
<p>Istio has two components which together make up a service mesh.</p>
<ul>
<li>
<p><strong>Data Plane</strong>:<br>
The data plane is responsible for translating, forwarding, and monitoring every network packet flowing to and from an instance. It owns key features such as health checking, routing, service discovery, load balancing, security, and telemetry. Istio uses Envoy proxy alongside each service in a cluster to observe, validate and manage all inbound and outbound requests. All incoming traffic from APIs is directed to an instance of Envoy (called, in this case, an ingress proxy). The proxy then follows rules or policies, specified in the Istio control plane, to decide how, when, or what service to route the traffic. Using these rules, Istio also enables techniques such as fault injections, circuit breaking, and canary deployments without any change to services.</p>
</li>
<li>
<p><strong>Control Plane</strong><br>
The Istio control plane provides rules and logic to carry out the communication between proxies in the service mesh. Say, when a new service is discovered, the control plane populates the existing list of services, which is then used by other proxies in the mesh to direct traffic. All the logic and configurations related to circuit breaking, retries, timeouts, load balancing, and security information are stored in the control plane. Whenever a new deployment happens, the data plane becomes aware of the new node but the mechanism of routing and shifting traffic will be defined by the control plane. The control plane provides policy, rules and configuration to the data plane without modifying any data in the mesh. Refer to the image below:</p>
  <p align="center">
      <img src="./assets/k8s/istio-arch2.png" alt="drawing" width="700" height="400" style="center">
  </p>
<p>Most of the features stated in the data plane such as routing, health checking, service discovery, load balancing, security, etc, are available in projects such as HAProxy, Nginx, and Envoy. However, the configuration has to be written, deployed, and maintained manually using multiple other tools. Istio combines all these and provides an integrated platform, removing boilerplate configurations and offering durability in the solution.</p>
</li>
</ul>
<p>Under the hood , Istio uses five major tools:</p>
<ul>
<li><strong>Envoy Proxy</strong>:&nbsp;Istio uses Envoy proxy as a sidecar proxy in the data plane. Envoy in the data plane is responsible for functionalities like failure handling, health checks, service discovery, and load balancing. Envoy proxies provide detailed information about each service request</li>
<li><strong>Mixer</strong>:&nbsp;in the control plane, it acts as Istio’s telemetry hub, gathering attributes about service requests from Envoy proxies in the mesh. Mixer provides an API to fetch those attributes for monitoring and logging purposes</li>
<li><strong>Pilot</strong>:&nbsp;Istio uses Pilot in the control plane to provide traffic controls and load balancing based on the service mesh. All the traffic rules can be specified in Istio, and Pilot under the hood can communicate with Kubernetes infrastructure about deployment changes affecting traffic. Istio also uses Pilot to distribute security policies (such as authentication and authorization policies) to all Envoy proxies.</li>
<li><strong>Citadel</strong>:&nbsp;Istio uses Citadel to provide policy-driven and secured communication between Envoy proxies. All the authentication and key-based credential management between sidecar proxies is managed by Citadel.</li>
<li><strong>Galley</strong>:&nbsp;Istio control plane uses Galley for interpreting user-defined Kubernetes YAML files into a format that Istio understands. Galley stores the user configuration, validates it, and then sends it to Pilot for further action.</li>
</ul>
<h3 id="sidecar-injection">Sidecar Injection </h3>
<p>Modifying Kubernetes deployment manifests to bundle proxies as sidecars with each pod is both a burden to development teams, error-prone, and not maintainable. Part of Istio's codebase is dedicated to providing the capability to automatically modify Kubernetes deployment manifests to include sidecars with each pod. This capability is exposed in two ways, the first and simpler mechanism is known as manual sidecar injection, and the second is called automatic sidecar injection.</p>
<ul>
<li>
<p><strong>Manual sidecar injection</strong>:<br>
Istio has a command-line interface (CLI) named <strong>istioctl</strong> with the subcommand kube-inject. The subcommand processes the original deployment manifest to produce a modified manifest with the sidecar container specification added to the pod (or pod template) specification. The modified output can then be applied to a Kubernetes cluster with the <code>kubectl apply -f</code> command. With manual injection, the process of altering the manifests is explicit.</p>
</li>
<li>
<p><strong>Automatic sidecar injection</strong>:<br>
With automatic injection, the bundling of the sidecar is made transparent to the user. This process relies on a Kubernetes feature known as Mutating Admission Webhooks, a mechanism that allows for the registration of a webhook that can intercept the application of a deployment manifest and mutate it before the final, modified specification is applied to the Kubernetes cluster. The webhook is triggered according to a simple convention, where the application of the label <code>istio-injection=enabled</code> to a Kubernetes namespace governs whether the webhook should modify any deployment or pod resource applied to that namespace to include the sidecar.</p>
</li>
</ul>
<h4 id="routing-application-traffic-through-the-sidecar">Routing Application Traffic Through the Sidecar </h4>
<p>With the sidecar deployed, the next problem is ensuring that the proxy transparently captures the traffic. The outbound traffic should be diverted from its original destination to the proxy, and inbound traffic should arrive at the proxy before the application has a chance to handle the incoming request. This is performed by applying <strong>iptables</strong> rules. The video by Matt Turner titled <a href="https://www.youtube.com/watch?v=oZrZlx2fmcM">Life of a Packet through Istio</a> explains elegantly how this process works.</p>
<p>In addition to the Envoy sidecar, the sidecar injection process injects a Kubernetes <strong>init container</strong>. This init-container is a process that applies these iptables rules before the Pod containers are started. Today Istio provides two alternative mechanisms for configuring a Pod to allow Envoy to intercept requests. The first is the original iptables method and the second uses a <a href="https://istio.io/latest/docs/setup/additional-setup/cni/">Kubernetes CNI plugin</a>.</p>
<h3 id="install-istio---commmand-line">Install Istio - Commmand Line </h3>
<p>First download Istio:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">-L</span> https://istio.io/downloadIstio <span class="token operator">|</span> <span class="token function">sh</span> -
</code></pre><p>The command above downloads the latest release (numerically) of Istio. To download a specific version of Istio eg. 1.20.2 for the x86_64 architecture, run:<br>
<code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.20.2 TARGET_ARCH=x86_64 sh -</code></p>
<p>To run the istioctl from any folder, we should include its fully-qualified path in the PATH environment variable:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token builtin class-name">cd</span> istio-1.14.3
/istio-1.20.2$ <span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PWD</span>/bin:<span class="token environment constant">$PATH</span>
</code></pre><p>To check that the Istio CLI is on the path, run <code>istioctl version</code>. Usinf istioctl, you can install Istio:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>/istio-1.20.2$ istioctl <span class="token function">install</span> <span class="token parameter variable">-f</span> demo-profile.yaml

This will <span class="token function">install</span> the Istio <span class="token number">1.14</span>.3 demo profile with <span class="token punctuation">[</span><span class="token string">"Istio core"</span> <span class="token string">"Istiod"</span> <span class="token string">"Ingress gateways"</span> <span class="token string">"Egress gateways"</span><span class="token punctuation">]</span> components into the cluster. Proceed? <span class="token punctuation">(</span>y/N<span class="token punctuation">)</span> y
✔ Istio core installed
✔ Istiod installed
✔ Egress gateways installed
✔ Ingress gateways installed
✔ Installation complete
Making this installation the default <span class="token keyword keyword-for">for</span> injection and validation.
Thank you <span class="token keyword keyword-for">for</span> installing Istio <span class="token number">1.22</span>. Please take a few minutes to tell us about your install/upgrade experience<span class="token operator">!</span> 
ht‌tps://forms.gle/yEtCbt45FZ3VoDT5A

</code></pre><p>To check the deployed resource, we can look at the status of the pods in the <code>istio-system</code> namespace:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get po <span class="token parameter variable">-n</span> istio-system
</code></pre><p>We can enable automatic sidecar injection on any Kubernetes namespace. By labeling the namespace with the label <code>istio-injection=enabled</code>, the Istio control plane will monitor that namespace for new Kubernetes deployments. It will automatically intercept the deployments and inject Envoy sidecars into each pod.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>kubectl label namespace default istio-injection<span class="token operator">=</span>enabled
</code></pre><h2 id="observability-1">Observability </h2>
<p>Monitoring vs Observability<br>
The term observability is broader than monitoring. It encompasses not only monitoring but anything that assists system engineers in understanding how a system behaves. Observability in microservices includes not only the collection and visualization of metrics but also log aggregation and dashboards that help visualize distributed traces.</p>
<p>Each facet of observability complements the other.</p>
<ul>
<li>Dashboards exposing metrics may indicate a performance problem somewhere in the system.</li>
<li>Distributed traces can help locate the performance bottleneck to a specific service.</li>
<li>Service's logs can provide the context necessary to determine what specifically may be the issue.</li>
</ul>
<p>This trio: <strong>metrics</strong>, <strong>logs</strong>, and <strong>distributed traces</strong> are the foundation for modern distributed systems observability.</p>
<h3 id="how-service-meshes-simplify-and-improve-observability">How Service Meshes Simplify and Improve Observability </h3>
<p>Before service meshes, the burden of capturing, exposing, and publishing metrics was on the shoulders of application developers. So was the burden of constructing dashboards for the purpose of monitoring application health. Not only was this an added burden, but it created a situation where the implementation of observability from one team to the next was not uniform. What metrics are exposed, how they are captured, what they are named, and even what monitoring system is used could all be different from one application to another.</p>
<p>Through the Envoy sidecar, Istio is able to collect metrics and expose scrape endpoints that allow for the collection of a uniform set of metrics for all microservices. Istio further allows for the development of dashboards that are uniform across all services. The burden is lifted from the shoulders of the developers responsible for a given microservice, and the end result is a uniform treatment of metrics collection and observability across the entire platform.</p>
<h3 id="how-istio-exposes-workload-metrics-1">How Istio Exposes Workload Metrics (1) </h3>
<p>Ensure that Istio is deployed with the demo profile:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token variable">$istioctl</span> <span class="token function">install</span> <span class="token parameter variable">--set</span> <span class="token assign-left variable">profile</span><span class="token operator">=</span>demo
</code></pre><p>Next, ensure that the default namespace is labeled for sidecar injection:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl label ns default istio-injection<span class="token operator">=</span>enabled
</code></pre><p>Verify that the label has been applied with the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get ns <span class="token parameter variable">-L</span> istio-injection
NAME              STATUS   AGE   ISTIO-INJECTION
default           Active   42h   enabled
istio-system      Active   21h
kube-node-lease   Active   42h
kube-public       Active   42h
kube-system       Active   42h
metallb-system    Active   22h
</code></pre><p>Deploy the <a href="https://istio.io/latest/docs/examples/bookinfo/">BookInfo</a> sample application that is bundled with the Istio distribution.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> samples/bookinfo/platform/kube/bookinfo.yaml
</code></pre><p>Finally, deploy the bundled sleep sample service:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> samples/sleep/sleep.yaml
$ kubectl <span class="token builtin class-name">exec</span> <span class="token variable">$SLEEP_POD</span> <span class="token parameter variable">-it</span> -- <span class="token function">curl</span> productpage:9080/productpage <span class="token operator">|</span> <span class="token function">head</span>
</code></pre><p>The output should show the start of the HTML response from the&nbsp;productpage&nbsp;service, like so:</p>
<pre data-role="codeBlock" data-info="html" class="language-html html"><code><span class="token doctype"><span class="token punctuation">&lt;!</span><span class="token doctype-tag">DOCTYPE</span> <span class="token name">html</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>html</span><span class="token punctuation">&gt;</span></span>
      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>head</span><span class="token punctuation">&gt;</span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>title</span><span class="token punctuation">&gt;</span></span>Simple Bookstore App<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>title</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">charset</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>utf-8<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">http-equiv</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>X-UA-Compatible<span class="token punctuation">"</span></span> <span class="token attr-name">content</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>IE=edge<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>viewport<span class="token punctuation">"</span></span> <span class="token attr-name">content</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>width=device-width, initial-scale=1.0<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>
    
        <span class="token comment">&lt;!-- Latest compiled and minified CSS --&gt;</span>
         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>link</span> <span class="token attr-name">rel</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>stylesheet<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>static/bootstrap/css/bootstrap.min.css<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>
</code></pre><p>The name of the sidecar container in Istio is&nbsp;<code>istio-proxy</code> which lives in the same pod as the service:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get pod <span class="token variable">$PRODUCTPAGE_POD</span> <span class="token parameter variable">-ojsonpath</span><span class="token operator">=</span><span class="token string">'{.spec.containers[*].name}'</span>
productpage istio-proxy
</code></pre><p>One of the benefits of running a sidecar is that it can expose a metrics collection endpoint, also known as the <strong>Prometheus</strong> "scrape endpoint," on behalf of the workload it proxies. We can query the scrape endpoint as follows:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token variable">$PRODUCTPAGE_POD</span> <span class="token parameter variable">-c</span> istio-proxy <span class="token function">curl</span> localhost:15090/stats/prometheus
</code></pre><p>Now we can deploy <strong>Prometheus</strong> and run some queries directly against the Prometheus server. Prometheus is configured to collect metrics from all workloads at a regular 15-second interval. Run the following command, which will deploy Prometheus to the istio-system namespace:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> samples/addons/prometheus.yaml
</code></pre><p>With Prometheus now running and collecting metrics, send another request to the productpage service:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token variable">$SLEEP_POD</span> <span class="token parameter variable">-it</span> -- <span class="token function">curl</span> productpage:9080/productpage <span class="token operator">|</span> <span class="token function">head</span>
</code></pre><p>or just visit the page productpage after exposing it (ingress gateway, for example.) . Now expose prometheus server using the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl expose svc prometheus <span class="token parameter variable">-n</span> istio-system <span class="token parameter variable">--type</span><span class="token operator">=</span>NodePort —name<span class="token operator">=</span>prometheus-ext 
</code></pre><p>Use the specified port to visit the prometheus server page. For example, to find out the subset of requests that returned an HTTP 200 response code, the query would be: <code>istio_requests_total{response_code=“200"}</code>. By collecting this extra metadata with each metric, we can obtain answers to many interesting questions. For example, to locate the call we made earlier from the sleep pod to the productpage service, the query would be: <code>istio_requests_total{source_app="sleep",destination_app="productpage"}</code>.</p>
<p>A more interesting question that applies to COUNTER-type metrics in general, is the rate of incoming requests (over a particular time window, say the last 5 minutes) against a particular service:<br>
<code>rate(istio_requests_total{destination_app=“productpage”[5m])</code></p>
<p>Look at the output from the <strong>Graph</strong>&nbsp;tab. Since there currently exists no load against our service, the rate should be zero. If, on the other hand, we query the&nbsp;productpage&nbsp;service every 1-2 seconds, like so...</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token keyword keyword-while">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword keyword-do">do</span> kubectl <span class="token builtin class-name">exec</span> <span class="token variable">$SLEEP_POD</span> <span class="token parameter variable">-it</span> -- <span class="token function">curl</span> productpage:9080/productpage <span class="token operator">&gt;</span> /dev/null<span class="token punctuation">;</span> <span class="token function">sleep</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token keyword keyword-done">done</span>
</code></pre><p>..then, within a couple of minutes, the rate of requests will rise from zero to a value between 0.5 and 1.0.</p>
<p>Prometheus'&nbsp;PromQL query&nbsp;language is powerful and can help diagnose issues, as illustrated by Karl Stoney in his blog entry Istio:&nbsp;503's with UC's and TCP Fun Times. But Prometheus queries are no substitute for a set of properly designed monitoring dashboards, to which we turn our attention in the next lab.</p>
<h3 id="grafana-dashboards-for-istio-1">Grafana Dashboards for Istio (1) </h3>
<p><strong>Grafana</strong> is a popular open-source tool that makes it easy to construct custom monitoring dashboards from a backing metrics source. Grafana has built-in support for Prometheus.</p>
<p>The Istio project team has developed a set of Grafana dashboards specifically designed for monitoring a service mesh. Deploy Grafana with the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> samples/addons/grafana.yaml
</code></pre><p>Expose the Grafana service to have access to its UI. To view the Istio dashboards in Grafana, navigate from the main page with the aid of the navigation bar on the left-hand side of the screen. Under Dashboards select Browse. To the right of the folder labeled Istio, click on the link captioned Go to folder. Inside that folder, you will find six dashboards:</p>
<ul>
<li>Mesh: provides a high-level overview of the health of services in the mesh.</li>
<li>Service: for monitoring a specific service. The metrics shown here are aggregated from multiple workloads.</li>
<li>Workload: this allows you to inspect the behavior of a single workload.</li>
<li>Control Plane: designed to monitor the health of istiod, the Control plane itself. It helps determine whether the control plane is healthy and able to synchronize the Envoy sidecars to the state of the mesh.</li>
<li>Performance: this allows you to monitor the resource consumption of istiod and the sidecars.</li>
<li>Wasm Extension: For monitoring the health of custom Web Assembly extensions deployed to the mesh.</li>
</ul>
<p>Let us send some traffic to the&nbsp;productpage&nbsp;service so that we have something to observe.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token keyword keyword-while">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword keyword-do">do</span> kubectl <span class="token builtin class-name">exec</span> <span class="token variable">$SLEEP_POD</span> <span class="token parameter variable">-it</span> -- <span class="token function">curl</span> productpage:9080/productpage <span class="token operator">&gt;</span> /dev/null<span class="token punctuation">;</span> <span class="token function">sleep</span> <span class="token number">0.3</span><span class="token punctuation">;</span> <span class="token keyword keyword-done">done</span>
</code></pre><p>You should see a global request volume of 2-3 operations per second, no 4xx or 5xx errors, and all services should show a 100% success rate. Some other metrics will show "N/A" ("Not Applicable") for the moment. Later in this course, you will define custom Istio resources, including Gateways, Virtual Services, and Destination Rules, and at that time, the count of each type of resource will display on this dashboard. Next, visit the Istio Service Dashboard, select the service named <code>productpage.default.svc.cluster.local</code>, and expand the&nbsp;General panel. There you will find the typical golden signals, including <strong>request volume</strong>, <strong>success rate (or errors)</strong>, and <strong>request duration</strong>. The other two panels,&nbsp;Client Workloads&nbsp;and&nbsp;Service Workloads,&nbsp;break down incoming requests to this service by source and by destination, respectively.</p>
<h3 id="distributed-tracing">Distributed Tracing </h3>
<p>Distributed tracing is an important component of observability that complements metrics dashboards. The idea is to provide the capability to "see" the end-to-end request-response flow through a series of microservices and to draw important information from it. From a view of a distributed trace, developers can discover potential <strong>latency issues</strong> in their applications.</p>
<h4 id="terms">Terms </h4>
<p>The end-to-end request-response flow is known as a <strong>trace</strong>. Each component of a trace, such as a single call from one service to another, is called a <strong>span</strong>. Traces have unique IDs, and so do spans. All spans that are part of the same trace bear the same trace ID. The IDs are propagated across the calls between services in HTTP headers whose names begin with <strong>x-b3</strong> and are known as B3 trace headers (see <a href="https://github.com/openzipkin/b3-propagation"><strong>B3 Propagation</strong></a>).</p>
<p>When Envoy sidecars receive the initial request that does not contain a B3 header and realize that this span represents the beginning of a new trace, they assign the request a new trace ID. However, the propagation of these headers onto other services cannot be performed automatically by Envoy, and so developers must ensure that they propagate these headers in upstream calls to other services (see <a href="https://istio.io/latest/about/faq/#how-to-support-tracing">Istio / FAQ</a>). This task is often easily accomplished by including a tracing client library as a dependency to the services.</p>
<h3 id="deploy-jaeger-and-review-some-traces">Deploy Jaeger and Review Some Traces </h3>
<p>Next, run the following command to send requests to the&nbsp;productpage&nbsp;service every 1-2 seconds:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token keyword keyword-while">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword keyword-do">do</span> kubectl <span class="token builtin class-name">exec</span> <span class="token variable">$SLEEP_POD</span> <span class="token parameter variable">-it</span> -- <span class="token function">curl</span> productpage:9080/productpage <span class="token operator">&gt;</span> /dev/null<span class="token punctuation">;</span> <span class="token function">sleep</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token keyword keyword-done">done</span>
</code></pre><p>Requests will be tagged with trace and span IDs and be sent and consequently collected by Jaeger.</p>
<p><strong>Kiali</strong> is an open-source graphical console specifically designed for Istio and includes numerous features. Through alerts and warnings, it can help validate that the service mesh configuration is correct and that it does not have any problems.</p>
<p>With Kiali, one can view Istio custom resources, services, workloads, or applications. As an alternative to drafting and applying Istio custom resources by hand, Kiali exposes actions that allow the operator to</p>
<ul>
<li>define routing rules</li>
<li>perform traffic shifting</li>
<li>configure timeouts and</li>
<li>inject faults</li>
</ul>
<p>Kiali relies on the metrics collected in Prometheus. In addition, Kiali has the ability to combine information from metrics, traces, and logs to provide deeper insight into the functioning of the mesh. One feature of Kiali that stands out is the Graph section, which provides a live visualization of traffic inside the mesh. Begin by deploying Kiali to your Kubernetes cluster:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> samples/addons/kiali.yaml
</code></pre><p>Next, run the following command to send requests to the productpage service at a 1-2 second interval:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token keyword keyword-while">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword keyword-do">do</span> kubectl <span class="token builtin class-name">exec</span> <span class="token variable">$SLEEP_POD</span> <span class="token parameter variable">-it</span> -- <span class="token function">curl</span> productpage:9080/productpage<span class="token punctuation">;</span> <span class="token function">sleep</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token keyword keyword-done">done</span>
</code></pre><p>In the UI, select the&nbsp;Graph&nbsp;option from the sidebar, and select the&nbsp;default&nbsp;namespace. Through the&nbsp;Display&nbsp;options, interesting bits of additional information can be overlaid on the graph, including whether calls between services are encrypted with mutual TLS, traffic rate, and traffic animations that reflect the relative rate of requests between services.</p>
<p>One can also navigate directly from the&nbsp;Graph&nbsp;view to a particular service. The screenshot below is a view of the ratings service, where one can clearly see that both reviews-v2 and reviews-v3 call this service and that those calls are indeed using mutual TLS encryption. The green color of the arrows linking the services indicate that requests are succeeding with HTTP 200 response codes.</p>
<p align="center">
    <img src="./assets/k8s/kiali.png" alt="drawing" width="700" height="700" style="center">
</p>
<p>Monitoring solutions are often referred to as Application Performance Monitoring (APM tools). There exist many alternative APM tools and solutions out in the marketplace. One popular open-source option is the <a href="https://skywalking.apache.org">Apache Foundation's Skywalking project</a>. Apache Skywalking supports monitoring service meshes. This blog entry provides a tutorial for installing Apache Skywalking on a Kubernetes cluster and configuring it to work with Istio. Skywalking can be installed with the popular Helm package manager. Up-to-date instructions for installing Skywalking with Helm can be found on <a href="https://github.com/apache/skywalking-helm">Apache Skywalking's GitHub repository</a>.<br>
Once Apache Skywalking is up and running, we can proceed to access its dashboard, which provides features similar to some of the other dashboards we visited in this chapter. Below is a screenshot of the Topology view from the Apache Skywalking dashboard showing traffic making its way through Istio's bookinfo sample application.</p>
<h2 id="traffic-management-1">Traffic Management </h2>
<h3 id="gateways">Gateways </h3>
<p>In the earlier installation lab, when we installed Istio using the demo profile, it included the ingress and egress gateways. Both gateways are Kubernetes deployments that run an instance of the Envoy proxy, and they operate as load balancers at the edge of the mesh. The ingress gateway receives inbound connections, while the egress gateway receives connections going out of the cluster. Using the ingress gateway, we can apply route rules to the inbound traffic entering the cluster. As part of the ingress gateway, a Kubernetes service of type LoadBalancer is deployed, giving us an external IP address.</p>
<p>We will deploy a Hello World application to the cluster. We will then deploy a Gateway resource and a VirtualService that binds to the Gateway to expose the application on an external IP address. Ensure you have a Kubernetes cluster with Istio installed, and the default namespace labeled for Istio sidecar injection before continuing.</p>
<p align="center">
    <img src="./assets/k8s/gateway-vs.png" alt="drawing" width="400" height="200" style="center">
</p>
<p>Let us start by deploying the Gateway resource. We will set the&nbsp;hosts&nbsp;field to&nbsp;*&nbsp;(*&nbsp;is a wildcard matcher) to access the ingress gateway directly from the external IP address, without any hostname. If we wanted to access the ingress gateway through a domain name, we could set the hosts' value to a domain name (e.g.,&nbsp;<a href="http://example.com">example.com</a>) and add the external IP address as an A record in the domain's DNS settings.</p>
<p>At the same time, create a <strong>VirtualService</strong> for the hello-world service and bind it to the Gateway resource. We use the * in the hosts field, just like in the Gateway resource. We have also added the Gateway resource we created earlier (gateway) to the gateway array. We say that we have attached the Gateway to the VirtualService.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Gateway
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> gateway
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    istio<span class="token punctuation">:</span> ingressgateway
  servers<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span>
        number<span class="token punctuation">:</span> <span class="token number">80</span>
        name<span class="token punctuation">:</span> http
        protocol<span class="token punctuation">:</span> HTTP
      hosts<span class="token punctuation">:</span>
        <span class="token punctuation">-</span> ‘<span class="token important">*'</span>
——
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> VirtualService
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  hosts<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token string">"*"</span>
  gateways<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> gateway
  http<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">route</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
            host<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world.default.svc.cluster.local
            port<span class="token punctuation">:</span>
              number<span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre><p>Using a fully qualified service name is the preferred way to reference the services in Istio resources. We can also use a short name (e.g., hello-world), which might lead to confusion and unexpected behavior if we have multiple services with the same name running in different namespaces.<br>
Save the above YAML to <code>hello-world-gateway.yaml</code> and create the VirtualService:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> hello-world-gateway.yaml
</code></pre><p>The following creates a deployment and a service for hello-world app.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  replicas<span class="token punctuation">:</span> <span class="token number">1</span>
  selector<span class="token punctuation">:</span>
    matchLabels<span class="token punctuation">:</span>
      app<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
  template<span class="token punctuation">:</span>
    metadata<span class="token punctuation">:</span>
      labels<span class="token punctuation">:</span>
        app<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
    spec<span class="token punctuation">:</span>
      containers<span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> gcr.io/tetratelabs/hello<span class="token punctuation">-</span>world<span class="token punctuation">:</span>1.0.0
          imagePullPolicy<span class="token punctuation">:</span> Always
          name<span class="token punctuation">:</span> svc
          ports<span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">3000</span>
<span class="token punctuation">---</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
  ports<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
      name<span class="token punctuation">:</span> http
      targetPort<span class="token punctuation">:</span> <span class="token number">3000</span>
</code></pre><p>Apply this manifest to the cluster. Now if we run <code>curl</code> against GATEWAY_IP or open it in the browser, we will get back a response Hello World:</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>$ curl -v http://$GATEWAY_IP/
* Trying $GATEWAY_IP...
* TCP_NODELAY set
* Connected to $GATEWAY_IP ($GATEWAY_IP) port 80 (#0)
&gt; GET / HTTP/1.1
&gt; Host: $GATEWAY_IP
&gt; User-Agent: curl/7.64.1
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; date: Mon, 18 Jul 2022 02:59:37 GMT
&lt; content-length: 11
&lt; content-type: text/plain; charset=utf-8
&lt; x-envoy-upstream-service-time: 1
&lt; server: istio-envoy
&lt;
* Connection #0 to host $GATEWAY_IP left intact
Hello World* Closing connection 0
</code></pre><p>Also, notice the server header set to istio-envoy, indicating that the request went through the sidecar proxy.</p>
<h3 id="traffic-routing-in-istio">Traffic Routing in Istio </h3>
<p>Istio features a couple of resources we can use to configure how traffic is routed within the mesh. We have already mentioned the VirtualService and the Gateway resource in the Gateway section.</p>
<p>We can use the VirtualService resource to configure routing rules for services within the Istio service mesh. For example, in the <a href="https://istio.io/latest/docs/reference/config/networking/virtual-service/">VirtualService</a> resource, we match the incoming traffic based on the request properties and then route the traffic to one or more destinations. For example, once we match the traffic, we can split it by weight, inject failures and/or delays, mirror the traffic, and so on.</p>
<p>The <a href="https://istio.io/latest/docs/reference/config/networking/destination-rule/"><strong>DestinationRule</strong></a> resource contains the rules applied after routing decisions (from the VirtualService) have already been made. With the DestinationRule, we can configure how to reach the target service. For example, we can configure outlier detection, load balancer settings, connection pool settings, and TLS settings for the destination service.</p>
<p>The last resource we should mention is the <a href="https://istio.io/latest/docs/reference/config/networking/service-entry/"><strong>ServiceEntry</strong></a>. This resource allows us to take an external service or an API and make it appear as part of the mesh. The resource adds the external service to the internal service registry, allowing us to use Istio features such as traffic routing, failure injection, and others against external services.</p>
<h4 id="where-to-route-the-traffic">Where to Route the Traffic? </h4>
<p>Before the Envoy proxy can decide where to route the requests, we need a way to describe what our system and services look like.&nbsp;Let us look at an example where we have a <code>web-frontend</code> service and two versions (v1 and v2) of a customers service running in the cluster. Regarding resources, we have the following deployed in the cluster:</p>
<ul>
<li>Kubernetes deployments: <code>customers-v1</code>, <code>customers-v2</code> and <code>web-frontend</code>.</li>
<li>Kubernetes services: <code>web-frontend</code> and <code>customers</code></li>
</ul>
<p>To describe the different service versions, we use the concept of labels in Kubernetes. The pods created from the two customer deployments have the labels <code>version: v1</code> and <code>version: v2</code> set.</p>
<p>Note that we only have a single customers service that load-balances the traffic across all customer service pods (regardless of which deployment they were created from). How does Istio know or distinguish between the different versions of the service? We can set different labels in the pod spec template in each versioned deployment and then use these labels to make Istio aware of the two distinct versions or destinations for traffic. The labels are used in a construct called subset that can be defined in the DestinationRule.</p>
<p>To describe the two versions of the service, we would create a DestinationRule that looks like this:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> DestinationRule
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  host<span class="token punctuation">:</span> customers.default.svc.cluster.local
  subsets<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> v1
      labels<span class="token punctuation">:</span>
        version<span class="token punctuation">:</span> v1
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> v2
      labels<span class="token punctuation">:</span>
        version<span class="token punctuation">:</span> v2
</code></pre><p>Under the hood, when Istio translates these resources into Envoy proxy configuration, unique Envoy clusters get created that correspond to different versions. Istio takes the Kubernetes Service endpoints and applies the labels defined in the subsets to create separate collections of endpoints. The logical collection of these endpoints in Envoy is called a cluster.</p>
<p>The Envoy clusters are named by concatenating the traffic direction, port, subset name, and service hostname.</p>
<p>In the VirtualService, we can specify the traffic matching and routing rules that decide which destinations traffic is routed to. We have multiple options when deciding on how we want the traffic to be routed:</p>
<ul>
<li>Route based on weights</li>
<li>Match and route the traffic</li>
<li>Redirect the traffic (HTTP 301)</li>
<li>Mirror the traffic to another destination</li>
</ul>
<p>The above options of routing the traffic can be applied and used individually or together within the same VirtualService resource. Additionally, we can add, set or remove request and response headers, and configure CORS settings, timeouts, retries, and fault injection.</p>
<p>Let us look at some examples:</p>
<ol>
<li><strong>Weight-based routing</strong></li>
</ol>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: customers-route
spec:
  hosts:
  - customers.default.svc.cluster.local
  http:
  - name: customers-v1-routes
    route:
    - destination:
        host: customers.default.svc.cluster.local
        subset: v1
      weight: <span class="token number">70</span>
  - name: customers-v2-routes
    route:
    - destination:
        host: customers.default.svc.cluster.local
        subset: v2
      weight: <span class="token number">30</span>
</code></pre><p>In this example, we split the traffic based on weight to two subsets of the same service, where 70% goes to subset v1 and 30% to subset v2.</p>
<ol start="2">
<li><strong>Match and route the traffic</strong></li>
</ol>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> VirtualService
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers<span class="token punctuation">-</span>route
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  hosts<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> customers.default.svc.cluster.local
  http<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">match</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">headers</span><span class="token punctuation">:</span>
        user<span class="token punctuation">-</span><span class="token key atrule">agent</span><span class="token punctuation">:</span>
          regex<span class="token punctuation">:</span> <span class="token string">".*Firefox.*"</span>
    route<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
        host<span class="token punctuation">:</span> customers.default.svc.cluster.local
        subset<span class="token punctuation">:</span> v1
  <span class="token punctuation">-</span> <span class="token key atrule">route</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
        host<span class="token punctuation">:</span> customers.default.svc.cluster.local
        subset<span class="token punctuation">:</span> v2
</code></pre><p>In this example, we provide a regular expression and try to match the User-Agent header value. If the header value matches, we route the traffic to subset v1. Otherwise, if the User-Agent header value doesn’t match, we route the traffic to subset v2.</p>
<ol start="3">
<li><strong>Redirect the traffic</strong></li>
</ol>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> VirtualService
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers<span class="token punctuation">-</span>route
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  hosts<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> customers.default.svc.cluster.local
  http<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">match</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">uri</span><span class="token punctuation">:</span>
        exact<span class="token punctuation">:</span> /api/v1/helloWorld
    redirect<span class="token punctuation">:</span>
      uri<span class="token punctuation">:</span> /v1/hello
      authority<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world.default.svc.cluster.local
</code></pre><p>In this example, we combine the matching on the URI and then redirect the traffic to a different URI and a different service.</p>
<ol start="4">
<li><strong>Traffic mirroring</strong></li>
</ol>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> VirtualService
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers<span class="token punctuation">-</span>route
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  hosts<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> customers.default.svc.cluster.local
  http<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">route</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
        host<span class="token punctuation">:</span> customers.default.svc.cluster.local
        subset<span class="token punctuation">:</span> v1
      weight<span class="token punctuation">:</span> <span class="token number">100</span>
    mirror<span class="token punctuation">:</span>
      host<span class="token punctuation">:</span> customers.default.svc.cluster.local
      subset<span class="token punctuation">:</span> v2
    mirrorPercentage<span class="token punctuation">:</span>
      value<span class="token punctuation">:</span> <span class="token number">100.0</span>
</code></pre><p>In this example, we mirror 100% of the traffic to the v2 subset. Mirroring takes the same request sent to subset v1 and “mirrors” it to the v2 subset. The request is “fire and forget”. Mirroring can be used for testing and debugging the requests by mirroring the production traffic and sending it to the service version of our choice.</p>
<h3 id="advanced-traffic-routing">Advanced Traffic Routing </h3>
<p>Earlier, we learned how to route traffic between multiple subsets using the proportion of the traffic (weight field). In some cases, pure weight-based traffic routing or splitting is enough. However, there are scenarios and cases where we might need more granular control over how the traffic is split and forwarded to destination services. Istio allows us to use parts of the incoming requests and match them to the defined values. For example, we can check the URI prefix of the incoming request and route the traffic based on that. The table below shows the different properties we can match on.</p>
<p align="center">
    <img src="./assets/k8s/traffic-management.png" alt="drawing" width="500" height="300" style="center">
</p>
<p>Each of the above properties can get matched using one of these methods:</p>
<ul>
<li>Exact match: e.g., exact: "value" matches the exact string</li>
<li>Prefix match: e.g., prefix: "value" matches the prefix only</li>
<li>Regex match: e.g., regex: "value" matches based on the ECMAScript style regex</li>
<li></li>
</ul>
<p>For example, let's say the request URI looks like this:<br>
<code>htt‌ps://dev.example.com/v1/api</code>. To match the request URI, we could write the configuration like this:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">http</span><span class="token punctuation">:</span> 
<span class="token punctuation">-</span> <span class="token key atrule">match</span><span class="token punctuation">:</span> 
  <span class="token punctuation">-</span> <span class="token key atrule">uri</span><span class="token punctuation">:</span> 
      prefix<span class="token punctuation">:</span> /v1 
</code></pre><p>The above snippet would match the incoming request, and the request would get routed to the destination defined in that route. Another example would be using Regex and matching on a header:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token punctuation">...</span>
<span class="token key atrule">http</span><span class="token punctuation">:</span> 
<span class="token punctuation">-</span> <span class="token key atrule">match</span><span class="token punctuation">:</span> 
  <span class="token punctuation">-</span> <span class="token key atrule">headers</span><span class="token punctuation">:</span> 
      user<span class="token punctuation">-</span><span class="token key atrule">agent</span><span class="token punctuation">:</span> 
        regex<span class="token punctuation">:</span> <span class="token string">'.*Firefox.*'</span>
<span class="token punctuation">...</span>
</code></pre><p>The above match will match any request where the User Agent header matches the Regex.</p>
<h3 id="rewriting-and-redirecting-traffic">Rewriting and Redirecting Traffic </h3>
<p>In addition to matching on properties and then directing the traffic to a destination, sometimes we also need to rewrite the incoming URI or modify the headers. For example, let us consider a scenario where we match the incoming requests to the <code>/v1/api</code> path. Once matched, we want to route the requests to a different URI, <code>/v2/api</code>, for example. We can do that using the rewrite functionality.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token punctuation">...</span>
<span class="token key atrule">http</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">match</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">uri</span><span class="token punctuation">:</span>
        prefix<span class="token punctuation">:</span> /v1/api
    rewrite<span class="token punctuation">:</span>
      uri<span class="token punctuation">:</span> /v2/api
    route<span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
          host<span class="token punctuation">:</span> customers.default.svc.cluster.local
<span class="token punctuation">...</span>
</code></pre><p>The above snippet will match the prefix and then rewrite the matched prefix portion with the URI we provide in the rewrite field. Even though the destination service doesn't expose or listen on the <code>/v1/api</code> endpoint, we can rewrite those requests to a different path, <code>/v2/api</code> in this case. We also can redirect or forward the request to a completely different service. Here is how we could match on a header and then redirect the request to another service:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token punctuation">...</span>
<span class="token key atrule">http</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">match</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">headers</span><span class="token punctuation">:</span>
        my<span class="token punctuation">-</span><span class="token key atrule">header</span><span class="token punctuation">:</span>
          exact<span class="token punctuation">:</span> hello
    redirect<span class="token punctuation">:</span>
      uri<span class="token punctuation">:</span> /hello
      authority<span class="token punctuation">:</span> my<span class="token punctuation">-</span>service.default.svc.cluster.local<span class="token punctuation">:</span><span class="token number">8000</span>
<span class="token punctuation">...</span>
</code></pre><p>The redirect and destination fields are mutually exclusive. If we use the redirect, there is no need to set the destination.</p>
<hr>
<hr>
<h1 id="security-1">Security </h1>
<p>Security in Istio involves multiple components:</p>
<ul>
<li>A Certificate Authority (CA) for key and certificate management</li>
<li>The configuration API server distributes to the proxies:
<ul>
<li>authentication policies</li>
<li>authorization policies</li>
<li>secure naming information</li>
</ul>
</li>
<li>Sidecar and perimeter proxies work as Policy Enforcement Points (PEPs) to secure communication between clients and servers</li>
<li>A set of Envoy proxy extensions to manage telemetry and auditing.</li>
</ul>
<p>The control plane handles configuration from the API server and configures the PEPs in the data plane. The PEPs are implemented using Envoy.</p>
<h2 id="access-control">Access Control </h2>
<p>The question access control tries to answer is: can a principal perform an action on an object? For example, consider the following two questions:</p>
<ul>
<li>Can a user delete a file?</li>
<li>Can a user execute a script?</li>
</ul>
<p>The user in the above example is the principal, actions are deleting and executing, and an object is a file and a script. If we stated a similar example using Kubernetes and Istio, we would ask: "Can service A perform an action on service B?" The key terms are <strong>authentication</strong> and <strong>authorization</strong> when discussing security and answering the access control question.</p>
<h3 id="authentication-authn">Authentication (authn) </h3>
<p>Authentication is all about the principal or, in our case, about services and their identities. It’s an act of validating a credential and ensuring the credential is valid and authentic. Once the authentication is performed, we can say that we have an authenticated principal.</p>
<p>In Kubernetes, each workload automatically gets assigned a unique identity. This identity is used when workloads communicate. The identity in Kubernetes takes the form of a Kubernetes <strong>service account</strong>. The pods in Kubernetes use the service account as their identity and present it at runtime. Specifically, Istio uses the <strong>X.509 certificate</strong> from the service account, creating a new identity according to the specification called <strong>SPIFFE</strong> (Secure Production Identity Framework For Everyone). SPIFFE&nbsp; defines a universal naming scheme that’s based on a URI. For example <code>spiffe://cluster.local/ns/default/sa/my-service-account</code>. The spec describes how to take the identity and encode it into different documents. The document we care about in this case is the X.509 certificate.</p>
<p>Based on the SPIFFE specification, when we create an X.509 certificate and fill out the <strong>Subject Alternative Name</strong> (SAN) field based on the naming scheme above and check that name during the certificate validation, we can authenticate the identity encoded in the certificate. We get a valid SPIFFE identity which is an authenticated principal.</p>
<p>The Envoy proxy sidecars are modified and can perform the extra validation step required by SPIFFE (i.e., checking the SAN), allowing those authenticated principals to be used for policy decisions.</p>
<p>Istio provides two types of authentication:</p>
<ol>
<li><strong>Peer authentication</strong>: used for service-to-service authentication to verify the client making the connection. Istio offers <strong>mutual TLS</strong> as a full stack solution for transport authentication, which can be enabled without requiring service code changes. This solution:
<ul>
<li>Provides each service with a strong identity representing its role to enable interoperability across clusters and clouds.</li>
<li>Secures service-to-service communication.</li>
<li>Provides a key management system to automate key and certificate generation, distribution, and rotation.</li>
</ul>
</li>
<li><strong>Request authentication</strong>: Used for end-user authentication to verify the credential attached to the request. Istio enables request-level authentication with <strong>JSON Web Token (JWT)</strong> validation and a streamlined developer experience using a custom authentication provider or any OpenID Connect providers, for example:
<ul>
<li>ORY Hydra</li>
<li>Keycloak</li>
<li>OAuth</li>
<li>Firebase Auth</li>
<li>Google Auth</li>
</ul>
</li>
</ol>
<p>In all cases, Istio stores the authentication policies in the Istio <strong>config store</strong> via a custom Kubernetes API. Istiod keeps them up-to-date for each proxy, along with the keys where appropriate. You can specify authentication requirements for workloads receiving requests in an Istio mesh using peer and request authentication policies.</p>
<p>Client services, those that send requests, are responsible for following the necessary authentication mechanism. For request authentication, the application is responsible for acquiring and attaching the JWT credential to the request. For peer authentication, Istio automatically upgrades all traffic between two PEPs to mutual TLS. If authentication policies disable mutual TLS mode, Istio continues to use plain text between PEPs.</p>
<h4 id="mutual-tls">Mutual TLS </h4>
<p>By default, Istio creates its own CA to issue control plane and workload certificates that identify workload proxies. When a workload starts its envoy proxy requests a certificate from the istio control plane via <strong>istio-agent</strong>. You can disable the default Istio CA to use a custom CA such as using A<a href="https://github.com/aws-samples/istio-on-eks/tree/main/modules/04-security/peer-authentication">WS Private CA</a> as the external CA for Peer Authentication through Cert-Manager and its plugin “istio-csr”.</p>
<h5 id="provisioning-identities-at-runtime">Provisioning Identities at Runtime </h5>
<p>Istio securely provisions strong identities to every workload with X.509 certificates. Istio agents, running alongside each Envoy proxy, work together with istiod to automate key and certificate rotation at scale. The following diagram shows the identity provisioning flow.</p>
<p align="center">
    <img src="./assets/k8s/istio-cert.png" alt="drawing" width="500" height="400" style="center">
</p>
<p>The process of provisioning identities involves the following components of the service mesh:</p>
<ul>
<li>Istio agent, running in the sidecar</li>
<li>Envoy’s Secret Discovery Service (SDS)</li>
<li>Citadel (part of Istio control plane)</li>
</ul>
<p>The Istio agent (a binary called pilot-agent) runs in the sidecar and works with Istio’s control plane to automate key and certificate rotation. Even though the agent runs in the sidecar containers, it’s still considered part of the control plane.</p>
<p align="center">
    <img src="./assets/k8s/cert-issuence-istio.png" alt="drawing" width="500" height="400" style="center">
</p>
<p>The Envoy proxy in the sidecar gets its initial bootstrap configuration as a JSON file. Amongst other settings, the JSON file configures the Istio agent as the SDS server, telling the Envoy proxy to go to the SDS server for any certificate/key needs. For example, the SDS server will automatically push the certificate to the Envoy proxy, removing the need for creating the certificates as Kubernetes secrets and mounting them inside the containers’ file system. The Istio agent is responsible for launching the Envoy proxy. When the Envoy proxy starts, it reads its bootstrap configuration and sends the SDS request to the Istio agent, telling it the workloads service account.</p>
<h5 id="certificate-issuance-flow-in-istio">Certificate issuance flow in Istio </h5>
<p>The Istio agent sends a request to the certificate authority (CA), which in this case is Citadel, for a new workload certificate. The Citadel component plays the role of the certificate authority. By default, Citadel uses a self-signed root certificate to sign all workload certificates. We can change that by providing our root certificate, signing certificate, and key for Citadel to use.</p>
<p>The request to the CA involves creating a certificate signing request (CSR) and includes proof of the workloads’ service account. In Kubernetes, that’s the pods’ service account JWT. Next, Citadel performs authentication and authorization and responds with a signed X.509 certificate. The Istio agent takes the signed certificate and caches it in memory. Additionally, the Istio agent registers the certificate for automatic rotation before the certificate expires. The default expiration of mesh certificates is <strong>24 hours</strong>. This value is configurable and can be changed. In the last step, the Istio agent streams back the signed certificate to the Envoy proxy via SDS over a Unix domain socket. This allows Envoy proxy to use the certificate when needed.</p>
<p>Once workloads have a strong identity, we can use them at runtime to do mutual TLS authentication (mTLS). Traditionally, TLS is done one way. Let’s take the example of going to <a href="https://google.com">https://google.com</a>. If you navigate to the page, you will notice the lock icon, and you can click on it to get the certificate details. However, we did not give any proof of identity to <a href="http://google.com">google.com</a>, we just opened the website. This is where mTLS is fundamentally different.&nbsp;When two services try to communicate using mTLS, it’s required that both of them provide certificates to each other. That way, both parties know the identity of who they are talking to. Using mTLS both client and server verify each others’ identities.</p>
<p>As we already learned, all communication between workloads goes through the Envoy proxies. When a workload sends a request to another, Istio re-routes the traffic to the sidecar Envoy proxy, regardless of whether mTLS or plain text communication is used. In the case of mTLS, once the mTLS connection is established, the request is forwarded from the client Envoy proxy to the server-side Envoy proxy. Then, the sidecar Envoy starts an mTLS handshake with the server-side Envoy. The workloads themselves aren’t performing the mTLS handshake - it’s the Envoy proxies doing the work. During the handshake, the caller does a secure naming check to verify the service account in the server certificate is authorized to run the target service. After the authorization on the server-side, the sidecar forwards the traffic to the workload.</p>
<p>The sidecars are involved in intercepting the incoming inbound traffic and facilitating or sending the outbound traffic. For that reason, two distinct resources control the inbound and outbound traffic. The <strong>PeerAuthentication</strong> for inbound traffic and the <strong>DestinationRule</strong> for outbound traffic.</p>
<p align="center">
    <img src="./assets/k8s/sec-auth .png" alt="drawing" width="500" height="300" style="center">
</p>
<p>PeerAuthentication is used to configure mTLS settings for inbound traffic, and DestinationRule for configuring TLS settings for outbound traffic.</p>
<h5 id="inbound-traffic-to-the-proxy">Inbound Traffic to the Proxy </h5>
<p>The resource that configures what type of mTLS traffic the sidecar accepts is PeerAuthentication. It supports four operating modes, as shown in the table below.</p>
<p></p><p align="center"><br>
<img src="./assets/k8s/mtls-inbound.png" alt="drawing" width="500" height="300" style="center"></p>
<p></p>
<p>The default mTLS mode is PERMISSIVE, which means that if a client tries to connect to a service via mTLS, the Envoy proxy will serve mTLS. The server will respond in plain text if the client uses plain text. With this setting, we are allowing the client to choose whether to use mTLS or not. The permissive mode is useful to help you understand how a policy change can affect your security posture before it is enforced. It helps when onboarding existing applications to the service mesh as it allows us to roll out mTLS to all workloads gradually. Once all applications are onboarded, we can turn on the STRICT mode. The strict mode says that workloads can only communicate using mTLS. The connection will be closed if a client tries to connect without presenting their certificate.</p>
<p>We can configure the mTLS mode at the mesh, namespace, workload, and port level. While Istio automatically upgrades all traffic between the proxies and the workloads to mutual TLS, workloads can still receive plain text traffic in permissive mode which is the default mode. To prevent non-mutual TLS traffic for the whole mesh, set a mesh-wide peer authentication policy with the mutual TLS mode set to STRICT. The mesh-wide peer authentication policy should not have a selector and must be applied in the root namespace.</p>
<p>Similarly we might prefer to set the STRICT mode at the namespace level and then individually set permissive mode or disable mTLS at either workload level using selectors or at the individual port level.</p>
<p>Here’s an example of how we could set STRICT mTLS mode for all workloads matching a specific label:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> security.istio.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PeerAuthentication
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> default
  namespace<span class="token punctuation">:</span> foo
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    matchLabels<span class="token punctuation">:</span>
      app<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
  mtls<span class="token punctuation">:</span>
    mode<span class="token punctuation">:</span> STRICT
</code></pre><h5 id="outbound-traffic-from-the-proxy">Outbound Traffic from the Proxy </h5>
<p>We use the DestinationRule resource to control and configure what type of TLS traffic the sidecar sends.</p>
<p>The supported TLS modes are shown in the table below.</p>
<p align="center">
    <img src="./assets/k8s/mtls-outbound.png" alt="drawing" width="500" height="300" style="center">
</p>
<p>Based on the configuration, the traffic can be sent as plain text (DISABLE), or a TLS connection can be initiated using SIMPLE for TLS and MUTUAL or ISTIO_MUTUAL for mTLS.</p>
<p>If the DestinationRule and the TLS mode are not explicitly set, the sidecar uses Istio’s certificates to initiate the mTLS connection. That means, by default, all traffic inside the mesh is encrypted. The configuration can be applied to individual hosts by their fully qualified domain names (e.g., <code>hello-world.my-namespace.svc.cluster.local</code>). A workload selector with labels can be set up to select specific workloads on which to apply the configuration.</p>
<p>We can configure specific ports on the workloads to apply the settings for even more fine-grained control.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> DestinationRule
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> tls<span class="token punctuation">-</span>example
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  host<span class="token punctuation">:</span> <span class="token string">"*.example.com"</span>
  trafficPolicy<span class="token punctuation">:</span>
    tls<span class="token punctuation">:</span>
      mode<span class="token punctuation">:</span> SIMPLE
</code></pre><p>The above YAML specifies that TLS connection should be initiated when talking to services whose domain matches the <code>*.example.com</code>.</p>
<h4 id="gateways-and-tls">Gateways and TLS </h4>
<p>In the case of gateways, we also talk about inbound and outbound connections. With ingress gateways, the inbound connection typically comes from the outside of the mesh (i.e., a request from the browser, curl, etc.), and the outgoing connection goes to services inside the mesh. The inverse applies to the egress gateway, where the inbound connection typically comes from within the mesh, and outbound connections go outside the mesh. In both cases, the configuration and resources used are identical. Behind the scenes, the mesh's ingress and egress gateway deployments are identical - it’s the configuration that specializes and adapts them for either ingress or egress traffic. The configuration of gateways is controlled with the Gateway resource.</p>
<p>The tls field in the Gateway resource controls how the gateway decodes the inbound traffic. The protocol field specifies whether the inbound connection is plaintext HTTP, HTTPS, GRCP, MONGO, TCP, or TLS. When the inbound connection is TLS, we have a couple of options to control the behavior of the gateway. <em>Do we want to terminate the TLS connection, pass it through, or do mTLS?</em> These additional options can be configured using the TLS mode in the Gateway resource. The table below describes the different TLS modes.</p>
<p align="center">
    <img src="./assets/k8s/tls-gateway.png" alt="drawing" width="500" height="500" style="center">
</p>
<p>Other TLS-related configurations in the Gateway resource, such as TLS versions, can be found in the <a href="https://istio.io/latest/docs/reference/config/networking/gateway/#ServerTLSSettings">documentation</a> and <a href="https://istio.io/latest/docs/tasks/traffic-management/ingress/secure-ingress/">secure gateways</a>. When configuring TLS settings for outgoing traffic through the egress gateway, the same configuration applies to the regular Envoy sidecar proxies in the mesh - the DestinationRule is used.</p>
<h3 id="mutual-tls---lab">Mutual TLS - Lab </h3>
<p>In this lab, we will deploy the sample application (web-frontend and customers service). The web-frontend will be deployed without an Envoy proxy sidecar, while the customers service will have the sidecar injected. With this setup, we will see how Istio can send both mTLS and plain text traffic and change the TLS mode to STRICT.</p>
<p>Let's start by deploying a Gateway resource:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Gateway
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> gateway
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    istio<span class="token punctuation">:</span> ingressgateway
  servers<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span>
        number<span class="token punctuation">:</span> <span class="token number">80</span>
        name<span class="token punctuation">:</span> http
        protocol<span class="token punctuation">:</span> HTTP
      hosts<span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token string">'*'</span>
</code></pre><p>NOTE: You can download the supporting YAML and other files from this Github repo.</p>
<p>Save the above YAML to <code>gateway.yaml</code> and deploy the Gateway using the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> gateway.yaml
</code></pre><p>Next, we will create the web-frontend, the customers-v1 deployments, and related Kubernetes services.</p>
<h5 id="disabling-sidecar-injection">Disabling Sidecar Injection </h5>
<p>Before deploying the applications, we will disable the automatic sidecar injection in the default namespace so the proxy doesn't get injected into the web-frontend deployment. Before we deploy the customers-v1 service, we will enable the injection again so the workload gets the proxy injected.</p>
<p>We are doing this to simulate a scenario where one workload is not part of the mesh.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl label namespace default istio-injection-
namespace/default labeled
</code></pre><p>With injection disabled, let's create the web-frontend deployment, service, and the VirtualService resource:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  replicas<span class="token punctuation">:</span> <span class="token number">1</span>
  selector<span class="token punctuation">:</span>
    matchLabels<span class="token punctuation">:</span>
      app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
  template<span class="token punctuation">:</span>
    metadata<span class="token punctuation">:</span>
      labels<span class="token punctuation">:</span>
        app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
        version<span class="token punctuation">:</span> v1
    spec<span class="token punctuation">:</span>
      containers<span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> gcr.io/tetratelabs/web<span class="token punctuation">-</span>frontend<span class="token punctuation">:</span>1.0.0
          imagePullPolicy<span class="token punctuation">:</span> Always
          name<span class="token punctuation">:</span> web
          ports<span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
          env<span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CUSTOMER_SERVICE_URL
              value<span class="token punctuation">:</span> <span class="token string">'ht‌tp://customers.default.svc.cluster.local'</span>
<span class="token punctuation">---</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
  ports<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
      name<span class="token punctuation">:</span> http
      targetPort<span class="token punctuation">:</span> <span class="token number">8080</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> VirtualService
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  hosts<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token string">'*'</span>
  gateways<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> gateway
  http<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">route</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
            host<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend.default.svc.cluster.local
            port<span class="token punctuation">:</span>
              number<span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre><p>Save the above YAML to <code>web-frontend.yaml</code> and create the deployment, service, and VirutalService using the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> web-frontend.yaml
</code></pre><p>If we look at the running Pods, we should see one Pod with a single container running, indicated by the 1/1 in the READY column:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get po
NAME                          READY  STATUS   RESTARTS  AGE
web-frontend-659f65f49-cbhvl  <span class="token number">1</span>/1    Running  <span class="token number">0</span>         7m31s
</code></pre><p>Let's re-enable the automatic sidecar proxy injection:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl label namespace default istio-injection<span class="token operator">=</span>enabled
namespace/default labeled
</code></pre><p>And then deploy the customers-v1 workload:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers<span class="token punctuation">-</span>v1
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> customers
    version<span class="token punctuation">:</span> v1
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  replicas<span class="token punctuation">:</span> <span class="token number">1</span>
  selector<span class="token punctuation">:</span>
    matchLabels<span class="token punctuation">:</span>
      app<span class="token punctuation">:</span> customers
      version<span class="token punctuation">:</span> v1
  template<span class="token punctuation">:</span>
    metadata<span class="token punctuation">:</span>
      labels<span class="token punctuation">:</span>
        app<span class="token punctuation">:</span> customers
        version<span class="token punctuation">:</span> v1
  spec<span class="token punctuation">:</span>
    containers<span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> gcr.io/tetratelabs/customers<span class="token punctuation">:</span>1.0.0
        imagePullPolicy<span class="token punctuation">:</span> Always
        name<span class="token punctuation">:</span> svc
        ports<span class="token punctuation">:</span>
          <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">3000</span>
<span class="token punctuation">---</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> customers
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> customers
  ports<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
      name<span class="token punctuation">:</span> http
      targetPort<span class="token punctuation">:</span> <span class="token number">3000</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> VirtualService
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  hosts<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token string">'customers.default.svc.cluster.local'</span>
  http<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">route</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
            host<span class="token punctuation">:</span> customers.default.svc.cluster.local
            port<span class="token punctuation">:</span>
             number<span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre><p>Save the above to <code>customers-v1.yaml</code> and crete the resources using the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> customers-v1.yaml
</code></pre><p>We should have both applications deployed and running. The <code>customers-v1</code> pod will have two containers, and the <code>web-frontend</code> pod will have one:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get po 
NAME                           READY  STATUS   RESTARTS  AGE
customers-v1-7857944975-qrqsz  <span class="token number">2</span>/2    Running  <span class="token number">0</span>         4m1s
web-frontend-659f65f49-cbhvl   <span class="token number">1</span>/1    Running  <span class="token number">0</span>         13m
</code></pre><h5 id="permissive-mode-in-action">Permissive Mode in Action </h5>
<p>Let’s set the environment variable called GATEWAY_IP that stores the gateway IP address:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">GATEWAY_IP</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>kubectl get svc <span class="token parameter variable">-n</span> istio-system istio-ingressgateway <span class="token parameter variable">-ojsonpath</span><span class="token operator">=</span><span class="token string">'{.status.loadBalancer.ingress[0].ip}'</span><span class="token variable">)</span></span>
</code></pre><p>If we try to navigate to the GATEWAY_IP, we will get the web page with the customer service's response. Accessing the GATEWAY_IP works because of the permissive mode, where plain text traffic gets sent to the services that do not have the proxy. In this case, the ingress gateway sends plain text traffic to the web-frontend because there's no proxy.</p>
<h5 id="observing-the-permissive-mode-in-kiali">Observing the Permissive Mode in Kiali </h5>
<p>To  install observability tools such as <strong>Kiali</strong>, <strong>Prometheus</strong>, <strong>Grafana</strong>, <strong>Jaeger</strong>, look at <a href="https://istio.io/latest/docs/tasks/observability/gateways/">https://istio.io/latest/docs/tasks/observability/gateways/</a>  to build a secure (https) or insecure (http) access for these services.</p>
<p>If we open Kiali with <code>istioctl dash kiali</code> and look at the Graph, you will notice that Kiali detects calls made from the ingress gateway to the web frontend. Make sure you select both the default namespace and the <code>istio-system</code> namespace. However, the calls made to the customers service are coming from unknown service. This is because there's no proxy next to the web-frontend. Therefore Istio doesn't know who, where or what that service is. Calls to customers-v1 service are coming from an unknown service because there’s no proxy injected</p>
<p align="center">
    <img src="./assets/k8s/mtls-permissive.png" alt="drawing" width="500" height="400" style="center">
</p>
<h5 id="exposing-the-customers-service">Exposing the "customers" Service </h5>
<p>Let's update the customers VirtualService and attach the gateway to it. Attaching the gateway allows us to make calls directly to the service.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> VirtualService
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  hosts<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token string">'customers.default.svc.cluster.local'</span>
  gateways<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> gateway
  http<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">route</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
            host<span class="token punctuation">:</span> customers.default.svc.cluster.local
            port<span class="token punctuation">:</span>
              number<span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre><p>Save the above to <code>vs-customers-gateway.yaml</code> and update the VirtualService using the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> vs-customers-gateway.yaml
</code></pre><h5 id="generating-traffic-to-the-customers-service">Generating Traffic to the "customers" Service </h5>
<p>We can now specify the Host header and send the requests through the ingress gateway (GATEWAY_IP) to the customers service:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">-H</span> <span class="token string">"Host: customers.default.svc.cluster.local"</span> http://<span class="token variable">$GATEWAY_IP</span>
<span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Jewel Schaefer"</span><span class="token punctuation">}</span>,<span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Raleigh Larson"</span><span class="token punctuation">}</span>,<span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Eloise Senger"</span><span class="token punctuation">}</span>,<span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Moshe Zieme"</span><span class="token punctuation">}</span>,<span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Filiberto Lubowitz"</span><span class="token punctuation">}</span>,<span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Ms.Kadin Kling"</span><span class="token punctuation">}</span>,<span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Jennyfer Bergstrom"</span><span class="token punctuation">}</span>,<span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Candelario Rutherford"</span><span class="token punctuation">}</span>,<span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Kenyatta Flatley"</span><span class="token punctuation">}</span>,<span class="token punctuation">{</span><span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"Gianni Pouros"</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
</code></pre><p>To generate some traffic to both the <code>web-frontend</code> and <code>customers-v1</code> deployments through the ingress, open the two terminal windows and run one command in each:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>// Terminal <span class="token number">1</span>
$ <span class="token keyword keyword-while">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword keyword-do">do</span> <span class="token function">curl</span> <span class="token parameter variable">-H</span> <span class="token string">"Host: customers.default.svc.cluster.local"</span> http://<span class="token variable">$GATEWAY_IP</span><span class="token punctuation">;</span> <span class="token keyword keyword-done">done</span>
// Terminal <span class="token number">2</span>
$ <span class="token keyword keyword-while">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword keyword-do">do</span> <span class="token function">curl</span> ht‌tp://<span class="token variable">$GATEWAY_IP</span><span class="token punctuation">;</span> <span class="token keyword keyword-done">done</span>
</code></pre><h5 id="observing-the-traffic-in-kiali">Observing the Traffic in Kiali </h5>
<p>Open Kiali and look at the Graph. From the Display dropdown, make sure you check the Security option. You should see a graph showing the traffic between istio-ingressgateway and customers-v1 is using mTLS as indicated by the padlock icon which means the traffic gets sent using mTLS.</p>
<p>NOTE: If you do not see the padlock icon, click the Display dropdown and ensure the "Security" option is selected.</p>
<p>However, there's no padlock between the unknown and the <code>customers-v1</code> service, as well as the istio-ingress-gateway and web-frontend. Proxies send plain text traffic between services that do not have the sidecar injected.</p>
<h5 id="enabling-strict-mode">Enabling "STRICT" mode </h5>
<p>Let's see what happens if we enable mTLS in STRICT mode. We expect the calls from the web-frontend to the customers-v1 service to fail because there's no proxy injected to do the mTLS communication. On the other hand, the calls from the ingress gateway to the customers-v1 service will continue working.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> security.istio.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PeerAuthentication
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> default
  namespace<span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  mtls<span class="token punctuation">:</span>
    mode<span class="token punctuation">:</span> STRICT
</code></pre><p>Save the above YAML to <code>strict-mtls.yaml</code> and create the PeerAuthentication resource using the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> strict-mtls.yaml
</code></pre><p>If we still have the request loop running, we will see the ECONNRESET error message from the web-frontend. This error indicates that the <code>customers-v1</code> service closed the connection. In our case, it was because it was expecting an mTLS connection. On the other hand, the requests we make directly to the customers-v1 service continue to work because the customers-v1 service has an Envoy proxy running next to it and can do mTLS. If we delete the PeerAuthentication resource deployed earlier using <code>kubectl delete peerauthentication default</code>, Istio returns to its default, the PERMISSIVE mode, and the errors will disappear.</p>
<h6 id="cleanup">Cleanup </h6>
<p>To clean up the resources, run:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl delete deploy web-frontend customers-v1
$ kubectl delete svc customers web-frontend
$ kubectl delete vs customers web-frontend
$ kubectl delete gateway gateway
</code></pre><h2 id="authenticating-users">Authenticating Users </h2>
<p>While the PeerAuthentication resource is used to control service authentication, the <strong>RequestAuthentication</strong> resource is used for end-user authentication. The authentication is done per request and verifies the credentials attached to the request in JSON Web Tokens (JWTs). Just like we used the SPIFFE identity to identify services, we can use JWT to authenticate users.</p>
<p>Request authentication policies specify the values needed to validate a JSON Web Token (JWT). These values include, among others, the following:</p>
<ul>
<li>The location of the token in the request</li>
<li>The issuer or the request</li>
<li>The public JSON Web Key Set (JWKS)</li>
</ul>
<p>Istio checks the presented token against the rules in the request authentication policy, and rejects requests with invalid tokens. When requests carry no token, they are accepted by default. To reject requests without tokens, provide authorization rules that specify the restrictions for specific operations, for example paths or actions.</p>
<p>Let’s look at an example RequestAuthentication resource:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> security.istio.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> RequestAuthentication
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
<span class="token key atrule">name</span><span class="token punctuation">:</span> customers<span class="token punctuation">-</span>v1
<span class="token key atrule">namespace</span><span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
 selector<span class="token punctuation">:</span>
   matchLabels<span class="token punctuation">:</span>
     app<span class="token punctuation">:</span> customers<span class="token punctuation">-</span>v1
 jwtRules<span class="token punctuation">:</span>
 <span class="token punctuation">-</span> <span class="token key atrule">issuer</span><span class="token punctuation">:</span> <span class="token string">"issuer@tetrate.io"</span>
   jwksUri<span class="token punctuation">:</span> <span class="token string">"someuri"</span>
</code></pre><p>The above resource applies to all workloads in the default namespace that match the selector labels. The resource is saying that any requests made to those workloads will need a JWT attached to the request. The RequestAuthentication resource configures how the token and its signature are authenticated using the settings in the jwtRules field. If the request doesn’t have a valid JWT attached, the request will be rejected because the token doesn’t conform to JWT rules. The request will not be authenticated if we do not provide a token. If the token is valid, then we have an authenticated principal. We can use the principal to configure authorization policies.</p>
<h3 id="authorization-authz">Authorization (authz) </h3>
<p>Authorization is answering the access control question. Is a principal allowed to perform an action on an object? Even though we might have an authenticated principal, we still might not be able to perform a certain action.&nbsp;For example, you can be an authenticated user, but you won’t be able to perform administrative actions if you are not authorized. Similarly, a service can be authenticated but is not allowed to make POST requests to other services, for example.</p>
<p>To properly enforce access control, we need both authentication and authorization. <em>If we only authenticate the principles without authorizing them, the principals can perform any action on any objects.</em> Similarly, if we only authorize the actions or requests, a principal can pretend to be someone else and perform all actions again.</p>
<p>Istio’s authorization features provide mesh-, namespace-, and workload-wide access control for your workloads in the mesh. This level of control provides the following benefits:</p>
<ul>
<li>Workload-to-workload and end-user-to-workload authorization.</li>
<li>A simple API: it includes a single AuthorizationPolicy CRD, which is easy to use and maintain.</li>
<li>Flexible semantics: use <code>CUSTOM</code>, <code>DENY</code> and <code>ALLOW</code> actions.</li>
<li>High performance: Istio authorization (ALLOW and DENY) is enforced natively on Envoy.</li>
<li>High compatibility: supports <code>gRPC</code>, <code>HTTP</code>, <code>HTTPS</code> and <code>HTTP/2</code> natively, as well as any plain <code>TCP</code> protocols.</li>
</ul>
<h4 id="authorization-policies">Authorization Policies </h4>
<p>The authorization policy enforces access control to the inbound traffic in the server side Envoy proxy. Each Envoy proxy runs an authorization engine that authorizes requests at runtime. When a request comes to the proxy, the authorization engine evaluates the request context against the current authorization policies, and returns the authorization result, either ALLOW or DENY.</p>
<p>To configure an authorization policy, you create an <strong>AuthorizationPolicy</strong> custom resource. An authorization policy includes a <strong>selector</strong>, an <strong>action</strong>, and a list of <strong>rules</strong>:</p>
<ul>
<li>The selector field specifies the target of the policy</li>
<li>The action field specifies whether to allow or deny the request</li>
<li>The rules specify when to trigger the action</li>
<li>The <strong>from</strong> field in the rules specifies the sources of the request</li>
<li>The <strong>to</strong> field in the rules specifies the operations of the request</li>
<li>The <strong>when</strong> field specifies the conditions needed to apply the rule</li>
</ul>
<p>For more info see istio <a href="https://istio.io/latest/docs/concepts/security/">security</a>, <a href="https://istio.io/latest/docs/tasks/security/authorization/authz-tcp/">authz-tcp</a> and <a href="https://istio.io/latest/docs/tasks/security/authorization/authz-http/">authz-http</a>.</p>
<p>When you use peer authentication policies and mutual TLS, Istio extracts the identity from the peer authentication into the <code>source.principal</code>. Similarly, when you use request authentication policies, Istio assigns the identity from the JWT to the <code>request.auth.principal</code>.</p>
<p>Whether we get the principals from the PeerAuthentication resource or RequestAuthentication, we can use these identities to write authorization policies with the AuthorizationPolicy resource. We can use the <strong>principal</strong> field to write policies based on service identities and the <strong>requestPrincipals</strong> field to write policies based on users.</p>
<p>Let’s look at an example:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> security.istio.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> AuthorizationPolicy
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> require<span class="token punctuation">-</span>jwt
  namespace<span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span> 
 selector<span class="token punctuation">:</span>
   matchLabels<span class="token punctuation">:</span>
     app<span class="token punctuation">:</span> hello<span class="token punctuation">-</span>world
 action<span class="token punctuation">:</span> ALLOW
<span class="token key atrule">rules</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">source</span><span class="token punctuation">:</span>
     requestPrincipals<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"*"</span><span class="token punctuation">]</span>
</code></pre><p>The policy applies to all workloads in the default namespace matching the selector labels. Each AuthorizationPolicy includes an action - that says whether the action is allowed or denied based on the rules set in the rules field. Note that there are other actions we can set, and we will explain them later. In this example, we aren’t checking for any specific principal. Instead, we just need a principal to be set. With this combination and a corresponding RequestAuthentication resource, we guarantee that only authenticated requests will reach the workloads in the default namespace, labeled with the app: hello-world label.</p>
<p>Let’s look at the three possible scenarios we can encounter in this case:</p>
<ul>
<li>
<p>Token is not present: it means that the request is not authenticated (that’s what RequestAuthentication resource ensures). Because the request is not authenticated, the requestPrincipal field value won’t be set. However, because we require a request principal to be set (the * notation) in the AuthorizationPolicy, but there isn’t one, the request will be denied</p>
</li>
<li>
<p>Token is present but invalid: The validity of the JWT is checked by the RequestAuthentication. If that fails, the AuthorizationPolicy won’t even be processed.</p>
</li>
<li>
<p>Token is valid: the requestPrincipals field will be set. In the rules field of the AuthorizationPolicy, we only allow the calls to be made from sources with the request principal set. Since that is set, we can send the requests to the workloads with the <code>app: hello-world</code> labels set.</p>
</li>
</ul>
<h5 id="sources-identities-from-field">Sources Identities ("from" field) </h5>
<p>Using the from field we can set the source identities of a request. The sources we can use are:</p>
<ul>
<li><strong>principals</strong></li>
<li><strong>request principals</strong></li>
<li><strong>namespaces</strong></li>
<li><strong>IP blocks</strong></li>
<li><strong>remote IP blocks</strong></li>
</ul>
<p>We can also provide a list of negative matches for each one of the sources and combine them with logical <code>AND</code>.</p>
<p>For example:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">rules</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">source</span><span class="token punctuation">:</span>
      ipBlocks<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"10.0.0.1"</span><span class="token punctuation">,</span> “10.0.0.2”<span class="token punctuation">]</span>
  <span class="token punctuation">-</span> <span class="token key atrule">source</span><span class="token punctuation">:</span>
      notNamespaces<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"prod"</span><span class="token punctuation">]</span>
  <span class="token punctuation">-</span> <span class="token key atrule">source</span><span class="token punctuation">:</span>
      requestPrincipals<span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token string">"tetrate.io/peterj"</span>
</code></pre><p>The above rules would apply to sources coming from one of the two IP addresses that are not in the prod namespace and have the request principal set to <code>etrate.io/peterj</code>.</p>
<h5 id="request-operation-to-field">Request Operation ("to" field) </h5>
<p>The <code>to</code> field specifies the operations of a request. We can use the following operations:</p>
<ul>
<li>hosts</li>
<li>ports</li>
<li>methods</li>
<li>paths</li>
</ul>
<p>For each of the above, we can set the list of negative matches and combine them with the logical <code>AND</code>. For example:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">to</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">operation</span><span class="token punctuation">:</span>
      host<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"*.hello.com"</span><span class="token punctuation">]</span>
      methods<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"DELETE"</span><span class="token punctuation">]</span>
      notPaths<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"/admin*"</span><span class="token punctuation">]</span>
</code></pre><p>The above operation matches if the host ends with <code>*.hello.com</code> and the method is <code>DELETE</code>, but the path doesn’t start with <code>/admin</code>.</p>
<h5 id="conditions-when-field">Conditions ("when" field) </h5>
<p>With the when field, we can specify any additional conditions based on the Istio attributes. The attributes include headers, source and remote IP address, auth claims, destination port and IP addresses, and others.</p>
<p>For example:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">when</span><span class="token punctuation">:</span>
   <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> request.auth.claims<span class="token punctuation">[</span>iss<span class="token punctuation">]</span>
     values<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"ht‌tps://accounts.google.com"</span><span class="token punctuation">]</span>
   <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> request.headers<span class="token punctuation">[</span>User<span class="token punctuation">-</span>Agent<span class="token punctuation">]</span>
     notValues<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"curl/*"</span><span class="token punctuation">]</span>
</code></pre><p>The above condition evaluates to true if the iss claim from the authenticated JWT equals the provided value and the User Agent header doesn’t start with <code>curl/</code>.</p>
<h5 id="configure-the-action-field">Configure the "action" Field </h5>
<p>Once we have written the rules, we can also configure the action field. We can either <code>ALLOW</code> or <code>DENY</code> the requests matching those rules. The additional supported actions are <code>CUSTOM</code> and <code>AUDIT</code>.</p>
<p>For example:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span>
  action<span class="token punctuation">:</span> DENY
  rules<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
    to<span class="token punctuation">:</span>
    when<span class="token punctuation">:</span>
    <span class="token punctuation">...</span>
</code></pre><p>The <code>CUSTOM</code> action is when we specify our custom extension to handle the request. The custom extension needs to be configured in the <code>MeshConfig</code>. An example of using this is if we wanted to integrate a custom external authorization system to delegate the authorization decisions to it. Note that the `CUSTOM action is experimental, so it might break or change in future Istio versions.</p>
<p>The <code>AUDIT</code> action can be used to audit a request that matches the rules. If the request matches the rules, the <code>AUDIT</code> action triggers logging that request. This action doesn’t affect whether the requests are allowed or denied. Only <code>DENY</code>, <code>ALLOW</code>, and <code>CUSTOM</code> actions can do that. A sample scenario for when one would use the <code>AUDIT</code> action is when you are migrating workloads from <code>PERMISSIVE</code> to <code>STRICT</code> mTLS mode.</p>
<h4 id="how-are-rules-evaluated">How Are Rules Evaluated? </h4>
<p>If we set any authorization policies, everything will be allowed. However, requests that do not conform to the allow policy rules will be denied as soon as we set any <code>ALLOW</code> policies. The rules get evaluated in the following order:</p>
<ul>
<li>CUSTOM policies</li>
<li>DENY policies</li>
<li>ALLOW policies</li>
</ul>
<p>If we have multiple policies defined, they are aggregated and evaluated, starting with the <code>CUSTOM</code> policies. So, if we have two rules - one for <code>ALLOW</code> and one for <code>DENY</code>, the <code>DENY</code> rule will be enforced. If we use all three actions, the <code>CUSTOM</code> action is evaluated first.</p>
<p>Assume we have defined policies that use all three actions. We will evaluate the <code>CUSTOM</code> policies and then deny or allow the request based on that. Next, the <code>DENY</code> policies matching the request get evaluated. If we get past those, and if there are no ALLOW policies set, we will allow the request. If any <code>ALLOW</code> policies match the request, we will also allow it. If they do not, the request gets denied. For a complete flowchart of authorization policy precedence, refer to <a href="https://istio.io/latest/docs/concepts/security/#implicit-enablement">this</a> page.</p>
<p>A good practice is to create a policy denying all requests. Once we have that in place, we can create individual ALLOW policies and explicitly allow communication between services.</p>
<h3 id="access-control---lab">Access Control - Lab </h3>
<p>In this lab, we will learn how to use an authorization policy to control access between workloads. Let's start by deploying the Gateway:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Gateway
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> gateway
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    istio<span class="token punctuation">:</span> ingressgateway
  servers<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span>
        number<span class="token punctuation">:</span> <span class="token number">80</span>
        name<span class="token punctuation">:</span> http
        protocol<span class="token punctuation">:</span> HTTP
      hosts<span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token string">'*'</span>
</code></pre><p>NOTE: You can download the supporting YAML and other files from <a href="https://tetr8.io/cncf-source-files">this</a> Github repo.</p>
<p>Save the above YAML to file and apply it to the cluster. Next, we will create the <code>web-frontend</code> deployment, service account, Kubernetes service, and a VirtualService.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  replicas<span class="token punctuation">:</span> <span class="token number">1</span>
  selector<span class="token punctuation">:</span>
    matchLabels<span class="token punctuation">:</span>
      app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
  template<span class="token punctuation">:</span>
    metadata<span class="token punctuation">:</span>
      labels<span class="token punctuation">:</span>
        app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
        version<span class="token punctuation">:</span> v1
    spec<span class="token punctuation">:</span>
      serviceAccountName<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
      containers<span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> gcr.io/tetratelabs/web<span class="token punctuation">-</span>frontend<span class="token punctuation">:</span>1.0.0
          imagePullPolicy<span class="token punctuation">:</span> Always
          name<span class="token punctuation">:</span> web
          ports<span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
          env<span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CUSTOMER_SERVICE_URL
              value<span class="token punctuation">:</span> <span class="token string">'ht‌tp://customers.default.svc.cluster.local'</span>
<span class="token punctuation">---</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
  ports<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
      name<span class="token punctuation">:</span> http
      targetPort<span class="token punctuation">:</span> <span class="token number">8080</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> VirtualService
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  hosts<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token string">'*'</span>
  gateways<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> gateway
  http<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">route</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
            host<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend.default.svc.cluster.local
            port<span class="token punctuation">:</span>
              number<span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre><p>Apply it. Finally, we will deploy the customers v1 service:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers<span class="token punctuation">-</span>v1
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers<span class="token punctuation">-</span>v1
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> customers
    version<span class="token punctuation">:</span> v1
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  replicas<span class="token punctuation">:</span> <span class="token number">1</span>
  selector<span class="token punctuation">:</span>
    matchLabels<span class="token punctuation">:</span>
      app<span class="token punctuation">:</span> customers
      version<span class="token punctuation">:</span> v1
  template<span class="token punctuation">:</span>
    metadata<span class="token punctuation">:</span>
      labels<span class="token punctuation">:</span>
        app<span class="token punctuation">:</span> customers
        version<span class="token punctuation">:</span> v1
    spec<span class="token punctuation">:</span>
      serviceAccountName<span class="token punctuation">:</span> customers<span class="token punctuation">-</span>v1
      containers<span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> gcr.io/tetratelabs/customers<span class="token punctuation">:</span>1.0.0
          imagePullPolicy<span class="token punctuation">:</span> Always
          name<span class="token punctuation">:</span> svc
          ports<span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">3000</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers
  labels<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> customers
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    app<span class="token punctuation">:</span> customers
  ports<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
      name<span class="token punctuation">:</span> http
      targetPort<span class="token punctuation">:</span> <span class="token number">3000</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> VirtualService
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> customers
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  hosts<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token string">'customers.default.svc.cluster.local'</span>
  <span class="token key atrule">gateways</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> gateway
  http<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">route</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">destination</span><span class="token punctuation">:</span>
            host<span class="token punctuation">:</span> customers.default.svc.cluster.local
            port<span class="token punctuation">:</span>
              number<span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre><p>Apply it. We can set the environment variable called GATEWAY_IP that stores the gateway IP address:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">GATEWAY_IP</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>kubectl get svc <span class="token parameter variable">-n</span> istio-system istio-ingressgateway <span class="token parameter variable">-ojsonpath</span><span class="token operator">=</span><span class="token string">'{.status.loadBalancer.ingress[0].ip}'</span><span class="token variable">)</span></span>
</code></pre><p>If we open the GATEWAY_IP, the web page with the data from the customers-v1 service is displayed as shown in the figure.<br>
&nbsp;<br>
Let's start by creating an AuthorizationPolicy resource that denies all requests between services in the default namespace.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> security.istio.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> AuthorizationPolicy
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> deny<span class="token punctuation">-</span>all
  namespace<span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
   <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre><p>Apply it. If we try to send a request to GATEWAY_IP we will get back the access denied response:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">curl</span> <span class="token variable">$GATEWAY_IP</span> 
RBAC: access denied
</code></pre><p>Similarly, if we run a pod inside the cluster and make a request from within the default namespace to either the web-frontend or the <code>customers-v1</code> service, we will get the same error.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl run <span class="token function">curl</span> <span class="token parameter variable">--image</span><span class="token operator">=</span>radial/busyboxplus:curl <span class="token parameter variable">-i</span> <span class="token parameter variable">--tty</span>
If you don't see a <span class="token builtin class-name">command</span> prompt, try pressing enter.
<span class="token punctuation">[</span> root@curl:/ <span class="token punctuation">]</span>$ <span class="token function">curl</span> customers 
RBAC: access denied
<span class="token punctuation">[</span> root@curl:/ <span class="token punctuation">]</span>$ <span class="token function">curl</span> web-frontend
RBAC: access denied
</code></pre><p>In both cases, we get back the access denied error. You can type exit, to exit the container.</p>
<h4 id="enabling-requests-from-ingress-to-web-frontend">Enabling Requests from Ingress to “web-frontend" </h4>
<p>We will first allow requests to be sent from the ingress gateway to the web-frontend workload using the ALLOW action. The rules section specifies the source namespace (istio-system) where the ingress gateway is running and the ingress gateway's service account name in the principals field.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> security.istio.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> AuthorizationPolicy
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> allow<span class="token punctuation">-</span>ingress<span class="token punctuation">-</span>frontend
  namespace<span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    matchLabels<span class="token punctuation">:</span>
      app<span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
  action<span class="token punctuation">:</span> ALLOW
  rules<span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">source</span><span class="token punctuation">:</span>
            namespaces<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"istio-system"</span><span class="token punctuation">]</span>
        <span class="token punctuation">-</span> <span class="token key atrule">source</span><span class="token punctuation">:</span>
            principals<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account"</span><span class="token punctuation">]</span>
</code></pre><p>Apply it. Suppose we try to send a request to the GATEWAY_IP. In that case, we will get a different error:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">curl</span> ht‌tp://<span class="token variable">$GATEWAY_IP</span>
<span class="token string">"Request failed with status code 403"</span>
</code></pre><p>Note:&nbsp; It takes a couple of seconds for Istio to distribute the policy to all proxies, so you might still see the RBAC: access denied message for the first couple of requests.</p>
<p>This error is coming from the <code>customers-v1</code> service - remember, we allowed calls from the ingress gateway to the web-frontend. However, web-frontend still isn’t allowed to make calls to the customers-v1 service. If we go back to the curl pod we are running inside the cluster and try to send a request to ht‌tp://web-frontend we will get an RBAC error:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl run <span class="token function">curl</span> <span class="token parameter variable">--image</span><span class="token operator">=</span>radial/busyboxplus:curl <span class="token parameter variable">-i</span> <span class="token parameter variable">--tty</span>
$ <span class="token function">curl</span> ht‌tp://web-frontend
RBAC: access denied
</code></pre><p>The initial DENY policy we deployed is still in effect. We only allow calls to be made from the ingress gateway to the web-frontend. Calls between other services (including the curl pod) will be denied.</p>
<p>When we deployed the web-frontend, we created a service account and assigned it to the pod. The service account represents the pods’ identity. Each namespace in Kubernetes has a default service account. If we do not explicitly assign a service account to the pod, the default service account from the namespace is used. It is good practice to create a separate service account for each deployment. Otherwise, all pods in the same namespace will have the same identity, which is useless when trying to enforce access policies.</p>
<h5 id="enabling-requests-from-web-frontend-to-customers">Enabling Requests from "web-frontend" to “customers" </h5>
<p>We can use the service account to configure which workloads can make requests to the customer.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> security.istio.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> AuthorizationPolicy
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> allow<span class="token punctuation">-</span>web<span class="token punctuation">-</span>frontend<span class="token punctuation">-</span>customers
  namespace<span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  selector<span class="token punctuation">:</span>
    matchLabels<span class="token punctuation">:</span>
        app<span class="token punctuation">:</span> customers
        version<span class="token punctuation">:</span> v1
  action<span class="token punctuation">:</span> ALLOW
  rules<span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">source</span><span class="token punctuation">:</span>
        namespaces<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"default"</span><span class="token punctuation">]</span>
      source<span class="token punctuation">:</span>
        principals<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"cluster.local/ns/default/sa/web-frontend"</span><span class="token punctuation">]</span>
</code></pre><p>Apply it. As soon as the policy gets created, we can send a couple of requests to <code>$GATEWAY_IP</code>, and we will get back the responses from customers service. If we would go back to the curl and try sending requests to web-frontend or customers, we would still get back the <code>RBAC: access denied</code> error message because the curl pod is not permitted to make calls to those services.</p>
<p>In this lab, we have used multiple authorization policies to explicitly allow calls from the ingress to the front end and the front end to the backend service. <em>Using a deny-all approach is an excellent way to start because we can control, manage, and then explicitly allow the communication we want to happen between individual services.</em></p>
<h4 id="gateway-and-auth">Gateway and Auth </h4>
<p>The Istio ingress gateway supports routing based on authenticated JWT, which is useful for routing based on end user identity and more secure compared using the unauthenticated HTTP attributes (e.g. path or header).</p>
<ol>
<li>
<p>First create the request authentication policy to enable JWT validation. Apply the policy in the namespace of the workload it selects:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> - <span class="token operator">&lt;&lt;</span><span class="token string">EOF
apiVersion: security.istio.io/v1
kind: RequestAuthentication
metadata:
  name: "jwt-example"
  namespace: istio-system # namespace in which gateway exists 
spec:
  selector:
    matchLabels:
      istio: ingressgateway
  jwtRules:
  - issuer: "testing@secure.istio.io"
    jwksUri: "https://raw.githubusercontent.com/istio/istio/release-1.22/security/tools/jwt/samples/jwks.json"
EOF</span>
</code></pre><p>If you provide a token in the authorization header, its implicitly default location, Istio validates the token using the <a href="https://raw.githubusercontent.com/istio/istio/release-1.22/security/tools/jwt/samples/jwks.json">public key set</a>, and rejects requests if the bearer token is invalid. However, requests without tokens are accepted.</p>
</li>
<li>
<p>Add a Authorization policy to only allow requests with valid JWT tokens or deny any request without JWT token (means, it has to have request principals …)</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> - <span class="token operator">&lt;&lt;</span><span class="token string">EOF
apiVersion: security.istio.io/v1
kind: AuthorizationPolicy
metadata:
  name: "frontend-ingress"
  namespace: istio-system
spec:
  selector:
    matchLabels:
      istio: ingressgateway
  action: DENY
  rules:
  - from:
    - source:
        notRequestPrincipals: ["*"]
EOF</span>
</code></pre></li>
</ol>
<p>For more info see <a href="https://istio.io/latest/docs/tasks/security/authentication/authn-policy/">auth policies</a> and <a href="https://istio.io/latest/docs/tasks/security/authentication/jwt-route/">jwt-route</a>.</p>
<h2 id="policy-storage">Policy storage </h2>
<p>Istio stores mesh-scope policies in the <strong>root namespace</strong> (default: istio-system). These policies have an empty selector apply to all workloads in the mesh. Policies that have a namespace scope are stored in the corresponding namespace. They only apply to workloads within their namespace.</p>
<p>If you configure a selector field, the authentication policy only applies to workloads matching the conditions you configured. Peer and request authentication policies are stored separately by kind, PeerAuthentication and RequestAuthentication respectively. Peer and request authentication policies use selector fields to specify the label of the workloads to which the policy applies. If you don’t provide a value for the selector field, Istio matches the policy to all workloads in the storage scope of the policy. Thus, the selector fields help you specify the scope of the policies:</p>
<ul>
<li><strong>Mesh-wide policy</strong>: A policy specified for the root namespace without or with an empty selector field.</li>
<li><strong>Namespace-wide policy</strong>: A policy specified for a non-root namespace without or with an empty selector field.</li>
<li><strong>Workload-specific policy</strong>: a policy defined in the regular namespace, with non-empty selector field.</li>
</ul>
<p>There can be only one mesh-wide peer authentication policy, and only one namespace-wide peer authentication policy per namespace. When you configure multiple mesh- or namespace-wide peer authentication policies for the same mesh or namespace, Istio ignores the newer policies. When more than one workload-specific peer authentication policy matches, Istio picks the oldest one. Istio applies the narrowest matching policy for each workload using the following order:</p>
<ul>
<li>workload-specific</li>
<li>namespace-wide</li>
<li>mesh-wide</li>
</ul>
<hr>
<h1 id="advanced-topics-for-istio">Advanced Topics for Istio </h1>
<h2 id="external-authorization">External Authorization </h2>
<p>Istio provides a way to integrate external auth providers into your auth flow. To do this,</p>
<ol>
<li>Define the external authorizer<br>
CUSTOM Action: In order to use the CUSTOM action in the authorization policy, you must define the external authorizer that is allowed to be used in the mesh. This is currently defined in the extension provider in the <strong>mesh config</strong>. Currently, the only supported extension provider type is the Envoy <strong>ext_authz</strong> provider. The external authorizer must implement the corresponding Envoy ext_authz check API. Add the extension provider</li>
<li>Enable with external authorization<br>
Apply an authorization policy with the CUSTOM action value. Then at runtime, requests to destination workload will be paused by the ext_authz filter, and a check request will be sent to the external authorizer to decide whether the request should be allowed or denied</li>
</ol>
<p>To see external auth implemented using Kubernetes Ingress, look at <a href="https://ibrahimhkoyuncu.medium.com/kubernetes-ingress-external-authentication-with-oauth2-proxy-and-keycloak-9924a3b2d34a">K8s Ingress External Authentication with OAuth2 Proxy</a> for session storage.</p>
<p>Ingress external authentication is a mechanism that enables authentication for incoming requests to services deployed within a Kubernetes cluster through an Ingress controller. It allows you to enforce authentication before granting access to your applications, providing an additional layer of security and control. Benefits of using Kubernetes Ingress external authentication:<br>
- Centralized Authentication: With external authentication, you can centralize the authentication logic for multiple services deployed within your Kubernetes cluster. This eliminates the need for individual authentication mechanisms for each service.<br>
- Flexible Authentication Providers: including OAuth2 providers like Google, GitHub, or OpenID Connect providers. This flexibility allows you to leverage existing authentication systems or choose the one that best suits your needs.<br>
- Simplified User Experience: External authentication allows users to authenticate using their existing credentials from well-known providers. This simplifies the user experience, as users don’t need to create new accounts or remember additional usernames and passwords.</p>
<p>For more details, see [istio better-external-authz](<a href="https://istio.io/latest/docs/tasks/security/authorization/authz-custom/">https://istio.io/latest/docs/tasks/security/authorization/authz-custom/</a><br>
<a href="https://istio.io/v1.16/blog/2021/better-external-authz/">https://istio.io/v1.16/blog/2021/better-external-authz/</a>) and <a href="https://istio.io/v1.7/docs/ops/common-problems/security-issues/">more security problems</a></p>
<p><strong>Tip</strong>: It’s hard to figure out where the Envoy configuration gets hooked up. Use <code>istioctl proxy-status</code><br>
to see what envoy is doing. Verify the Envoy proxy configuration of the target workload using <code>istioctl proxy-config</code> command.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token assign-left variable">POD</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>kubectl get pod <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>httpbin <span class="token parameter variable">-n</span> foo <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token punctuation">{</span>.items<span class="token punctuation">..</span>metadata.name<span class="token punctuation">}</span><span class="token variable">)</span></span>
$ istioctl proxy-config listener <span class="token variable">${POD}</span> <span class="token parameter variable">-n</span> foo <span class="token parameter variable">--port</span> <span class="token number">80</span> <span class="token parameter variable">-o</span> json
</code></pre><h2 id="deployment-models-for-serice-mesh">Deployment Models for Serice Mesh </h2>
<h3 id="multi-cluster-deployments">Multi-cluster Deployments </h3>
<p>A multi-cluster deployment (two or more clusters) gives us greater isolation and availability, but the cost we pay is increased complexity. We will deploy clusters across multiple zones and regions if the scenarios require high availability (HA).<br>
The next decision we need to make is to decide if we want to run the clusters within one network or if we want to use multiple networks. The following figure shows a multi-cluster scenario (Cluster A, B, and C) deployed across two networks.</p>
<p align="center">
    <img src="./assets/k8s/multicluster1.png" alt="drawing" width="500" height="300" style="center">
</p>
<h5 id="network-deployment-models">Network Deployment Models </h5>
<p>When multiple networks are involved, the workloads running inside the clusters must use Istio gateways to reach workloads in other clusters. Using various networks allows for better fault tolerance and scaling of network addresses.</p>
<p align="center">
    <img src="./assets/k8s/multicluster2.png" alt="drawing" width="500" height="300" style="center">
</p>
<p>The gateways services use to communicate across or within the cluster are called east-west gateways.</p>
<h5 id="control-plane-deployment-models">Control Plane Deployment Models </h5>
<p>Istio service mesh uses the control plane to configure all communications between workloads inside the mesh. The control plane the workloads connect to depends on their configuration. In the simplest case, we have a service mesh with a single control plane in a single cluster. This is the configuration we've been using throughout this course.</p>
<p align="center">
    <img src="./assets/k8s/multicluster3.png" alt="drawing" width="500" height="300" style="center">
</p>
<p>The shared control plane model involves multiple clusters where the control plane only runs in one cluster. That cluster is referred to as a primary cluster, while other clusters in the deployment are called remote clusters. These clusters don't have their control plane. Instead, they are sharing the control plane from the primary cluster.</p>
<p>Another deployment model is where we treat all clusters as remote clusters controlled by an external control plane. The external control plane gives us a complete separation between the control plane and the data plane. A typical example of an external control plane is when a cloud vendor manages it.<br>
For high availability, we should deploy multiple control plane instances across multiple clusters, zones, or regions, as shown in the figure below.</p>
<p align="center">
    <img src="./assets/k8s/multicluster4.png" alt="drawing" width="500" height="300" style="center">
</p>&nbsp;
<p>This model offers <strong>improved availability</strong> and <strong>configuration isolation</strong>. If one of the control planes becomes unavailable, the outage is limited to that one control plane. To improve that, you can implement failover and configure workload instances to connect to another control plane in case of failure.<br>
For the highest availability possible, we can deploy a control plane inside each cluster.</p>
<h4 id="mesh-deployment-models">Mesh Deployment Models </h4>
<p>All scenarios we have discussed so far use a single mesh. In a single mesh model, all services are in one mesh, regardless of how many clusters and networks they are spanning. A deployment model where multiple meshes are federated together is called a multi-mesh deployment. In this model, services can communicate across mesh boundaries. The model gives us a cleaner organizational boundary and stronger isolation and reuses service names and namespaces.</p>
<p>When federating two meshes, each mesh can expose a set of services and identities that all participating meshes can recognize. To enable cross-mesh service communication, we have to enable trust between the two meshes. Trust can be established by importing a trust bundle to a mesh and configuring local policies for those identities.</p>
<h3 id="tenancy-models">Tenancy Models </h3>
<p>A tenant is a group of users sharing common access and privileges to a set of workloads. Isolation between the tenants gets done through network configuration and policies. Istio supports namespace and cluster tenancies. Note that the tenancy we discuss here is <strong>soft multi-tenancy</strong>, not hard. There is no guaranteed protection against noisy neighbor problems when multiple tenants share the same Istio control plane.</p>
<p>Within a mesh, Istio uses namespaces as a unit of tenancy. If using Kubernetes, we can grant permissions for workloads deployments per namespace. By default, services from different namespaces can communicate with each other through fully qualified names. In the security section, we learned how to improve isolation using authorization policies and restrict access to only the appropriate callers. In the multi-cluster deployment models, the namespaces in each cluster sharing the same name are considered the same. Service customers from namespace default in cluster A refers to the same service as service customers from namespace default in cluster B. Load balancing is done across merged endpoints of both services when traffic is sent to service customers, as shown in the following figure.</p>
<p align="center">
    <img src="./assets/k8s/multicluster5.png" alt="drawing" width="500" height="300" style="center">
</p>&nbsp;
<p>To configure cluster tenancy in Istio, we must configure each cluster as an independent service mesh. The meshes can be controlled and operated by separate teams, and we can connect the meshes into a multi-mesh deployment. Suppose we use the same example as before. In that case, service customers running in the default namespace in cluster A does not refer to the same service as service customers from the default namespace in cluster B.</p>
<p>Another critical part of the tenancy is isolating configuration from different tenants. At the moment, Istio does not address this issue. However, it encourages it through the namespace scoped configuration.</p>
<p>A typical multi-cluster deployment topology is one where each cluster has its control plane. For regular service mesh deployments at scale, you should use multi-mesh deployments and have a separate system orchestrating the meshes externally.</p>
<p>It is always recommended to use ingress gateways across clusters, even if they span a single network. Direct pod-to-pod connectivity requires populating endpoint data across multiple clusters, which can slow down and complicate things. A more straightforward solution is to have traffic flow through ingresses across clusters instead.</p>
<h3 id="locality-failover">Locality Failover </h3>
<p>When dealing with multiple clusters, we need to understand the definition of the term locality, which determines a geographical location of a workload inside the mesh. The locality comprises region (ex. canada), zone (central), and subzone (a1), like AWS <code>ca-centrl-1</code>. The region and zone are typically set automatically if your Kubernetes cluster runs on cloud providers' infrastructure. For example, nodes of a cluster running GCP in the <code>us-west1</code> region and zone <code>us-west1-a</code> will have the following labels set:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>topology.kubernetes.io/region<span class="token operator">=</span>us-west1
topology.kubernetes.io/zone<span class="token operator">=</span>us-west1-a
</code></pre><p>The sub-zones allow us to divide individual zones further. However, the concept of sub-zones doesn’t exist in Kubernetes. To use sub-zones, we can use the label <code>topology.istio.io/subzone</code>.</p>
<p>Once set up, Istio can use the locality information to control load balancing, and we can configure locality failover or locality weighted distribution. The failover settings can be configured in the DestinationRule under the localityLbSettings field. For example:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> DestinationRule
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> helloworld
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  host<span class="token punctuation">:</span> helloworld.sample.svc.cluster.local
  trafficPolicy<span class="token punctuation">:</span>
    loadBalancer<span class="token punctuation">:</span>
      simple<span class="token punctuation">:</span> ROUND_ROBIN
      localityLbSetting<span class="token punctuation">:</span>
        enabled<span class="token punctuation">:</span> <span class="token boolean important">true</span>
        failover<span class="token punctuation">:</span>
          <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span> us<span class="token punctuation">-</span>west
            to<span class="token punctuation">:</span> us<span class="token punctuation">-</span>east
    outlierDetection<span class="token punctuation">:</span>
      consecutive5xxErrors<span class="token punctuation">:</span> <span class="token number">100</span>
      interval<span class="token punctuation">:</span> 1s
      baseEjectionTime<span class="token punctuation">:</span> 1m
</code></pre><p>The above example specifies that when the endpoints within the us-west region are unhealthy, the traffic should failover to any zones and sub-zones in the us-east region. Note that the outlier detection settings are required for the failover to function properly. The outlier tells Envoy how to determine whether the endpoints are unhealthy.</p>
<p>Similarly, we can use the locality information to control the distribution of traffic using weights. Consider the following example:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> DestinationRule
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> helloworld
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  host<span class="token punctuation">:</span> helloworld.sample.svc.cluster.local
  trafficPolicy<span class="token punctuation">:</span>
    loadBalancer<span class="token punctuation">:</span>
      simple<span class="token punctuation">:</span> ROUND_ROBIN
      localityLbSetting<span class="token punctuation">:</span>
        enabled<span class="token punctuation">:</span> <span class="token boolean important">true</span>
        distribute<span class="token punctuation">:</span>
          <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span> <span class="token string">"us-west1/us-west1-a/*"</span>
            to<span class="token punctuation">:</span>
              "us<span class="token punctuation">-</span><span class="token key atrule">west1/us-west1-a/*"</span><span class="token punctuation">:</span> <span class="token number">50</span>
              "us<span class="token punctuation">-</span><span class="token key atrule">west1/us-west1-b/*"</span><span class="token punctuation">:</span> <span class="token number">30</span>
              "us<span class="token punctuation">-</span><span class="token key atrule">east1/us-east1-a/*"</span><span class="token punctuation">:</span> <span class="token number">20</span>
  outlierDetection<span class="token punctuation">:</span>
    consecutive5xxErrors<span class="token punctuation">:</span> <span class="token number">100</span>
    interval<span class="token punctuation">:</span> 1s
    baseEjectionTime<span class="token punctuation">:</span> 1m
</code></pre><p>In the above example, we’re distributing traffic that originates in us-west1/us-west1-a in the following manner:</p>
<ul>
<li>50% of the traffic is sent to workloads in `us-west1/us-west1-a</li>
<li>30% of the traffic is sent to workloads in <code>us-west1/us-west1-b</code></li>
<li>20% of the traffic is sent to workloads in `us-east1/us-east1-a</li>
</ul>
<p>Upon completing this chapter, you should now be able to:<br>
Understand the purpose of the Sidecar resource.<br>
Understand the process of onboarding virtual machines to the mesh.<br>
Understand the different facets of deploying Istio in multi-cluster scenarios.</p>
<h3 id="onboarding-vms">Onboarding VMs </h3>
<p>So far, we have assumed that all our workloads run inside Kubernetes clusters as pods. In production, you will most likely have a services architecture with a mix of workloads: some will be containerized and running on a Kubernetes cluster, but others will be running on Virtual Machines (VMs). It's important for a service mesh to support this use case and to be able to onboard workloads running on VMs.</p>
<p>Let us explore how a workload running on a VM can be made a part of the Istio service mesh.</p>
<ul>
<li>
<p>The VM workload will need to be accompanied by a sidecar. Istio makes its istio-proxy sidecar available as a Debian (.deb) or CentOS (.rpm) package and can simply be installed on a Linux VM and configured as a <strong>systemd service</strong>.</p>
</li>
<li>
<p>Services on the cluster should be able to call a service backed by a workload running on a VM, using the same fully-qualified hostname resolution used for on-cluster workloads. Conversely, a workload running on the VM should be able to call a service running on the Kubernetes cluster.<br>
In Kubernetes, the endpoints of a service consist of pod IP addresses. A workload running on a VM is akin to a pod, so it's important to be able to target these workloads using labels as selectors.</p>
</li>
<li>
<p>A pod running on Kubernetes is associated with a Kubernetes service account and a namespace. That is the basis for its SPIFFE identity. Similarly, the workload running on a VM will require an associated namespace and service account. Istio provides the <a href="https://istio.io/latest/docs/reference/config/networking/workload-entry/"><strong>WorkloadEntry</strong></a> custom resource as a mechanism for configuring the VM workload and providing all of these details: the namespace, labels, and service account. Istio also provides the <a href="https://istio.io/latest/docs/reference/config/networking/workload-group/">WorkloadGroup</a> custom resource as a template for WorkloadEntry resources, which can be automatically created when Istio registers a workload with the mesh. The WorkloadGroup also has a provision for specifying a readiness probe for VM workloads to support health-checking.</p>
</li>
</ul>
<p align="center">
    <img src="./assets/k8s/external-VM.png" alt="drawing" width="500" height="400" style="center">
</p>
<p>On the left-hand side in the above illustration, we see a Kubernetes cluster with Istio installed and a service (Service A) running inside the cluster. On the right are two additional workloads (Services B and C), each running on a VM. Note how these VMs have an Envoy proxy running as a sidecar. The east-west gateway depicted in the center allows communication between the sidecars running on the VMs and the Istio control plane.</p>
<p>The green arrows show how, in this particular configuration, the services communicate directly with one another (because they all exist within a single network). In a scenario where the VMs reside in a separate network, that traffic would route through the east-west gateway.</p>
<p>The general procedure for onboarding a VM can be summarized by the following steps:</p>
<ul>
<li>Create and configure the east-west gateway as described above</li>
<li>Construct the WorkloadGroup resource</li>
<li>Install the sidecar on the VM</li>
<li>Generate the configuration files for the sidecar and copy them to the VM.</li>
<li>Place all configuration files in their proper locations on the VM.</li>
<li>Start the sidecar service on the VM</li>
</ul>
<h4 id="connect-a-vm-workload-to-the-istio-mesh">Connect a VM Workload to the Istio Mesh </h4>
<p>This lab demonstrates how a workload running on a VM can join the Istio service mesh.</p>
<p>Prerequisites<br>
This exercise was developed on GCP, and some of the steps shown are specific to that infrastructure. We begin with a Kubernetes cluster with Istio installed, and running the BookInfo sample application. Next, we will turn off the ratings deployment running inside the Kubernetes cluster, and in its place, we will provision a VM to run the ratings application on it.</p>
<h5 id="create-a-kubernetes-cluster">Create a Kubernetes Cluster </h5>
<p>Armed with your GCP (or other) cloud account, create a Kubernetes cluster:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ gcloud container clusters create my-istio-cluster <span class="token punctuation">\</span>
  --cluster-version latest <span class="token punctuation">\</span>
  --machine-type <span class="token string">"n1-standard-2"</span> <span class="token punctuation">\</span>
  --num-nodes <span class="token string">"3"</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--network</span> <span class="token string">"default"</span>
</code></pre><p>Note: make sure you specify a zone or region close to you in the command above.</p>
<p>Wait until the cluster is ready. Install Istio:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ istioctl <span class="token function">install</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--set</span> <span class="token assign-left variable">values.pilot.env.PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION</span><span class="token operator">=</span>true <span class="token punctuation">\</span>
  <span class="token parameter variable">--set</span> <span class="token assign-left variable">values.pilot.env.PILOT_ENABLE_WORKLOAD_ENTRY_HEALTHCHECKS</span><span class="token operator">=</span>true
</code></pre><p>The essential difference between previous installations of Istio and the above command are the two new pilot configuration options that enable workload entry auto-registration and health checks. Auto-registration means that when a workload is created on a VM, Istio will automatically create a WorkloadEntry custom resource.</p>
<p>Finally, deploy the BookInfo sample application, as follows:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl label ns default istio-injection<span class="token operator">=</span>enabled
$ <span class="token builtin class-name">cd</span> ~/istio-1.14.3
istio-1.14.3$ kubectl apply <span class="token parameter variable">-f</span> samples/bookinfo/platform/kube/bookinfo.yaml
</code></pre><p>Topology of the BookInfo application:</p>
<p align="center">
    <img src="./assets/k8s/bookinfo-vm-istio.png" alt="drawing" width="500" height="400" style="center">
</p>
<p>In the following steps, we will focus specifically on BookInfo's ratings service. We will be inspecting the "star" ratings shown on BookInfo's product page as a means of determining whether the service is up and reachable. Notice in the above illustration that the product page load balances requests to the reviews service between three different versions of the application and that reviews-v1 never calls ratings. To ensure that we can see the ratings each time we request the product page, scale down reviews-v1 to zero replicas, effectively turning off that endpoint:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl scale deploy reviews-v1 —replicas<span class="token operator">=</span><span class="token number">0</span>
</code></pre><p>Expose the BookInfo application via the ingress gateway:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> samples/bookinfo/networking/bookinfo-gateway.yaml
</code></pre><p>Grab your load balancer public IP address:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token assign-left variable">GATEWAY_IP</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>kubectl get svc <span class="token parameter variable">-n</span> istio-system istio-ingressgateway <span class="token parameter variable">-ojsonpath</span><span class="token operator">=</span><span class="token string">'{.status.loadBalancer.ingress[0].ip}'</span><span class="token variable">)</span></span>
</code></pre><p>Open a browser and visit the BookInfo product page at <code>http://$GATEWAY_IP/productpage</code>. Make sure to replace the $GATEWAY_IP in the URL with an actual IP address you got from running the previous command. Verify that you can see ratings stars on the page. With the BookInfo application fully deployed with ingress and functioning, we can now turn our attention to the VM.</p>
<p>Scale down the ratings-v1 deployment running inside the Kubernetes cluster to zero replicas:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl scale deploy ratings-v1 <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token number">0</span>
</code></pre><p>Refresh the page in your browser and ensure the ratings stars are now gone and have been replaced with the message "Ratings service is currently unavailable." This indicates that the ratings service has no available endpoints to handle requests.</p>
<p>In the subsequent steps, we replace this workload with an instance running on a VM.  Start by creating a VM in the same network that the Kubernetes cluster is running:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ gcloud compute instances create my-mesh-vm <span class="token parameter variable">--tags</span><span class="token operator">=</span>mesh-vm <span class="token punctuation">\</span>
  --machine-type<span class="token operator">=</span>n1-standard-2 <span class="token punctuation">\</span>
  <span class="token parameter variable">--network</span><span class="token operator">=</span>default <span class="token parameter variable">--subnet</span><span class="token operator">=</span>default <span class="token punctuation">\</span>
  --image-project<span class="token operator">=</span>ubuntu-os-cloud <span class="token punctuation">\</span>
  --image-family<span class="token operator">=</span>ubuntu-2204-lts
</code></pre><p>The above command is specific to the GCP cloud environment; please adapt the command to your specific environment. Wait for the machine to be ready.</p>
<h5 id="install-the-ratings-service-on-the-vm">Install the Ratings Service on the VM </h5>
<p>The ratings service is a Node.js application.<br>
SSH onto the VM:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ gcloud compute <span class="token function">ssh</span> ubuntu@my-mesh-vm
</code></pre><p>Install Node.js. The ratings service is a Node.js application.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">apt-get</span> update
$ <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> nodejs <span class="token function">npm</span> jq
</code></pre><p>Grab a copy of the ratings app source code from the Istio GitHub repository.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">mkdir</span> ratings <span class="token operator">&amp;&amp;</span> <span class="token builtin class-name">cd</span> ratings
/ratings$ <span class="token function">wget</span> ht‌tps://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/src/ratings/package.json 
/ratings$ <span class="token function">wget</span> ht‌tps://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/src/ratings/ratings.js
</code></pre><p>Install the application's dependencies:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token function">npm</span> <span class="token function">install</span>
</code></pre><p>Run the app:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token function">npm</span> run start <span class="token number">9080</span> <span class="token operator">&amp;</span>
<span class="token operator">&gt;</span> start
<span class="token operator">&gt;</span> <span class="token function">node</span> ratings.js <span class="token string">"9080"</span>
Server listening on: <span class="token function">link</span> 
*Note that the ampersand <span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token punctuation">)</span> <span class="token keyword keyword-in">in</span> the above <span class="token builtin class-name">command</span> causes the process to run <span class="token keyword keyword-in">in</span> the background. If desired, the <span class="token function">fg</span> <span class="token builtin class-name">command</span> can be used to bring the process back to the foreground.
</code></pre><p>Test the app by retrieving a rating:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">curl</span> ht‌tp://localhost:9080/ratings/123 <span class="token operator">|</span> jq
<span class="token punctuation">{</span>
      <span class="token string">"id"</span><span class="token builtin class-name">:</span> <span class="token number">123</span>,
      <span class="token string">"ratings"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
        <span class="token string">"Reviewer1"</span><span class="token builtin class-name">:</span> <span class="token number">5</span>,
        <span class="token string">"Reviewer2"</span><span class="token builtin class-name">:</span> <span class="token number">4</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
</code></pre><h5 id="allow-pod-to-vm-traffic-on-port-9080">Allow POD-to-VM Traffic on Port 9080 </h5>
<p>Open another terminal window and set up a variable that captures the CIDR IP address range of the pods in the Kubernetes cluster.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token assign-left variable">CLUSTER_POD_CIDR</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>gcloud container clusters describe my-istio-cluster <span class="token parameter variable">--format</span><span class="token operator">=</span>json <span class="token operator">|</span> jq <span class="token parameter variable">-r</span> <span class="token string">'.clusterIpv4Cidr'</span><span class="token variable">)</span></span>
</code></pre><p>Note: use <code>--zone</code> or <code>--region</code> to specify the location for the above command.</p>
<p>The variable <code>CLUSTER_POD_CIDR</code> is used as an input to the following command, which creates a firewall rule to allow communication from the pods to the VM.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ gcloud compute firewall-rules create <span class="token string">"cluster-pods-to-vm"</span> <span class="token punctuation">\</span>
  --source-ranges<span class="token operator">=</span><span class="token variable">$CLUSTER_POD_CIDR</span> <span class="token punctuation">\</span>
  --target-tags<span class="token operator">=</span>mesh-vm <span class="token punctuation">\</span>
  <span class="token parameter variable">--action</span><span class="token operator">=</span>allow <span class="token punctuation">\</span>
  <span class="token parameter variable">--rules</span><span class="token operator">=</span>tcp:9080
</code></pre><p>Above, the <code>target-tags=mesh-vm</code> matches the tag given to the VM when it was created.</p>
<h5 id="install-the-east-west-gateway-and-expose-istiod">Install the East-West Gateway and Expose Istiod </h5>
<p>An east-west gateway is necessary to enable communication between the sidecar that will be running on the VM and istiod, the Istio control plane (see the <a href="https://istio.io/latest/docs/ops/deployment/vm-architecture/">Istio documentation</a>).</p>
<p>Install the east-west gateway:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ /samples/multicluster/gen-eastwest-gateway.sh --single-cluster <span class="token operator">|</span> istioctl <span class="token function">install</span> <span class="token parameter variable">-y</span> <span class="token parameter variable">-f</span> -
</code></pre><p>If you list the pods in the istio-system namespace you’ll notice the istio-eastwestgateway instance was created. Expose istiod though the east-west gateway:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-n</span> istio-system <span class="token parameter variable">-f</span> ./samples/multicluster/expose-istiod.yaml
</code></pre><h5 id="create-the-workloadgroup">Create the WorkloadGroup </h5>
<p>A WorkloadGroup is a template for WorkloadEntry objects.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> WorkloadGroup
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  name<span class="token punctuation">:</span> ratings
  namespace<span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  metadata<span class="token punctuation">:</span>
    labels<span class="token punctuation">:</span>
      app<span class="token punctuation">:</span> ratings
  template<span class="token punctuation">:</span>
    serviceAccount<span class="token punctuation">:</span> bookinfo<span class="token punctuation">-</span>ratings
</code></pre><p>Apply it. This WorkloadGroup will ensure that our VM workload is labeled with <code>app: ratings</code> and associated with the service account <code>bookinfo-ratings</code> (this service account was one of the resources deployed together with the BookInfo application).</p>
<p>We now focus on installing and configuring the sidecar on the VM.</p>
<h5 id="generate-vm-artifacts">Generate VM Artifacts </h5>
<p>The Istio CLI provides a command to automatically generate all artifacts needed to configure the VM. Create a subdirectory to collect the artifacts to be generated:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">mkdir</span> vm_files
/vm_files$ istioctl x workload entry configure <span class="token punctuation">\</span>
     <span class="token parameter variable">--file</span> ratings-workloadgroup.yaml <span class="token punctuation">\</span>
     <span class="token parameter variable">--output</span> vm_files <span class="token punctuation">\</span>
     <span class="token parameter variable">--autoregister</span>
</code></pre><p>Inspect the contents of the folder vm_files. There, you will find five files, including a root certificate, an addition to the VM's hosts file that resolves the istiod endpoint to the IP address of the east-west gateway, a token used by the VM to securely join the mesh, an environment file containing metadata about the workload running on the VM (ratings), and a mesh configuration file necessary to configure the proxy.</p>
<p>In the next few steps, we copy these files to the VM and install them to their proper locations.</p>
<h5 id="vm-configuration-recipe">VM Configuration Recipe </h5>
<p>Copy the generated artifacts to the VM:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ gcloud compute <span class="token function">scp</span> vm_files/* ubuntu@my-mesh-vm:
</code></pre><p>SSH onto the VM:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ gcloud compute <span class="token function">ssh</span> ubuntu@my-mesh-vm
</code></pre><p>On the VM, run the following commands (see reference):</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token comment"># place the root certificate in its proper place:</span>
$ <span class="token function">sudo</span> <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /etc/certs
$ <span class="token function">sudo</span> <span class="token function">cp</span> ~/root-cert.pem /etc/certs/root-cert.pem 
<span class="token comment"># place the token to the correct location on the file system:</span>
$ <span class="token function">sudo</span> <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /var/run/secrets/tokens
$ <span class="token function">sudo</span> <span class="token function">cp</span> ~/istio-token /var/run/secrets/tokens/istio-token
<span class="token comment"># fetch and install the istio sidecar package:</span>
$ <span class="token function">curl</span> <span class="token parameter variable">-LO</span> ht‌tps://storage.googleapis.com/istio-release/releases/1.14.3/deb/istio-sidecar.deb
$ <span class="token function">sudo</span> dpkg <span class="token parameter variable">-i</span> istio-sidecar.deb
<span class="token comment"># copy over the environment file and mesh configuration file:</span>
$ <span class="token function">sudo</span> <span class="token function">cp</span> ~/cluster.env /var/lib/istio/envoy/cluster.env
$ <span class="token function">sudo</span> <span class="token function">cp</span> ~/mesh.yaml /etc/istio/config/mesh
<span class="token comment"># add the entry for istiod to the /etc/hosts file:</span>
$ <span class="token function">sudo</span> <span class="token function">sh</span> <span class="token parameter variable">-c</span> <span class="token string">'cat $(eval echo ~$SUDO_USER)/hosts &gt;&gt; /etc/hosts'</span>
$ <span class="token function">sudo</span> <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /etc/istio/proxy
<span class="token comment"># make the user "istio-proxy" the owner of all these files:</span>
$ <span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> istio-proxy /etc/certs /var/run/secrets /var/lib/istio /etc/istio/config /etc/istio/proxy
</code></pre><p>The VM is now configured. Watch the WorkloadEntry get created as a consequence of the VM registering with the mesh. Run the following kubectl command against your Kubernetes cluster:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get workloadentry <span class="token parameter variable">--watch</span>
</code></pre><p>On the VM, start the sidecar (istio-proxy) service:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> systemctl start istio
NAME                 AGE  ADDRESS
ratings-10.138.0.53  0s   <span class="token number">10.138</span>.0.53
</code></pre><p>The workload entry will appear in the listing (this can take up to a minute). If we were to stop the sidecar service, we would see the WorkloadEntry resource removed. With the VM on-boarded to the mesh, we can proceed to verify communications between the VM and other services. Although the ratings service does not need to call back into the mesh, we can manually test communication from the VM to the mesh. For example, we can call the details service from the VM with the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token function">curl</span> details.default.svc:9080/details/123 <span class="token operator">|</span> jq
<span class="token punctuation">{</span>
      <span class="token string">"id"</span><span class="token builtin class-name">:</span> <span class="token number">123</span>,
      <span class="token string">"author"</span><span class="token builtin class-name">:</span> <span class="token string">"William Shakespeare"</span>,
      <span class="token string">"year"</span><span class="token builtin class-name">:</span> <span class="token number">1595</span>,
      <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"paperback"</span>,
      <span class="token string">"pages"</span><span class="token builtin class-name">:</span> <span class="token number">200</span>,
      <span class="token string">"publisher"</span><span class="token builtin class-name">:</span> <span class="token string">"PublisherA"</span>,
      <span class="token string">"language"</span><span class="token builtin class-name">:</span> <span class="token string">"English"</span>,
      <span class="token string">"ISBN-10"</span><span class="token builtin class-name">:</span> <span class="token string">"1234567890"</span>,
      <span class="token string">"ISBN-13"</span><span class="token builtin class-name">:</span> <span class="token string">"123-1234567890"</span>
  <span class="token punctuation">}</span>
</code></pre><p>This capability is supported via a DNS proxy bundled with the sidecar. Next, let us test the communication in the reverse direction: to the ratings application running on the VM. Head back to the web browser, and refresh the <code>ht‌tp://$GATEWAY_IP/productpage</code> URL. You should see that the ratings service is once more available, and the ratings stars are displaying.<br>
The workload entry has become an effective endpoint for the ratings service!<br>
We can verify this with the <code>istioctl proxy-config command</code> (which lists the endpoints of the reviews service), as follows:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ istioctl proxy-config endpoints deploy/reviews-v2.default <span class="token operator">|</span> <span class="token function">grep</span> ratings
A10.128.0.45:9080    HEALTHY       OK outbound<span class="token operator">|</span><span class="token number">9080</span><span class="token operator">||</span>ratings.default.svc.cluster.local
</code></pre><p>Compare the above IP address of the endpoint with the address of the WorkloadEntry for the VM workload:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get workloadentry
NAME                 AGE    ADDRESS
ratings-10.128.0.45  5m55s  <span class="token number">10.128</span>.0.45
</code></pre><p>The two addresses match.</p>
<h3 id="custom-ca-integration-using-kubernetes-csr">Custom CA Integration using Kubernetes CSR </h3>
<h4 id="what-is-a-certificate">What Is a Certificate? </h4>
<p>There are many types of certificates, but in this article, we am explicitly referring to X.509 V3 certificates. X.509 certificates are a standard digital format used to identify entities in computer networks. X.509 is the international standard for public key infrastructure (PKI) and is primarily used for identity authentication and information encryption, such as TLS. The contents of a certificate are hashed using a hash function and then signed with the issuer’s private key. This way, when a certificate is received, the recipient can use the issuer’s public key to verify the certificate’s validity.</p>
<p>A hash function is a function that maps an input of any length (also called a message) to a fixed-length output, also called a <strong>hash value</strong> or message digest. There are many hash functions, such as <strong>MD5</strong>, <strong>SHA-1</strong>, etc.<br>
A certificate is like a business card issued by an authoritative agency for the user to prove their identity, and it can also be used to encrypt information to ensure the security and integrity of communication. The following diagram shows the general steps of TLS communication, where the certificate proves the server’s identity and encrypts the communication. The figure below shows an example of an HTTP call to a website, issuing a digital certificate, authenticating, and encrypting the communication.</p>
<p align="center">
    <img src="./assets/k8s/tls-cert-usage.png" alt="drawing" width="500" height="400" style="center">
</p>
<p>Here are the detailed steps:</p>
<ul>
<li>The server (website owner) submits a <strong>certificate signing request</strong> to the CA</li>
<li>The CA verifies the server’s identity and the authenticity of the website and issues a digital certificate to the server, which the server installs so that visitors can verify the website’s security</li>
<li>The user sends a request to the website through a browser (client)</li>
<li>The server returns the TLS certificate to the client</li>
<li>The client verifies the certificate’s validity with the CA and establishes the connection if the certificate is valid, or prompts the user to reject the connection if it is invalid</li>
<li>The client generates a pair of random public and private keys</li>
<li>The client sends its public key to the server</li>
<li>The server encrypts the message using the client’s public key</li>
<li>The server sends the encrypted data to the client</li>
<li>The client decrypts the data sent by the server using its private key</li>
</ul>
<p>At this point, both parties have established a secure channel and can transmit encrypted data in both directions.</p>
<h4 id="certificate-trust-chain">Certificate Trust Chain </h4>
<p>The validation of a certificate requires using a certificate trust chain (Certificate Trust Chain). A certificate trust chain refers to a series of certificates used for identity verification, which form a chain starting from a trusted root certificate issuing agency, connecting downward step by step, until a specific intermediate or terminal certificate is used for verifying a particular certificate. The trustworthiness of a digital certificate increases as the certificate level increases in the certificate trust chain.</p>
<p>In the figure below, you can see four trust chains.</p>
<p align="center">
    <img src="./assets/k8s/cert-trust-chain.png" alt="drawing" width="500" height="400" style="center">
</p>
<p>Certificate trust chains are tree-like structures, where each CA can have one or more child CAs. There are three roles:</p>
<ul>
<li>Root CA: The top-level CA can issue certificates to intermediate CAs.</li>
<li>Intermediate CA: They can issue end-entity certificates.</li>
<li>End entity: A device or service that holds an end-entity certificate.</li>
</ul>
<p>Root CAs are the top-level issuing authorities for digital certificates, so the certificates they issue are the most trustworthy. Root certificate authorities are usually operated and regulated by government agencies or other authoritative organizations such as the International Infrastructure Security Organization. Common root CAs include:</p>
<ul>
<li>Symantec/VeriSign</li>
<li>Comodo</li>
<li>DigiCert</li>
<li>GlobalSign</li>
<li>GoDaddy</li>
<li>Entrust</li>
<li>GeoTrust</li>
<li>RapidSSL</li>
<li>Baltimore CyberTrust Root</li>
</ul>
<p>Please note that the above list is just a sample. There are many other root CAs.</p>
<p>The certificate trust chain allows the client (such as a web browser) to verify each certificate step by step when verifying the terminal certificate to determine whether it is trustworthy. The principle of digital certificate issuance is that the CA binds the certificate owner’s public key and identity information together. Then the CA uses its proprietary private key to generate a formal digital signature to indicate that the CA issued this certificate. The digital signature on the certificate can be verified using the CA’s public key during certificate verification.</p>
<h4 id="how-to-incorporate-istio-into-the-pki-certificate-trust-chain">How to Incorporate Istio into the PKI Certificate Trust Chain </h4>
<p>Before using Istio, enterprises usually have their own internal public key infrastructure . How can Istio be incorporated into the PKI certificate trust chain?</p>
<p>Istio has built-in certificate management functions that can be used out of the box. When Istio is started, it will create a self-signed certificate for istiod as the root certificate for all workloads in the mesh. <em>The problem with this is that the built-in root certificate cannot achieve mutual trust between meshes if you have multiple meshes</em>. The correct approach is not to use Istio’s self-signed certificate, but to incorporate Istio into your certificate trust chain and integrate Istio into your PKI, creating an intermediate certificate for each PKI mesh. In this way, two meshes have a shared trust root and can achieve mutual trust between meshes.</p>
<p>Integrate the Istio mesh into your internal PKI certificate trust chain by creating an intermediate CA, as shown in Figure 4.</p>
<p align="center">
    <img src="./assets/k8s/istio-CA-integration.png" alt="drawing" width="500" height="400" style="center">
</p>
<p>There are many benefits to incorporating Istio into your company’s internal PKI certificate trust chain:</p>
<ul>
<li>Secure communication across grids/clusters: with a common trust root, clusters can verify each other’s identities and communicate with each other;</li>
<li>More granular certificate revocation: you can revoke the certificate of an individual entity or intermediate CA to revoke the certificate for service or cluster;</li>
<li>Easy implementation of certificate rotation: you can rotate certificates by cluster/mesh rather than rotating the root node certificate, which reduces downtime.</li>
</ul>
<p>It is recommended to use <a href="https://github.com/cert-manager/cert-manager"><strong>cert-manager</strong></a> to achieve automated large-scale CA certificate rotation in production. For more information, see <a href="https://tetrate.io/blog/automate-istio-ca-rotation-in-production-at-scale/">Automate Istio CA Rotation</a> in Production at Scale. For detailed instructions on incorporating Istio into your company’s internal PKI certificate trust chain, see this <a href="https://tetrate.io/blog/istio-trust/">blog post</a>.</p>
<p>By default, the Istio CA generates a self-signed root certificate and key and uses them to sign workload certificates. To protect the root CA key, you should use a root CA that runs offline on a secure machine and the root CA to issue <strong>intermediate certificates</strong> to the <strong>Istio CA</strong> running on each cluster. The Istio CA can then use the administrator-specified certificate and key to sign workload certificates and distribute the administrator-specified root certificate as the trusted root to workloads.</p>
<p align="center">
    <img src="./assets/k8s/istio-CA-issuence.png" alt="drawing" width="400" height="300" style="center">
</p>
<p>Here is how the certificate issuance and mounting process in Istio works:</p>
<ul>
<li>In Istio, the Envoy proxy injects two processes into the Pod: Envoy and istio-agent (or pilot-agent). The istio-agent generates a private key and uses the Secret Discovery Service (SDS) via a Unix Domain Socket (UDS) to request a certificate signing request (CSR) from the Certificate Authority (CA) through the <code>istiod</code>, if you have not configured the CA plugin</li>
<li><code>istiod</code>, which has a built-in CA, returns the certificate to the istio-agent</li>
<li>The istio-agent then sends the generated private key and the CA-returned certificate to the Envoy instance to mount.</li>
</ul>
<p>Istio defaults to using the CA built into <code>istiod</code>, but also supports injecting other CAs, see the <a href="https://istio.io/latest/docs/tasks/security/cert-management/plugin-ca-cert/">Istio documentation</a>. Suppose you want to create identities for your services using custom CA certificates and keys. In that case, you will need to:</p>
<ul>
<li>Create a CA configuration file and use it to create self-signed CA root certificates and keys</li>
<li>Create a private key and signing request configuration files for the service</li>
<li>Create a certificate signing request (CSR) for the service.</li>
<li>Use the root certificate and key, along with the service’s signing request file, to create a certificate for the service</li>
</ul>
<p><a href="https://tetrate.io/blog/how-are-certificates-managed-in-istio/">Reference</a></p>
<p>There are two ways available for integrate custom CA in Istio. The first is to use cert-manger plug-in called <a href="https://cert-manager.io/docs/usage/istio-csr/installation/"><code>istio-csr</code></a> which uses cert-manager to issue Istio certificates, and needs to be able to reference an issuer resource to do this.You can choose to configure an issuer when installing with the Helm chart and / or to configure a ConfigMap to watch which can then be used to configure an issuer at runtime.</p>
<p>The alternative is to use Kubernetes CSR instead(without using cert-manager csr agent). For implementation, we are following <code>https://istio.io/latest/docs/tasks/security/cert-management/custom-ca-k8s/</code> using <strong>Helm</strong> and <strong>Kustomization</strong> to manipulate charts.</p>
<h4 id="steps-for-using-a-custom-ca-in-istio">Steps for Using a Custom CA in Istio </h4>
<p>This task shows how to provision workload certificates using a custom certificate authority that integrates with the Kubernetes CSR API. Different workloads can get their certificates signed from different cert-signers. Each cert-signer is effectively a different CA. It is expected that workloads whose certificates are issued from the same cert-signer can talk mTLS to each other while workloads signed by different signers cannot. This feature leverages Chiron, a lightweight component linked with <code>istiod</code> that signs certificates using the <strong>Kubernetes CSR API</strong>.</p>
<p>For this example, we use open-source cert-manager. Cert-manager has added experimental Support for Kubernetes CertificateSigningRequests starting with version 1.4.</p>
<h5 id="deploy-cert-manager-according-to-the-installation-doc">Deploy cert-manager according to the installation doc. </h5>
<p>Make sure to enable feature gate: <code>—feature-gates=ExperimentalCertificateSigningRequestControllers=true</code></p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ helm repo <span class="token function">add</span> jetstack https://charts.jetstack.io
$ helm repo update
$ helm <span class="token function">install</span> cert-manager jetstack/cert-manager <span class="token parameter variable">--namespace</span> cert-manager --create-namespace <span class="token parameter variable">--set</span> <span class="token assign-left variable">featureGates</span><span class="token operator">=</span><span class="token string">"ExperimentalCertificateSigningRequestControllers=true"</span> <span class="token parameter variable">--set</span> <span class="token assign-left variable">crds.enabled</span><span class="token operator">=</span>true
</code></pre><h5 id="create-self-signed-cluster-issuers">Create self-signed cluster issuers </h5>
<p>Use cert-manager to create <code>istio-system</code>, <code>tenanta</code> and <code>tenantb</code> for cert-manager. Namespace issuers and other types of issuers can also be used.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterIssuer
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> selfsigned<span class="token punctuation">-</span>tenanta<span class="token punctuation">-</span>issuer
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selfSigned</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Certificate
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> tenanta<span class="token punctuation">-</span>ca
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">isCA</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">duration</span><span class="token punctuation">:</span> 87600h <span class="token comment"># 10 years</span>
  <span class="token key atrule">commonName</span><span class="token punctuation">:</span> tenanta
  <span class="token key atrule">secretName</span><span class="token punctuation">:</span> tenanta<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>selfsigned
  <span class="token key atrule">privateKey</span><span class="token punctuation">:</span>
    <span class="token key atrule">algorithm</span><span class="token punctuation">:</span> ECDSA
    <span class="token key atrule">size</span><span class="token punctuation">:</span> <span class="token number">256</span>
  <span class="token key atrule">issuerRef</span><span class="token punctuation">:</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> selfsigned<span class="token punctuation">-</span>tenanta<span class="token punctuation">-</span>issuer
    <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterIssuer
    <span class="token key atrule">group</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterIssuer
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> tenanta
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ca</span><span class="token punctuation">:</span>
    <span class="token key atrule">secretName</span><span class="token punctuation">:</span> tenanta<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>selfsigned
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterIssuer
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> selfsigned<span class="token punctuation">-</span>tenantb<span class="token punctuation">-</span>issuer
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selfSigned</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Certificate
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> tenantb<span class="token punctuation">-</span>ca
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">isCA</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">duration</span><span class="token punctuation">:</span> 87600h <span class="token comment"># 10 years</span>
  <span class="token key atrule">commonName</span><span class="token punctuation">:</span> tenantb
  <span class="token key atrule">secretName</span><span class="token punctuation">:</span> tenantb<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>selfsigned
  <span class="token key atrule">privateKey</span><span class="token punctuation">:</span>
    <span class="token key atrule">algorithm</span><span class="token punctuation">:</span> ECDSA
    <span class="token key atrule">size</span><span class="token punctuation">:</span> <span class="token number">256</span>
  <span class="token key atrule">issuerRef</span><span class="token punctuation">:</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> selfsigned<span class="token punctuation">-</span>tenantb<span class="token punctuation">-</span>issuer
    <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterIssuer
    <span class="token key atrule">group</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterIssuer
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> tenantb
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ca</span><span class="token punctuation">:</span>
    <span class="token key atrule">secretName</span><span class="token punctuation">:</span> tenantb<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>selfsigned
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterIssuer
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> selfsigned<span class="token punctuation">-</span>istio<span class="token punctuation">-</span>issuer
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selfSigned</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Certificate
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>ca
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">isCA</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">duration</span><span class="token punctuation">:</span> 87600h <span class="token comment"># 10 years</span>
  <span class="token key atrule">commonName</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>system
  <span class="token key atrule">secretName</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>selfsigned
  <span class="token key atrule">privateKey</span><span class="token punctuation">:</span>
    <span class="token key atrule">algorithm</span><span class="token punctuation">:</span> ECDSA
    <span class="token key atrule">size</span><span class="token punctuation">:</span> <span class="token number">256</span>
  <span class="token key atrule">issuerRef</span><span class="token punctuation">:</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> selfsigned<span class="token punctuation">-</span>istio<span class="token punctuation">-</span>issuer
    <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterIssuer
    <span class="token key atrule">group</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterIssuer
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>system
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ca</span><span class="token punctuation">:</span>
    <span class="token key atrule">secretName</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>selfsigned
</code></pre><p><strong>ClusterIssuer</strong> is not bound to any namespace. An <strong>Issuer</strong> is a namespaced resource, and it is not possible to issue certificates from an Issuer in a different namespace. This means you will need to create an Issuer in each namespace you wish to obtain Certificates in. If you want to create a single Issuer that can be consumed in multiple namespaces, you should consider creating a ClusterIssuer resource.</p>
<h5 id="deploy-istio-on-the-cluster">Deploy Istio on the cluster </h5>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">ISTIOCA</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>kubectl get clusterissuers istio-system <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token string">'{.spec.ca.secretName}'</span> <span class="token operator">|</span> <span class="token function">xargs</span> kubectl get secret <span class="token parameter variable">-n</span> cert-manager <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token string">'{.data.ca\.crt}'</span> <span class="token operator">|</span> base64 <span class="token parameter variable">-d</span> <span class="token operator">|</span> <span class="token function">sed</span> <span class="token string">'s/^/        /'</span><span class="token variable">)</span></span>
$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">TENANTACA</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>kubectl get clusterissuers tenanta <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token string">'{.spec.ca.secretName}'</span> <span class="token operator">|</span> <span class="token function">xargs</span> kubectl get secret <span class="token parameter variable">-n</span> cert-manager <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token string">'{.data.ca\.crt}'</span> <span class="token operator">|</span> base64 <span class="token parameter variable">-d</span> <span class="token operator">|</span> <span class="token function">sed</span> <span class="token string">'s/^/        /'</span><span class="token variable">)</span></span>
$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">TENANTBCA</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>kubectl get clusterissuers tenantb <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token string">'{.spec.ca.secretName}'</span> <span class="token operator">|</span> <span class="token function">xargs</span> kubectl get secret <span class="token parameter variable">-n</span> cert-manager <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token string">'{.data.ca\.crt}'</span> <span class="token operator">|</span> base64 <span class="token parameter variable">-d</span> <span class="token operator">|</span> <span class="token function">sed</span> <span class="token string">'s/^/        /'</span><span class="token variable">)</span></span>
</code></pre><p>Use Helm with the following configuration. The ISTIO_META_CERT_SIGNER is the default cert-signer for workloads. Use environment variables with caution as these environment variables are experimental and can change anytime. Save the follwoing as <code>istiod_values.yaml</code>.</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">global</span><span class="token punctuation">:</span>
  <span class="token key atrule">pilotCertProvider</span><span class="token punctuation">:</span> istiod
  <span class="token comment"># List of cert-signers to allow "approve" action in the cluster role</span>
  <span class="token key atrule">certSigners</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> clusterissuers.cert<span class="token punctuation">-</span>manager.io/istio<span class="token punctuation">-</span>system
    <span class="token punctuation">-</span> clusterissuers.cert<span class="token punctuation">-</span>manager.io/tenanta
    <span class="token punctuation">-</span> clusterissuers.cert<span class="token punctuation">-</span>manager.io/tenantb
<span class="token key atrule">pilot</span><span class="token punctuation">:</span>
  <span class="token comment"># Use with caution as these environment variables are experimental and can change anytime</span>
  <span class="token key atrule">env</span><span class="token punctuation">:</span>
    <span class="token comment"># External CA Integration Type. Permitted value is ISTIOD_RA_KUBERNETES_API</span>
    <span class="token key atrule">EXTERNAL_CA</span><span class="token punctuation">:</span> ISTIOD_RA_KUBERNETES_API
    <span class="token key atrule">CERT_SIGNER_DOMAIN</span><span class="token punctuation">:</span> clusterissuers.cert<span class="token punctuation">-</span>manager.io
<span class="token key atrule">meshConfig</span><span class="token punctuation">:</span>
  <span class="token key atrule">accessLogFile</span><span class="token punctuation">:</span> /dev/stdout
  <span class="token comment"># Default proxy config for mesh wide defaults, used by gateways and sidecars</span>
  <span class="token key atrule">defaultConfig</span><span class="token punctuation">:</span>
    <span class="token key atrule">proxyMetadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">ISTIO_META_CERT_SIGNER</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>system
  <span class="token key atrule">caCertificates</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">pem</span><span class="token punctuation">:</span> <span class="token punctuation">|</span>
$ISTIOCA
      <span class="token key atrule">certSigners</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> clusterissuers.cert<span class="token punctuation">-</span>manager.io/istio<span class="token punctuation">-</span>system
    <span class="token punctuation">-</span> <span class="token key atrule">pem</span><span class="token punctuation">:</span> <span class="token punctuation">|</span>
$TENANTACA
      <span class="token key atrule">certSigners</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> clusterissuers.cert<span class="token punctuation">-</span>manager.io/tenanta
    <span class="token punctuation">-</span> <span class="token key atrule">pem</span><span class="token punctuation">:</span> <span class="token punctuation">|</span>
$TENANTBCA
      <span class="token key atrule">certSigners</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> clusterissuers.cert<span class="token punctuation">-</span>manager.io/tenantb
</code></pre><p>The environment variable <code>PILOT_CERT_PROVIDER</code> is hidden with the default value <code>istiod</code>. We need set this value to <code>k8s.io/clusterissuers.cert-manager.io/istio-system</code> but only for <code>istiod</code> pilot and not gloablly for all Envoy proxies. The problem is that the helm chart only allows setting this value globally (under global) which will set it for all proxies. To resolve this problem, we use <code>kustomize</code> to patch the charts using <a href="https://istio.io/v1.14/docs/setup/additional-setup/customize-installation-helm/"><strong>post rendered kustomization</strong></a>. Save the followings in a folder called <code>kustomize</code>.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token function">cat</span> <span class="token operator">&gt;</span> cert-provider-patch.yaml <span class="token operator">&lt;&lt;</span> <span class="token string">EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: istio-system
  name: istiod
spec:
  template:
    spec:
      containers:
        - name: discovery
          env:
          - name: PILOT_CERT_PROVIDER
            value: "k8s.io/clusterissuers.cert-manager.io/istio-system"
EOF</span>
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token function">cat</span> <span class="token operator">&gt;</span> kustomize.sh <span class="token operator">&lt;&lt;</span> <span class="token string">EOF
#!/bin/bash
cat &gt; base.yaml
exec kubectl kustomize &amp;&amp; rm base.yaml
EOF</span>
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token function">cat</span> <span class="token operator">&gt;</span> kustomization.yaml <span class="token operator">&lt;&lt;</span> <span class="token string">EOF
resources:
- base.yaml
patchesStrategicMerge:
- cert-provider-patch.yaml
EOF</span>
</code></pre><h5 id="install-istio">Install Istio </h5>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ helm repo <span class="token function">add</span> istio https://istio-release.storage.googleapis.com/charts
$ helm repo update
 
$ kubectl create namespace istio-ingress
$ helm <span class="token function">install</span> istio-base istio/base <span class="token punctuation">\</span>
    <span class="token parameter variable">--namespace</span> istio-system <span class="token punctuation">\</span>
    --create-namespace <span class="token punctuation">\</span>
    <span class="token parameter variable">--version</span> <span class="token variable">${ISTIO_VERSION}</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--wait</span>
</code></pre><p>After installing istio base, install istiod with the patch:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>/kustomize$ helm <span class="token function">install</span> istiod istio/istiod <span class="token punctuation">\</span>
   <span class="token parameter variable">--namespace</span> istio-system <span class="token punctuation">\</span>
   --post-renderer ./kustomize.sh <span class="token punctuation">\</span>
   <span class="token parameter variable">--values</span> <span class="token punctuation">..</span>/istiod_values.yaml <span class="token punctuation">\</span>
   <span class="token parameter variable">--wait</span>
</code></pre><p>Or if already installed, upgrade it:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>/kustomize$ helm upgrade istio-istiod istio/istiod <span class="token punctuation">\</span>
   <span class="token parameter variable">--namespace</span> istio-system <span class="token punctuation">\</span>
   --post-renderer ./kustomize.sh <span class="token punctuation">\</span>
   <span class="token parameter variable">--values</span> <span class="token punctuation">..</span>/istiod_values.yaml <span class="token punctuation">\</span>
   <span class="token parameter variable">--wait</span>
</code></pre><p>Restart istiod for changings to take effect:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl rollout restart deploy/istiod <span class="token parameter variable">-n</span> istio-system
</code></pre><p>Note: To ensure the desired changes are reflected in the final chart before committing it to the cluster, use</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>/kustomize$ helm template istiod istio/istiod <span class="token punctuation">\</span>
   <span class="token parameter variable">--namespace</span> istio-system <span class="token punctuation">\</span>
   --post-renderer ./kustomize.sh <span class="token punctuation">\</span>
   <span class="token parameter variable">--values</span> <span class="token punctuation">..</span>/istiod_values.yaml
</code></pre><p>before the installing <code>istiod</code>. The final <code>yaml</code> file is now printed for checking.</p>
<p>To see the installed charts with values in action after installation, use:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ helm get manifest istiod <span class="token parameter variable">-n</span> istio-system
</code></pre><h5 id="congigure-sidecars-to-use-custom-ca-in-each-namespace">Congigure sidecars to use custom CA in each namespace </h5>
<p>Deploy the <code>proxyconfig-tenanta.yaml</code> in the tenanta namespace to define cert-signer for workloads in that namespace.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token function">cat</span> <span class="token operator">&lt;&lt;</span><span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">&gt;</span> ./proxyconfig-tenanta.yaml</span>
apiVersion: networking.istio.io/v1beta1
kind: ProxyConfig
metadata:
  name: tenantapc
  namespace: tenanta
spec:
  environmentVariables:
    ISTIO_META_CERT_SIGNER: tenanta
EOF</span>
</code></pre><pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token function">cat</span> <span class="token operator">&lt;&lt;</span><span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">&gt;</span> ./proxyconfig-tenantb.yaml</span>
apiVersion: networking.istio.io/v1beta1
kind: ProxyConfig
metadata:
  name: tenantbpc
  namespace: tenantb
spec:
  environmentVariables:
    ISTIO_META_CERT_SIGNER: tenantb
EOF</span>
</code></pre><h5 id="test">Test </h5>
<p>Deploy two workload in two namespaces to see they cannot communicate.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl label ns tenanta istio-injection<span class="token operator">=</span>enabled
$ kubectl label ns tenantb istio-injection<span class="token operator">=</span>enabled
$ kubectl apply <span class="token parameter variable">-f</span> samples/httpbin/httpbin.yaml <span class="token parameter variable">-n</span> tenanta
$ kubectl apply <span class="token parameter variable">-f</span> samples/sleep/sleep.yaml <span class="token parameter variable">-n</span> tenanta
$ kubectl apply <span class="token parameter variable">-f</span> samples/httpbin/httpbin.yaml <span class="token parameter variable">-n</span> tenantb
</code></pre><p>When the workloads are deployed, they send CSR requests with related signer info in <code>istiod</code>. <code>istiod</code> forwards the CSR request to the custom CA for signing. The custom CA will use the correct cluster issuer to sign the cert back. Workloads under <code>tenanta</code> namespace will use <code>tenanta</code> cluster issuers while workloads under <code>tenentb</code> namespace will use the <code>tenantb</code> cluster issuers. To verify that they have indeed been signed by correct cluster issuers, we can check istiod logs</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl logs <span class="token operator">&lt;</span>pod_name<span class="token operator">&gt;</span> <span class="token parameter variable">-n</span> istio-system
</code></pre><p>Also can verify workloads under the same namespace can communicate while workloads under the different namespace cannot communicate.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl <span class="token builtin class-name">exec</span> sleep-64fc688f66-cq8b4 <span class="token parameter variable">-n</span> tenanta <span class="token parameter variable">-c</span> <span class="token function">sleep</span> -- <span class="token function">curl</span> http://httpbin.tenanta:8000/html
</code></pre><p>Custom CA Integration - By specifying a Signer name in the Kubernetes CSR Request, this feature allows Istio to integrate with custom Certificate Authorities using the Kubernetes CSR API interface. This does require the custom CA to implement a Kubernetes controller to watch the CertificateSigningRequest Resources and act on them. This provide better multi-tenancy - By specifying a different cert-signer for different workloads, certificates for different tenant’s workloads can be signed by different CAs.</p>
<h3 id="secure-gatways-using-tls-self-signed-certificates">Secure Gatways using TLS self-signed certificates </h3>
<p><a href="https://cert-manager.io"><strong>cert-manager</strong></a> is a tool that automates certificate management. This can be integrated with Istio gateways to manage TLS certificates. Consult the <a href="https://cert-manager.io/docs/releases/">cert-manager installation documentation</a> to get started. cert-manager can be used to write a secret to Kubernetes, which can then be referenced by a Gateway. To get started, configure a Certificate resource, following the cert-manager documentation. The Certificate should be created in the same namespace as the istio-ingressgateway is deployed. The follwoing is an example of certificate created:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token comment">#ingress-issuer.yaml</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Issuer
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> gateway<span class="token punctuation">-</span>issuer
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>ingress
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selfSigned</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Certificate
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> gateway<span class="token punctuation">-</span>tls
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>ingress
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">isCA</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">duration</span><span class="token punctuation">:</span> 87600h <span class="token comment"># 10 years</span>
  <span class="token key atrule">secretName</span><span class="token punctuation">:</span> gateway<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>tls
  <span class="token key atrule">commonName</span><span class="token punctuation">:</span> example.com
  <span class="token key atrule">privateKey</span><span class="token punctuation">:</span>
    <span class="token key atrule">algorithm</span><span class="token punctuation">:</span> ECDSA
    <span class="token key atrule">size</span><span class="token punctuation">:</span> <span class="token number">256</span>
  <span class="token key atrule">issuerRef</span><span class="token punctuation">:</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> gateway<span class="token punctuation">-</span>issuer
    <span class="token key atrule">kind</span><span class="token punctuation">:</span> Issuer
    <span class="token key atrule">group</span><span class="token punctuation">:</span> cert<span class="token punctuation">-</span>manager.io
  <span class="token key atrule">dnsNames</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token string">'example.com'</span>
  <span class="token punctuation">-</span> <span class="token string">'tenanta.example.com'</span>
  <span class="token punctuation">-</span> <span class="token string">'tenantb.example.com'</span>
</code></pre><p>Once we have the certificate created, we should see the secret created in the istio-system namespace:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get secrets —all-namespaces<span class="token operator">=</span>true
</code></pre><p>While it's possible to configure Istio such that it can automatically "discover" the root CA, this can be dangerous in some specific scenarios involving other security holes, enabling signer hijacking attacks. As such, we'll export our Root CA and configure Istio later using that static cert. Also we need the root key in this secret to be fed into browser so it trusts ours endpoint later.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get <span class="token parameter variable">-n</span> istio-ingress secret gateway-ca-tls -ogo-template<span class="token operator">=</span><span class="token string">'{{index .data "tls.crt"}}'</span> <span class="token operator">|</span> base64 <span class="token parameter variable">-d</span> <span class="token operator">&gt;</span> ~/Downloads/ca.pem 
<span class="token comment"># Out of interest, we can check out what our CA looks like</span>
$ openssl x509 <span class="token parameter variable">-in</span> ca.pem <span class="token parameter variable">-noout</span> <span class="token parameter variable">-text</span>
</code></pre><h5 id="add-our-ca-to-a-secret">Add our CA to a secret </h5>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl create secret generic <span class="token parameter variable">-n</span> cert-manager istio-root-ca —from-file<span class="token operator">=</span>ca.pem<span class="token operator">=~</span>/Downloads/ca.pem
</code></pre><p>Use this command to check our secrets are ready and active at ingress gateway:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ istioctl <span class="token parameter variable">-n</span> istio-ingress proxy-config secret deploy/istio-ingressgateway
RESOURCE NAME                   TYPE           STATUS     VALID CERT     SERIAL NUMBER                        NOT AFTER                NOT BEFORE
default                         Cert Chain     ACTIVE     <span class="token boolean">true</span>           713f96b97b33b02f920f9f018edaf954     <span class="token number">2024</span>-07-16T04:42:20Z     <span class="token number">2024</span>-07-15T04:42:20Z
kubernetes://gateway-ca-tls     CA             ACTIVE     <span class="token boolean">true</span>           d06e57986e3cc1ac630b289c6feda894     <span class="token number">2034</span>-07-13T17:16:39Z     <span class="token number">2024</span>-07-15T17:16:39Z
ROOTCA                          CA             ACTIVE     <span class="token boolean">true</span>           563955dc6f7dff4fb998d06f4390eefc     <span class="token number">2034</span>-07-09T13:39:34Z     <span class="token number">2024</span>-07-11T13:39:34Z
</code></pre><p>Also see the issuers are ready:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get clusterissuers —-all-namespaces
</code></pre><p>This can then be referenced in the tls config for a Gateway under credentialName:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.istio.io/v1alpha3
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Gateway
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mygateway
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">istio</span><span class="token punctuation">:</span> ingressgateway
  <span class="token key atrule">servers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span>
      <span class="token key atrule">number</span><span class="token punctuation">:</span> <span class="token number">443</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> https
      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> HTTPS
    <span class="token key atrule">tls</span><span class="token punctuation">:</span>
      <span class="token key atrule">mode</span><span class="token punctuation">:</span> SIMPLE
      <span class="token key atrule">credentialName</span><span class="token punctuation">:</span> gateway<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>tls
    <span class="token key atrule">hosts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> tenanta.example.com
    <span class="token punctuation">-</span> tenantb.example.com
</code></pre><p>Use this command to see the route is added to the ingress.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ istioctl <span class="token parameter variable">-n</span> istio-ingress proxy-config route deploy/istio-ingressgateway
</code></pre><p>Now, <a href="https://istio.io/latest/docs/tasks/traffic-management/ingress/secure-ingress/">install your application</a>:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> samples/httpbin/httpbin.yaml
</code></pre><p>Next, configure the gateway’s ingress traffic routes by defining a corresponding virtual service</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">cat</span> <span class="token operator">&lt;&lt;</span><span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">|</span> kubectl apply <span class="token parameter variable">-f</span> -</span>
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
  - "tenanta.example.com"
  gateways:
  - mygateway
  http:
  - match:
    - uri:
        prefix: /status
    - uri:
        prefix: /delay
    route:
    - destination:
        port:
          number: 8000
        host: httpbin
EOF</span>
</code></pre><p>Add record 10.0.0.240  <a href="http://tenanta.example.com">tenanta.example.com</a> to <code>/etc/hosts</code> on your machine. Now, the moment of truth:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">-v</span> <span class="token parameter variable">-HHost:tenanta.example.com</span> <span class="token parameter variable">--resolve</span> <span class="token string">"tenanta.example.com:443:10.0.0.240"</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--cacert</span> ~/Downloads/ca.pem <span class="token string">"https://tenanta.example.com:443/status/418"</span>
* Added tenanta.example.com:443:10.0.0.240 to DNS cache
* Hostname tenanta.example.com was found <span class="token keyword keyword-in">in</span> DNS cache
*   Trying <span class="token number">10.0</span>.0.240:443<span class="token punctuation">..</span>.
* Connected to tenanta.example.com <span class="token punctuation">(</span><span class="token number">10.0</span>.0.240<span class="token punctuation">)</span> port <span class="token number">443</span>
* ALPN: <span class="token function">curl</span> offers h2,http/1.1
* <span class="token punctuation">(</span><span class="token number">304</span><span class="token punctuation">)</span> <span class="token punctuation">(</span>OUT<span class="token punctuation">)</span>, TLS handshake, Client hello <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>:
*  CAfile: /Users/yas/Downloads/ca.pem
*  CApath: none
* <span class="token punctuation">(</span><span class="token number">304</span><span class="token punctuation">)</span> <span class="token punctuation">(</span>IN<span class="token punctuation">)</span>, TLS handshake, Server hello <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>:
* <span class="token punctuation">(</span><span class="token number">304</span><span class="token punctuation">)</span> <span class="token punctuation">(</span>IN<span class="token punctuation">)</span>, TLS handshake, Unknown <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span>:
* <span class="token punctuation">(</span><span class="token number">304</span><span class="token punctuation">)</span> <span class="token punctuation">(</span>IN<span class="token punctuation">)</span>, TLS handshake, Certificate <span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">)</span>:
* <span class="token punctuation">(</span><span class="token number">304</span><span class="token punctuation">)</span> <span class="token punctuation">(</span>IN<span class="token punctuation">)</span>, TLS handshake, CERT verify <span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">)</span>:
* <span class="token punctuation">(</span><span class="token number">304</span><span class="token punctuation">)</span> <span class="token punctuation">(</span>IN<span class="token punctuation">)</span>, TLS handshake, Finished <span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>:
* <span class="token punctuation">(</span><span class="token number">304</span><span class="token punctuation">)</span> <span class="token punctuation">(</span>OUT<span class="token punctuation">)</span>, TLS handshake, Finished <span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>:
* SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256
* ALPN: server accepted h2
* Server certificate:
*  subject: <span class="token assign-left variable">CN</span><span class="token operator">=</span>example.com
*  start date: Jul <span class="token number">15</span> <span class="token number">17</span>:16:39 <span class="token number">2024</span> GMT
*  expire date: Jul <span class="token number">13</span> <span class="token number">17</span>:16:39 <span class="token number">2034</span> GMT
*  subjectAltName: <span class="token function">host</span> <span class="token string">"tenanta.example.com"</span> matched cert<span class="token string">'s "tenanta.example.com"
*  issuer: CN=example.com
*  SSL certificate verify ok.
* using HTTP/2
* [HTTP/2] [1] OPENED stream for https://tenanta.example.com:443/status/418
* [HTTP/2] [1] [:method: GET]
* [HTTP/2] [1] [:scheme: https]
* [HTTP/2] [1] [:authority: tenanta.example.com]
* [HTTP/2] [1] [:path: /status/418]
* [HTTP/2] [1] [user-agent: curl/8.4.0]
* [HTTP/2] [1] [accept: */*]
&gt; GET /status/418 HTTP/2
&gt; Host:tenanta.example.com
&gt; User-Agent: curl/8.4.0
&gt; Accept: */*
&gt;
&lt; HTTP/2 418
&lt; server: istio-envoy
&lt; date: Mon, 15 Jul 2024 19:35:49 GMT
&lt; x-more-info: http://tools.ietf.org/html/rfc2324
&lt; access-control-allow-origin: *
&lt; access-control-allow-credentials: true
&lt; content-length: 135
&lt; x-envoy-upstream-service-time: 10
&lt;

    -=[ teapot ]=-

       _...._
     .'</span>  _ _ <span class="token variable"><span class="token variable">`</span><span class="token builtin class-name">.</span>
    <span class="token operator">|</span> ."<span class="token variable">`</span></span> ^ <span class="token variable"><span class="token variable">`</span>". _,
    <span class="token punctuation">\</span>_<span class="token punctuation">;</span><span class="token variable">`</span></span><span class="token string">"---"</span><span class="token variable"><span class="token variable">`</span><span class="token operator">|</span>//
      <span class="token operator">|</span>       <span class="token punctuation">;</span>/
      <span class="token punctuation">\</span>_     _/
        <span class="token variable">`</span></span><span class="token string">""</span>"`
Connection <span class="token comment">#0 to host tenanta.example.com left intact</span>
</code></pre><p>And the browser works fine. Gateway and virtual service should be in the same namespace. Use <code>istioctl analyze</code> for troubleshooting.</p>
<h2 id="use-envoy-as-front-proxy-or-reverse-proxy">Use Envoy as Front Proxy (or Reverse Proxy) </h2>
<p>The implementation of proxy servers is essential for businesses that serve content over the internet. Proxy servers are intermediaries connecting end users and the servers where content is located. They effectively manage network traffic and ensure optimal performance and security. By utilizing a proxy server, companies benefit from better load distribution and minimal service downtime.&nbsp;Envoy and HAProxy are among the leading proxy solutions. Both offer unique features and capabilities suited to different needs. Envoy is a cutting edge, high performance edge and service proxy that caters to an array of network protocols and functions. In contrast, HAProxy is a reliable and high performance proxy that’s a popular choice for load balancing.</p>
<p>Envoy is an open source, high performance, edge and service proxy designed to manage and route network traffic between microservices in modern, distributed architectures for cloud native applications So it is an ideal choice for cloud native applications and service mesh implementations. This Proxy is written in C++ language allowing for efficient memory and resource management. Envoy is designed to be scalable, resilient, and easy to operate, making it a popular choice for modern, distributed application architectures. Usually, you run Envoy as a sidecar alongside each service within an application, which in turn abstracts the network from the central business logic.</p>
<p>It receives requests and directs them to services that are positioned behind it. The 2 methods to deploy Envoy are:</p>
<ul>
<li><strong>Front Proxy</strong> – behaves similarly to HAProxy, NGINX, or Apache web server. The Envoy server has its IP address and is positioned separately on the network from the services it guards. Internet traffic comes in and is forwarded to several different services that sit behind it.</li>
<li><strong>Sidecar Proxy</strong> – In this mode of deployment, the Envoy server sits at the same IP address as all the services that it protects. When Envoy is deployed as a sidecar it only has a single instance behind it. The sidecar method intercepts all incoming traffic and optionally all outgoing traffic on behalf of the service instance. Use the IP Table rules to configure the OS to capture and route this traffic to Envoy.</li>
</ul>
<p><strong>HAProxy</strong> runs in two modes: HTTP or TCP. In HTTP mode, it acts as a layer 7 proxy. In TCP mode it acts as a layer 4 proxy. When running in TCP mode, HAProxy has access to the port and IP address that the client is trying to connect to on the backend server. It intercepts information by replacing the server with the target IP address and port.</p>
<p>As part of external auth solution, Envoy was deployed as front proxy (or reverse peoxy) to fan out the traffic from downstream (called <strong>listeners</strong>, in our case is Istio ingress gateway) to its upstream (called <strong>clusters</strong>, in our case is <strong>oauth2 proxies</strong> per tanent). In general, we need to configure listeners and clusters for Envoy in yaml files. This is done dynamically that means, when the files are changed on the filesystem replaced by a file move, and not when the file is edited in place, Envoy will automatically update its configuration. You will need node to uniquely identify the proxy node and dynamic_resources to tell Envoy where to find its dynamic configuration. In dynamic configuration, you need <strong>lds.yaml</strong> for listeners, <strong>cds.yaml</strong> for clusters. See <a href="https://www.envoyproxy.io/docs/envoy/latest/start/quick-start/configuration-dynamic-filesystem">this page</a> for more information.</p>
<p align="center">
      <img src="./assets/k8s/envoy0.png" alt="drawing" width="500" height="300" style="center">
  </p>
<p>The linked lds_config should be an implementation of a Listener discovery service (LDS). The linked cds_config should be an implementation of a Cluster discovery service (CDS). Envoy is usable in a variety of different scenarios, however it’s most useful when <a href="https://www.envoyproxy.io/docs/envoy/latest/intro/deployment_types/deployment_types">deployed</a> as a mesh across all hosts in an infrastructure.</p>
<ul>
<li>
<p><strong>Service to service only</strong></p>
<p align="center">
    <img src="./assets/k8s/envoy1.png" alt="drawing" width="400" height="300" style="center">
</p>
<p>The above diagram shows the simplest Envoy deployment which uses Envoy as a communication bus for all traffic internal to a service oriented architecture (SOA). In this scenario, Envoy exposes several listeners that are used for local origin traffic as well as service-to-service traffic.</p>
</li>
<li>
<p><strong>Service to service plus front proxy</strong></p>
  <p align="center">
      <img src="./assets/k8s/envoy2.png" alt="drawing" width="500" height="150" style="center">
  </p>
<p>The above diagram shows the service to service configuration sitting behind an Envoy cluster used as an HTTP L7 edge reverse proxy. This is the same as istio ingress gateway deployment of Envoy. The reverse proxy provides the following features:</p>
<ul>
<li>Terminates TLS</li>
<li>Supports HTTP/1.1, HTTP/2, and HTTP/3</li>
<li>Full HTTP L7 routing support</li>
<li>Talks to the service to service Envoy clusters via the standard ingress port and using the discovery service for host lookup. Thus, the front Envoy hosts work identically to any other Envoy host, other than the fact that they do not run collocated with another service. This means that are operated in the same way and emit the same statistics.</li>
</ul>
</li>
</ul>
<p>To get a flavor of what Envoy has to offer as a front proxy, we are releasing a docker compose sandbox that deploys a front Envoy and a couple of services (simple aiohttp apps) colocated with a running service Envoy. Below you can see a graphic showing the docker compose deployment. The three containers will be deployed inside a virtual network called <code>envoymesh</code>.</p>
<p align="center">
      <img src="./assets/k8s/envoy3.png" alt="drawing" width="500" height="300" style="center">
  </p>
<p>All incoming requests are routed via the front Envoy, which is acting as a reverse proxy sitting on the edge (or in front) of the envoymesh network. Port 8080, 8443, and 8001 are exposed by docker compose (see docker-compose.yaml) to handle HTTP, HTTPS calls to the services and requests to <code>/admin</code> respectively. Moreover, notice that all traffic routed by the front Envoy to the service containers is actually routed to the service Envoys (routes setup in <code>envoy.yaml</code>). See <a href="https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/front-proxy">this</a> and <a href="https://cloudinfrastructureservices.co.uk/envoy-vs-haproxy-whats-the-difference/">here</a> for more info.</p>
<p>Similarly, in our application, we used Envoy in front of two OAuth proxies that check authentications with two id providers using AWS Cognito service.</p>
<h3 id="proxy-protocol-and-envoy">Proxy Protocol and Envoy </h3>
<p>The <strong>PROXY protocol</strong> allows for exchanging and preservation of client attributes between TCP proxies, without relying on L7 protocols such as HTTP and the <strong>X-Forwarded-For</strong> and X-Envoy-External-Address headers.</p>
<p>It is intended for scenarios where an external TCP load balancer (eg. AWS Netwotk Load Balancer) needs to proxy TCP traffic to a backend TCP service and still expose client attributes such as <strong>source IP</strong> to upstream TCP service endpoints. PROXY protocol can be enabled via <strong>EnvoyFilter</strong>.</p>
<p>When processing the request, the backend often needs to know the client’s “Real” IP address for various reasons, below are some of them:</p>
<ul>
<li><strong>Fraud Prevention</strong>: The client IP address can help identify malicious actors and enable blocking of specific IP addresses associated with abusive behavior, hacking attempts, or denial-of-service attacks.</li>
<li><strong>Access Control</strong>: Some systems restrict access to certain resources based on IP addresses. Knowing the client IP address allows you to implement whitelisting policies.</li>
<li><strong>User Experience</strong>: Geolocation data derived from client IP addresses can be used to tailor content to users based on their location, such as displaying localized content or language.</li>
<li><strong>Application Performance</strong>: Client IP addresses are used to implement rate limiting to prevent abuse and ensure fair usage of resources. It can also be used to distribute traffic effectively and maintain session affinity.</li>
</ul>
<p>Envoy provides several methods to obtain the client’s IP address, including using</p>
<ul>
<li>X-Forwarded-For HTTP header</li>
<li>custom HTTP headers</li>
<li>proxy protocol</li>
</ul>
<p>The X-Forwarded-For header is a widely accepted de facto standard, making it simple to implement and read, as most proxy servers and load balancers support it. Envoy offers two ways to extract the client’s IP address from the X-Forwarded-For header: through the HTTP Connection Manager (HCM) and the IP Detection Extension.</p>
<p>To configure Envoy’s HTTP Connection Manager (HCM) to extract the client’s IP from the X-Forwarded-For header, you need to set <strong>xffNumTrustedHops</strong>. This parameter defines the number of IP addresses in the X-Forwarded-For header that Envoy should trust. Adjust xffNumTrustedHops according to your network topology for proper configuration. For instance, consider a request path like this: client -&gt; proxy1 -&gt; proxy2 -&gt; Envoy. If proxy1 and proxy2 are in a trusted network and both modify the X-Forwarded-For header, we need to set xffNumTrustedHops to 2, telling Envoy to extract the second rightmost IP address in the X-Forwarded-For header and use it as the client’s IP address for the request. Here’s an example of the Envoy configuration for this setting:</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>"name": "envoy.filters.network.http_connection_manager",
"typedConfig": {
  "@type": "type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager",
  // omitted for brevity
  // ...
   
  "xffNumTrustedHops": 2
}
</code></pre><p>As long as the number of hops set in xffNumTrustedHops is correct and these hops can be trusted, we can ensure that malicious users cannot forge the client IP address.</p>
<p>Passing the client’s IP address via HTTP headers works well, but it has a limitation: it only works with HTTP!. If your service also needs to support other protocols, consider using the Proxy Protocol.</p>
<h4 id="what-is-the-proxy-protocol">What is the Proxy Protocol? </h4>
<p><em>Proxy Protocol operates at the transport layer (TCP) to convey the client’s IP address between a proxy and a backend server</em>.</p>
<p>The Proxy Protocol works by adding a header that contains the client’s IP address at the beginning of a TCP connection. This header is inserted immediately after the TCP handshake and before any application data is transmitted. As a result, it’s transparent to the application protocol and can be used with any application protocol, including HTTP, HTTPS, SMTP, and more.</p>
<p align="center">
    <img src="./assets/k8s/proxy-protocol.png" alt="drawing" width="500" height="300" style="center">
</p>
<p>Proxy Protocol has two versions: version 1 and version 2. Version 1 uses a text format that’s human-readable, while version 2 uses a binary format that’s more efficient but less readable. When using Proxy Protocol, we need to ensure that the sending and receiving servers are configured with the same version.</p>
<p>After the TCP connection handshake (SYN(Synchronize Sequence Number), SYN + ACK, SYN) is complete, the sender sends a Proxy Protocol Header to the receiver. This header contains a few fields, what we are interested in is the client’s IP address. Then the proxy server forwards the client’s data right after the Proxy Protocol Header. When the receiver receives a new TCP connection with a Proxy Protocol Header, it first parses this header to extract the client’s IP address and other information. Then it strips the Proxy Protocol Header from the TCP data, ensuring that the actual HTTP request can be processed normally. If the receiver is also an intermediate hop supporting the Proxy Protocol, it can forward the client’s IP address to the next hop in the network, thus preserving the client’s identity throughout the request’s journey.</p>
<p>Here’s how to configure the Proxy Protocol in Envoy. The Proxy Protocol header is inserted during the TCP handshake, so we need to enable it in the Listener settings. We need to add an <code>envoy.filters.listener.proxy_protocol</code> Listener Filter in the Listener configuration. This filter will extract the client’s IP address by parsing the Proxy Protocol Header from the first data packet of the TCP connection. After that, it strips the Proxy Protocol Header and forwards the actual application data to the HTTP Connection Manager (HCM) for further processing.</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>"listener": {
  "@type": "type.googleapis.com/envoy.config.listener.v3.Listener",
  "address": {
    "socketAddress": {
      "address": "0.0.0.0",
      "portValue": 10080
    }
  },
  // omitted for brevity
  // ...

  "listenerFilters": [
    { 
      "name": "envoy.filters.listener.proxy_protocol",
      "typedConfig": {
        "@type": "type.googleapis.com/envoy.extensions.filters.listener.proxy_protocol.v3.ProxyProtocol"
      }
    }
  ],
}
</code></pre><p>Look at the following for more info: <a href="https://www.zhaohuabing.com/post/2024-05-17-client-ip-en/">1</a>, <a href="https://docs.tetrate.io/service-bridge/howto/gateway/https-with-proxy-protocol">2</a>, <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/proxy_protocol">3</a></p>
<p>Note that the Envoy TLS Inspector filter is needed to examine the initial TLS handshake of a connection, allowing Envoy to extract crucial information like the server name indication (SNI) and application-layer protocol negotiation (ALPN) from the client, which can then be used to dynamically route traffic to the appropriate backend based on the identified protocols or hostnames, even when the connection is encrypted with TLS. The Envoy TLS Inspector filter needs the Proxy Protocol filter because it allows Envoy to accurately identify the original source IP address of a connection, even when the connection is behind a load balancer or proxy that might have modified the source address, which is crucial for security analysis and routing decisions when inspecting TLS traffic that is typically encrypted and hides the source information.</p>
<p>If the connection is TLS encrypted, Envoy needs tls-listener filter as well to properly extract proxy protocol header. When a connection arrives at Envoy, the TLS Inspector filter analyzes the TLS handshake to extract information like the Server Name Indication (SNI). If the Proxy Protocol header is present, the filter also parses it to retrieve the original client IP address. SNI detection is the primary reason for using the TLS Inspector filter, as it allows Envoy to identify the intended hostname within a TLS connection, enabling features like SNI-based routing. While the TLS Inspector filter needs to decrypt the initial part of the TLS handshake to extract the necessary information, it does not decrypt the entire communication stream, making it a relatively secure way to leverage TLS information for routing decisions. If you are forwarding HTTP to the upstream not TCP, just use XFF filter (not proxy protocol) to pass user ip address to the upstream.</p>
<p>In the case Envoy is not configured to accept proxy protocol while load balancer is sending proxy protocol header to Envoy, TLS handshake will not be completed and a message like the following will return!</p>
<p align="center">
    <img src="./assets/k8s/proxy-protocol-error.png" alt="drawing" width="600" height="200" style="center">
</p>
<p>It was very painful for me to troubleshoot this!</p>
<hr>
<hr>
<h1 id="gitops-fluxcd">GitOps: FluxCD </h1>
<p>GitOps is an operating model for Kubernetes and other cloud native technologies. It provides a set of best practices that unifies deployment, management, and monitoring for clusters and applications. With the declarative definitions of your system stored in Git, and serving as your canonical source of truth, there is a single place from which your cluster can be managed. This also trivializes rollbacks and roll forwards to take you back to a ‘good state’ if needed.</p>
<p>With Git’s excellent security guarantees, SSH key signed commits to enforce strong security guarantees about authorship as well as the code’s provenance.<br>
With the desired state of your entire system kept under version control, and running in the cluster, you can now employ software controllers to bring the actual state of the cluster in alignment with that desired state, and inform you whenever reality doesn’t match your expectations. The use of a GitOps controller like Flux in combination with these software agents ensures that your entire system is self-healing in the case of human error, for example. In this case, software agents act as the feedback and control loop for your operations.</p>
<h3 id="control-and-feedback-loop">Control and feedback loop </h3>
<p>An essential component of GitOps is feedback and control. But what is meant exactly by that? In order to have control so that developers can ship faster, they need observability built into their deployment workflows. Built in observability allows developers to experiment and make informed decisions on real-time data. For example, when a deployment is being rolled out, a final health check can be made against your running cluster before committing to that update. Or maybe that update didn’t go as planned and needs to be rolled back to a good state. With a feedback control loop, you can effectively answer the following questions:</p>
<ul>
<li>How do I know if my deployment succeeded?</li>
<li>How do I know if the live system has converged to the desired state?</li>
<li>Can I be notified when this differs?</li>
<li>Can I trigger a convergence between the cluster and source control?</li>
</ul>
<p>While Git is the source of truth for the desired state of the system, observability is the benchmark for the actual production state of the running system. GitOps takes advantage of both to manage both applications and infrastructure to increase your team’s productivity.</p>
<h3 id="key-gitops-benefits">Key GitOps benefits </h3>
<p>With GitOps, your organization immediately benefits with the following:</p>
<ul>
<li>
<p>Stronger security guarantees: Git’s strong correctness and security guarantees, backed by Git’s&nbsp;strong cryptography used to track and manage changes, as well as the ability to SSH sign changes to prove authorship and origin are key to a correct and secure definition of the cluster’s desired state.</p>
</li>
<li>
<p>If a security breach does occur, the immutable and auditable source of truth can be used to recreate a new system independently of the compromised one, reducing downtime and allowing for a much better incident response and more effective disaster recovery to meet compliance.</p>
</li>
<li>
<p>Increased speed and productivity: continuous deployment automation with an integrated feedback and control loop speeds up your mean time to deployment by supporting more frequent releases. Declarative definitions kept in Git enable developers to use familiar workflows, reducing the time it takes to spin up new development or test environments to deploy new features. Your teams can ship more changes per day and this translates into faster turnaround for new features and functionality to the customer</p>
</li>
<li>
<p>Reduced mean time to detect and mean time to recovery: the amount of time it takes to recover from a cluster meltdown is also decreased with GitOps best practices. With Git’s built in capability to <strong>revert/rollback</strong> and <strong>fork</strong>, you gain stable and reproducible rollbacks. Since your entire system is described in Git, you have a single source of truth for what to recover after a cluster failure, reducing your meantime to recovery (MTTR) from hours or days to minutes. GitOps provides real time feedback and control loops. In conjunction with other tools like Prometheus for observability and Jaeger for end-to-end tracing, problems can be detected and tracked down, preventing entire cluster meltdowns more quickly, and overall reducing mean time to detect (MTTD) and mean time to locate (MTTL).</p>
</li>
<li>
<p>Improved stability and reliability: due to GitOps providing a single operating model for making infrastructure and apps, you have consistent end-to-end workflows across your entire organization. Not only are your continuous integration and continuous deployment pipelines all driven by pull requests, but your operations tasks are also fully reproducible through Git.</p>
</li>
<li>
<p>Easier compliance and auditing: by incorporating Git as part of your cluster management strategy, you automatically gain a convenient audit trail of who did what and when for all cluster changes outside of Kubernetes that can be used to meet SOC 2 compliance and also ensure stability.</p>
</li>
</ul>
<p>The Flux controllers together with a set of custom Kubernetes resources act on behalf of the cluster. It listens to events relating to custom resource changes, and then applies those changes (depending on the deployment policy) or it can send an alert indicating a divergence from the desired state kept in Git. The GitOps pipeline ensures that what’s in Git matches what’s running in the cluster.</p>
<h3 id="gitops-delivery-pipeline">GitOps delivery pipeline </h3>
<p>GitOps implements a Kubernetes <strong>reconciler</strong> like Flux that listens for and synchronizes deployments to your Kubernetes cluster. Flux makes use of a set of controllers to reconcile divergences that are significant in two ways:</p>
<ul>
<li>
<p>More secure than depending on your CI tool to deploy your application or infrastructure after testing, building, and pushing the image to the container registry</p>
</li>
<li>
<p>Automates complex error prone tasks like manually updating YAML manifests.</p>
</li>
<li>
<p>GitOps is a more secure way to deploy changes: Docker images are pulled into the cluster using <strong>Read Only</strong> access to the container registry.&nbsp; Your CI tool is not granted cluster privileges, reducing attack surface and eliminating significant security risks to your pipeline (known as common attack vectors). With GitOps, cluster credentials are kept within the domain of the cluster and not embedded within bespoke scripts outside of the cluster.</p>
</li>
</ul>
<p>The table below shows how read/write privileges are distributed between the cluster, your CI and CD tooling, and the container repository, as well as<br>
provides your team with a more secure method of creating updates to Kubernetes.</p>
<p>Table 1: GitOps separation of privileges</p>
<p align="center">
    <img src="./assets/k8s/cicd-gitops.png" alt="drawing" width="600" height="200" style="center">
</p>
<h3 id="fluxcd">FluxCD </h3>
<p><strong>FluxCD</strong> is a GitOps tool developed by Weaveworks that allows you to implement continuous and progressive delivery of your applications on Kubernetes. It is a CNCF graduated project that offers a set of controllers to monitor Git repositories and reconciles the cluster’s actual state with the desired state defined by manifests committed in the repo.</p>
<ul>
<li>Flux keeps Kubernetes clusters in sync with configuration kept under source control like Git repositories, and automates updates to that configuration when there is new code to deploy.</li>
<li>It is built using Kubernetes’ API extension server CRDs, and can integrate with Prometheus and other core components of the Kubernetes ecosystem.</li>
<li>Flux supports multi-tenancy and syncs an arbitrary number of Git repositories.</li>
<li>In order to create the reconciliation repository for Flux, you’ll need a personal access token for your GitHub account that has permissions to create repositories. The token must have all permissions under repo checked off.</li>
</ul>
<p>You need to</p>
<ol>
<li>Have a code repo ready</li>
<li>Have a container registry ready</li>
<li>Set up a CI pipeline using GitHub Actions. The actions builds a new container on a <code>git push</code>, tags it with the git-sha, and then pushes it to the container registry. It also updates and commits the image tag change to your kustomize file. Once the new image is in the repository, Flux notices the new image and then deploys it to the cluster</li>
<li>Create secrets with proper permission (principle of least privilege) to allows access to these repos</li>
<li>Install Flux.</li>
</ol>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">GITHUB_TOKEN</span><span class="token operator">=</span><span class="token punctuation">[</span>your-github-token<span class="token punctuation">]</span>
$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">GITHUB_USER</span><span class="token operator">=</span><span class="token punctuation">[</span>your-github-username<span class="token punctuation">]</span>
</code></pre><p>When <strong>bootstrapping</strong> a repository with Flux, it’s also possible to apply only a sub-directory in the repo and therefore connect to multiple clusters or locations on which to apply configuration.<br>
<a href="https://aws.amazon.com/blogs/containers/building-a-gitops-pipeline-with-amazon-eks/">Reference</a></p>
<h4 id="bootstrap-flux">Bootstrap Flux: </h4>
<p>Bootstrap Flux using the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux bootstrap github create <span class="token punctuation">\</span>
  <span class="token parameter variable">--owner</span><span class="token operator">=</span><span class="token variable">$GITHUB_USER</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--repository</span><span class="token operator">=</span>EKS-Istio-Multitenant<span class="token punctuation">\</span>
  <span class="token parameter variable">--personal</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--path</span><span class="token operator">=</span>./clusters/dev <span class="token punctuation">\</span>
  <span class="token parameter variable">--branch</span><span class="token operator">=</span>istio <span class="token punctuation">\</span>
  <span class="token parameter variable">--reconcile</span> <span class="token punctuation">\</span>
  --network-policy <span class="token punctuation">\</span>
  —components-extra<span class="token operator">=</span>image-reflector-controller,image-automation-controller
</code></pre><p>Flux uses the PAT we provided in the environment to bootstrap. The structure of the repo can be different based on different factors such as the number of clusters, number of environments in the clusters (multi-tenancy, namespaces etc.) or even multiple repos.</p>
<p>For us, the flux folders reside in a folder called <code>flux-cd</code> to separate it from the CDK app that creates the cloud infrastructure. This folder will be created and pushed by AWS CodeBuild when the infrastructure is being deployed.</p>
<h4 id="create-a-k8s-secret-to-be-used-by-flux-to-access-repos-instead-of-pat">Create a K8s secret to be used by flux to access repos instead of PAT: </h4>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token assign-left variable">GITHUB_REPO_URL</span><span class="token operator">=</span><span class="token string">'https://github.com/Yas2020/EKS-Istio-Multitenant.git'</span>
$ flux create secret <span class="token function">git</span> chatbot-flux <span class="token punctuation">\</span>
  <span class="token parameter variable">--url</span><span class="token operator">=</span><span class="token variable">$GITHUB_REPO_URL</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--username</span><span class="token operator">=</span><span class="token variable">$GITHUB_USER</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--password</span><span class="token operator">=</span><span class="token variable">$GITHUB_TOKEN</span>
</code></pre><h4 id="specify-the-source-repo-for-flux-to-monitor">Specify the source repo for flux to monitor </h4>
<p>We need to tell the <code>FluxCD</code> to connect to our GitHub repo and monitor the main branch for changes. To do this, we create a source repo using the following command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux create <span class="token builtin class-name">source</span> <span class="token function">git</span> chatbot-source <span class="token punctuation">\</span>
  <span class="token parameter variable">--url</span><span class="token operator">=</span><span class="token variable">$GITHUB_REPO_URL</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--branch</span><span class="token operator">=</span>main <span class="token punctuation">\</span>
  <span class="token parameter variable">--interval</span><span class="token operator">=</span>1m <span class="token punctuation">\</span>
  --secret-ref<span class="token operator">=</span>chatbot-flux <span class="token punctuation">\</span>
  <span class="token parameter variable">--export</span> <span class="token operator">&gt;</span> ./flux-cd/clusters/dev/chatbot-source.yaml
</code></pre><h4 id="specify-the-location-of-manifests-in-the-source-repo">Specify the location of manifests in the source repo </h4>
<p>We also tell Flux resource where to look for our manifests. So we create a <strong>kustomization</strong> resource that takes all the manifests in the base folder and adds some other manifests in the <code>path=./flux-cd/overlays/dev</code> using kustomize command to create the manifests for our dev environment:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux create kustomization chatbot <span class="token punctuation">\</span>
  <span class="token parameter variable">--source</span><span class="token operator">=</span>chatbot-source <span class="token punctuation">\</span>
  <span class="token parameter variable">--path</span><span class="token operator">=</span><span class="token string">"./flux-cd/overlays/dev"</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--prune</span><span class="token operator">=</span>true <span class="token punctuation">\</span>
  <span class="token parameter variable">--wait</span><span class="token operator">=</span>true <span class="token punctuation">\</span>
  <span class="token parameter variable">--interval</span><span class="token operator">=</span>1m <span class="token punctuation">\</span>
  --retry-interval<span class="token operator">=</span>2m <span class="token punctuation">\</span>
  --health-check-timeout<span class="token operator">=</span>3m <span class="token punctuation">\</span>
  <span class="token parameter variable">--export</span> <span class="token operator">&gt;</span> ./flux-cd/clusters/dev/chatbot-kustomization.yaml
</code></pre><p>We fill up the base folder with the manifest resources that are to be deployed in possibly different environments (dev, test, prod). Next, run this command:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kustomize create -- autodetect 
</code></pre><h4 id="create-a-kustomization-resource">Create a kustomization resource </h4>
<p>This kustomization resource collects all manifests in the base folder to be deployed in the cluster. We now create another kustomization resource that will refer to this kustomization resource to implement it. This how we can customize the base manifests for different purposes, such as patching image tag to latest or changing anything else. The following allows acreating new deployments by patching image tag of container images of our apps so the latest version is deployed. This requires creating image policy whcih will be done in the next step.</p>
<p>To this end, add a kustomization resource in path <code>./overlays/dev</code>:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> kustomize.config.k8s.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Kustomization
<span class="token key atrule">resources</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> tenanta<span class="token punctuation">-</span>ns.yaml
<span class="token punctuation">-</span> tenantb<span class="token punctuation">-</span>ns.yaml
<span class="token punctuation">-</span> ../../base
<span class="token key atrule">images</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ghcr.io/yas2020/eks<span class="token punctuation">-</span>istio<span class="token punctuation">-</span>multitenant/app<span class="token punctuation">-</span>ui
  <span class="token key atrule">newName</span><span class="token punctuation">:</span> ghcr.io/yas2020/eks<span class="token punctuation">-</span>istio<span class="token punctuation">-</span>multitenant/app<span class="token punctuation">-</span>ui <span class="token comment"># {"$imagepolicy": "flux-system:app-ui:name"}</span>
  <span class="token key atrule">newTag</span><span class="token punctuation">:</span> 1.0.6 <span class="token comment"># {"$imagepolicy": "flux-system:app-ui:tag"}</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ghcr.io/yas2020/eks<span class="token punctuation">-</span>istio<span class="token punctuation">-</span>multitenant/rag<span class="token punctuation">-</span>api
  <span class="token key atrule">newName</span><span class="token punctuation">:</span> ghcr.io/yas2020/eks<span class="token punctuation">-</span>istio<span class="token punctuation">-</span>multitenant/rag<span class="token punctuation">-</span>api <span class="token comment"># {"$imagepolicy": "flux-system:rag-api:name"}</span>
  <span class="token key atrule">newTag</span><span class="token punctuation">:</span> 1.0.6 <span class="token comment"># {"$imagepolicy": "flux-system:rag-api:tag"}</span>
</code></pre><p>which contains other resources for creating new namespaces. Very simple in our case here! The section images inside this file will patch the image address in the deployment manifests to have the update release tag and thats how the app is upgraded. The part starts with hashtag mark points to the specific image policy (will be created later) that specifies which conditions should be satisfied for images to be updated.  Use the following commands to see the resources:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux get kustomization
$ flux get sources <span class="token function">git</span>
</code></pre><p>Now you can also see flux reconciling the cluster. Run this:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux reconcile kustomization chatbot-source --with-source
</code></pre><p>Get info about the kustomization resource</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux get kustomization chatbot-source <span class="token parameter variable">-n</span> flux-system
</code></pre><p>Flux version 2 lets you easily work with multiple clusters and multiple repositories. New cluster configurations and applications can be applied from the same repo by specifying a new path for each.</p>
<h4 id="automating-image-updates">Automating Image Updates </h4>
<p>The goal here is to streamline the process of updating your application deployments in your cluster when new release is available. Here is our intended workflow:</p>
<ul>
<li>Modify application code, then commit and push the change to the repo</li>
<li>Use GitHub Actions to create a new release in GitHub which kicks off a release workflow to build and push an updated container image to a GitHub Container Registry</li>
<li>FluxCD detects the new image and updates the image tag in the deployment manifest and rolls out the new image to the cluster</li>
</ul>
<p>Add <code>release-chatbot.yaml</code> to <code>github/wrokflow/</code> folder that builds the images and the release.</p>
<p>Using the FluxCLI, run the following command to create the manifest for the ImageRepository resource.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux create image repository app-ui <span class="token punctuation">\</span>
  <span class="token parameter variable">--image</span><span class="token operator">=</span>ghcr.io/yas2020/eks-istio-multitenant/app-ui <span class="token punctuation">\</span>
  <span class="token parameter variable">--interval</span><span class="token operator">=</span>1m <span class="token punctuation">\</span>
  <span class="token parameter variable">--export</span> <span class="token operator">&gt;</span> ./flux-cd/clusters/dev/app-ui-image-source.yaml

$ flux create image repository rag-api <span class="token punctuation">\</span>
  <span class="token parameter variable">--image</span><span class="token operator">=</span>ghcr.io/yas2020/eks-istio-multitenant/rag-api <span class="token punctuation">\</span>
  <span class="token parameter variable">--interval</span><span class="token operator">=</span>1m <span class="token punctuation">\</span>
  <span class="token parameter variable">--export</span> <span class="token operator">&gt;</span> ./flux-cd/clusters/dev/rag-api-image-source.yaml
</code></pre><p>Run the following command to create an ImagePolicy resource to tell FluxCD how to determine the newest image tags. We’ll use the <code>semver</code> filter to only allow image tags that are <strong>valid semantic versions</strong> and equal to or greater than 1.0.0.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux create image policy app-ui <span class="token punctuation">\</span>
  --image-ref<span class="token operator">=</span>app-ui <span class="token punctuation">\</span>
  --select-semver<span class="token operator">=</span><span class="token string">'&gt;=1.0.0'</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--export</span> <span class="token operator">&gt;</span> ./flux-cd/clusters/dev/app-ui-image-policy.yaml

$ flux create image policy rag-api <span class="token punctuation">\</span>
  --image-ref<span class="token operator">=</span>rag-api <span class="token punctuation">\</span>
  --select-semver<span class="token operator">=</span><span class="token string">'&gt;=1.0.0'</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--export</span> <span class="token operator">&gt;</span> ./flux-cd/clusters/dev/rag-api-image-policy.yaml
</code></pre><p>There are <a href="https://fluxcd.io/flux/guides/image-update/#imagepolicy-examples">other filter tags</a> that exists and you can use.</p>
<p>Finally, run the following command to create an <code>ImageUpdateAutomation</code> resource which enables FluxCD to update images tags in our YAML manifests.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux create image update app-ui <span class="token punctuation">\</span>
  <span class="token parameter variable">--interval</span><span class="token operator">=</span>1m <span class="token punctuation">\</span>
  --git-repo-ref<span class="token operator">=</span>chatbot-source <span class="token punctuation">\</span>
  --git-repo-path<span class="token operator">=</span><span class="token string">"./flux-cd/overlays/dev"</span> <span class="token punctuation">\</span>
  --checkout-branch<span class="token operator">=</span>main <span class="token punctuation">\</span>
  --author-name<span class="token operator">=</span>fluxcdbot <span class="token punctuation">\</span>
  --author-email<span class="token operator">=</span>fluxcdbot@users.noreply.github.com <span class="token punctuation">\</span>
  --commit-template<span class="token operator">=</span><span class="token string">"{{range .Updated.Images}}{{println .}}{{end}}"</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--export</span> <span class="token operator">&gt;</span> ./flux-cd/clusters/dev/app-ui-image-update.yaml

$ flux create image update rag-api <span class="token punctuation">\</span>
  <span class="token parameter variable">--interval</span><span class="token operator">=</span>1m <span class="token punctuation">\</span>
  --git-repo-ref<span class="token operator">=</span>chatbot-source <span class="token punctuation">\</span>
  --git-repo-path<span class="token operator">=</span><span class="token string">"./flux-cd/overlays/dev"</span> <span class="token punctuation">\</span>
  --checkout-branch<span class="token operator">=</span>main <span class="token punctuation">\</span>
  --author-name<span class="token operator">=</span>fluxcdbot <span class="token punctuation">\</span>
  --author-email<span class="token operator">=</span>fluxcdbot@users.noreply.github.com <span class="token punctuation">\</span>
  --commit-template<span class="token operator">=</span><span class="token string">"{{range .Updated.Images}}{{println .}}{{end}}"</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--export</span> <span class="token operator">&gt;</span> ./flux-cd/clusters/dev/rag-api-image-update.yaml
</code></pre><p>If you are uncomfortable with FluxCD making changes directly to the checkout branch (in this case main), you can create a separate branch for FluxCD (using the --push-branch parameter) to specify where commits should be pushed to. This will then enable you to follow your normal Git workflows (e.g., create a pull request to merge the changes into the main branch).</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux get image repository store-front
$ flux get image policy store-front
$ flux get image update store-front
</code></pre><p>Update the manifest<br>
We’ll need to update the store-front deployment manifest to use the ImagePolicy resource we created earlier. This can be done by marking the manifest with a comment like this:</p>
<p><code>image: ghcr.io/&lt;REPLACE_THIS_WITH_YOUR_GITHUB_USERNAME&gt;/aks-store-demo/store-front:latest # {"$imagepolicy": "flux-system:store-front"}</code><br>
But ideally, it should be set in the kustomization manifest instead. We have done this before.</p>
<p>To test things, make a change in the app code and push. Create a new 2.0.0 release. This will trigger the release workflow.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ gh release create <span class="token number">2.0</span>.0 --generate-notes

<span class="token comment"># wait about 5 seconds then run the following command</span>
$ gh run <span class="token function">watch</span> 
</code></pre><p>Now check the deployment to see the changes:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get deploy chatbot <span class="token parameter variable">-n</span> dev <span class="token parameter variable">-o</span> yaml <span class="token operator">|</span> <span class="token function">grep</span> image:
</code></pre><p>Done!<br>
<a href="https://paulyu.dev/article/automating-image-updates-with-fluxcd-on-aks/">Reference</a></p>
<h3 id="progressive-deployment-flagger">Progressive Deployment: Flagger </h3>
<p>You can use helm to <a href="https://docs.flagger.app/install/flagger-install-with-flux">install <strong>Flagger</strong></a>, or create flux resources (helm repo, helmrelease- see ). Also install load tester.</p>
<p>Then deploy a canary. Create a <strong>canary resource</strong> and leave it in base folder. In our case, append it to each deployment that you want to deploy progressively. Flagger uses Prometheus for obtaining metrics (see <a href="https://docs.flagger.app/usage/metrics">https://docs.flagger.app/usage/metrics</a>) to test new deployment during canary deployments. These tests are defined in canary resources. For simplicity, we use Istio build-in Prometheus. So when installing Flagger, pass Istio Prometheus endpoint to Flagger like this:<br>
<code>-- set metricServer=http://prometheus.istio-system:9090</code></p>
<p>Get Flux to reconcile the changes:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ flux reconcile kustomization chatbot --with-source 
</code></pre><p>Check for deployments:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get canary <span class="token parameter variable">-n</span> dev store-front <span class="token parameter variable">-w</span>
<span class="token comment"># press ctrl-c to exit </span>
</code></pre><p>Also</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl logs <span class="token parameter variable">-n</span> flagger-system deployment/flagger-system-flagger
</code></pre><h4 id="test-the-canary">Test the Canary: </h4>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token assign-left variable">CURRENT_VERSION</span><span class="token operator">=</span><span class="token number">2.0</span>.0
$ gh release create <span class="token variable">$CURRENT_VERSION</span> --generate-notes
$ gh run <span class="token function">watch</span>

$ kubectl logs <span class="token parameter variable">-n</span> flagger-system deployment/flagger <span class="token parameter variable">-f</span> <span class="token operator">|</span> jq .msg
</code></pre><p>Image update automation is cool, but it’s even cooler when you can implement some sort of gating process around it. Flagger is a great tool to help you implement progressive delivery strategies in your Kubernetes cluster. It works well with Istio and can be configured to use a variety of metrics providers and if it detects an issue, it will rollback the deployment. It’s a great tool to have in your GitOps tool belt and will help you automate your deployments with confidence.<br>
<a href="https://paulyu.dev/article/more-gitops-with-fluxcd-and-flagger-on-aks/">Reference</a></p>
<h5 id="prometheus">Prometheus </h5>
<p>Install :</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ helm repo <span class="token function">add</span> prometheus-community https://prometheus-community.github.io/helm-charts
$ helm upgrade <span class="token parameter variable">-i</span> prometheus prometheus-community/kube-prometheus-stack <span class="token punctuation">\</span>
<span class="token parameter variable">--namespace</span> monitoring <span class="token punctuation">\</span>
--create-namespace 
</code></pre><p>See prometheus <a href="https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml">value</a> charts.</p>
<p>Istio has a version of Prometheus that comes in <code>sample/addons/extras</code> folder. This deployment of Prometheus is intentionally configured with a very short retention window (6 hours) and is also configured to collect metrics from each Envoy proxy running in the mesh, augmenting each metric with a set of labels about their origin (instance, pod, and namespace).</p>
<p>The recommended approach for production-scale monitoring of Istio meshes with Prometheus is to use <strong>hierarchical federation</strong> in combination with a collection of recording rules.</p>
<h5 id="workload-level-aggregation-via-recording-rules">Workload-level aggregation via recording rules </h5>
<p>It is a good idea to aggregate metrics by Istio prometheus and have the central prometheus to scrape the federate endpoint of istio prometheus. In order to aggregate metrics across instances and pods, update the default Prometheus configuration with the following recording rules:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl edit configmap prometheus <span class="token parameter variable">-n</span> istio-system
</code></pre><p>And then add the following under recording_rules.yml:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">groups</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token string">"istio.recording-rules"</span>
  <span class="token key atrule">interval</span><span class="token punctuation">:</span> 5s
  <span class="token key atrule">rules</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">record</span><span class="token punctuation">:</span> <span class="token string">"workload:istio_requests_total"</span>
    <span class="token key atrule">expr</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
      sum without(instance, kubernetes_namespace, kubernetes_pod_name) (istio_requests_total)</span>

  <span class="token punctuation">-</span> <span class="token key atrule">record</span><span class="token punctuation">:</span> <span class="token string">"workload:istio_request_duration_milliseconds_count"</span>
    <span class="token key atrule">expr</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
      sum without(instance, kubernetes_namespace, kubernetes_pod_name) (istio_request_duration_milliseconds_count)</span>

  <span class="token punctuation">-</span> <span class="token key atrule">record</span><span class="token punctuation">:</span> <span class="token string">"workload:istio_request_duration_milliseconds_sum"</span>
    <span class="token key atrule">expr</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
      sum without(instance, kubernetes_namespace, kubernetes_pod_name) (istio_request_duration_milliseconds_sum)</span>

  <span class="token punctuation">-</span> <span class="token key atrule">record</span><span class="token punctuation">:</span> <span class="token string">"workload:istio_request_duration_milliseconds_bucket"</span>
    <span class="token key atrule">expr</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
      sum without(instance, kubernetes_namespace, kubernetes_pod_name) (istio_request_duration_milliseconds_bucket)</span>

  <span class="token punctuation">-</span> <span class="token key atrule">record</span><span class="token punctuation">:</span> <span class="token string">"workload:istio_request_bytes_count"</span>
    <span class="token key atrule">expr</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
      sum without(instance, kubernetes_namespace, kubernetes_pod_name) (istio_request_bytes_count)</span>
</code></pre><p>Or something like:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">data</span><span class="token punctuation">:</span>
  <span class="token key atrule">alerting_rules.yml</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
    {}</span>
  <span class="token key atrule">alerts</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
    {}</span>
  <span class="token key atrule">prometheus.yml</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
  ...
  recording_rules.yml: |
    groups:
    - name: istio.workload.istio_requests_total
      interval: 10s
      rules:
      - record: workload:istio_requests_total
        expr: |
          sum(istio_requests_total{source_workload="istio-ingressgateway"})
          by (
            source_workload,
            source_workload_namespace,
            destination_service,
            source_app,
            destination_app,
            destination_workload,
            destination_workload_namespace,
            response_code,
            response_flags,
            reporter
          )
  rules: |
    {}</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap
</code></pre><p>Reference: <a href="https://docs.solo.io/gloo-mesh-gateway/2.3.x/observability/tools/prometheus/customize/">https://docs.solo.io/gloo-mesh-gateway/2.3.x/observability/tools/prometheus/customize/</a></p>
<p>Then deploy another Prometheus instance to scrape the federated metrics from the Prometheus instance. I used the prometheus instance that is already deployed when we installed Prometheus using helm. We need to create a <strong>ServiceMonitor</strong> resource to scrape the federated metrics:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> monitoring.coreos.com/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceMonitor
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> istio<span class="token punctuation">-</span>federation
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> monitoring
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>prometheus<span class="token punctuation">-</span>stack<span class="token punctuation">-</span>prometheus
    <span class="token key atrule">app.kubernetes.io/part-of</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>prometheus<span class="token punctuation">-</span>stack
    <span class="token key atrule">release</span><span class="token punctuation">:</span> <span class="token string">"prometheus"</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">namespaceSelector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchNames</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> istio<span class="token punctuation">-</span>system
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app.kubernetes.io/name</span><span class="token punctuation">:</span> prometheus
  <span class="token key atrule">endpoints</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">interval</span><span class="token punctuation">:</span> 30s
    <span class="token key atrule">scrapeTimeout</span><span class="token punctuation">:</span> 30s
    <span class="token key atrule">params</span><span class="token punctuation">:</span>
      <span class="token key atrule">'match[]'</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token string">'{__name__=~"workload:(.*)"}'</span>
      <span class="token punctuation">-</span> <span class="token string">'{__name__=~"pilot(.*)"}'</span>
      <span class="token punctuation">-</span> <span class="token string">'{job="envoy-stats", namespace=~"dev"}'</span> <span class="token comment">#You can delete namespace to scrape all the metrics over your cluster</span>
      <span class="token punctuation">-</span> <span class="token string">'{job="prometheus"}'</span>
      <span class="token punctuation">-</span> <span class="token string">'{__name__=~"job:.*"}'</span>
      <span class="token punctuation">-</span> <span class="token string">'{__name__=~"istio:(.*)"}'</span>
    <span class="token key atrule">path</span><span class="token punctuation">:</span> /federate
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">9090</span>
    <span class="token key atrule">honorLabels</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
    <span class="token key atrule">metricRelabelings</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">sourceLabels</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"__name__"</span><span class="token punctuation">]</span>
      <span class="token key atrule">regex</span><span class="token punctuation">:</span> <span class="token string">'workload:(.*)'</span>
      <span class="token key atrule">targetLabel</span><span class="token punctuation">:</span> <span class="token string">"__name__"</span>
      <span class="token key atrule">action</span><span class="token punctuation">:</span> replace 
</code></pre><p>Now, expose Prometheus to have access to Prometheus webpage:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl patch svc prometheus-prometheus <span class="token parameter variable">-n</span> monitoring <span class="token parameter variable">-p</span> <span class="token string">'{"spec": {"type": "LoadBalancer"}}'</span>
</code></pre><p>You see that the federate endpoint is recognized by Prometheus. Wait a bit and then query the metrics to see them available.</p>
<p><strong>Grafana</strong> comes installed with prometheus helm installation. Expose it as well as Kiali. Install Kiali yaml from istio folder…</p>
<p>Ref:<br>
<a href="https://aws.amazon.com/blogs/mt/how-to-reduce-istio-sidecar-metric-cardinality-with-amazon-managed-service-for-prometheus/">https://aws.amazon.com/blogs/mt/how-to-reduce-istio-sidecar-metric-cardinality-with-amazon-managed-service-for-prometheus/</a><br>
<a href="https://istio.io/latest/docs/ops/best-practices/observability/">https://istio.io/latest/docs/ops/best-practices/observability/</a><br>
<a href="https://istio.io/latest/docs/tasks/observability/metrics/querying-metrics/">https://istio.io/latest/docs/tasks/observability/metrics/querying-metrics/</a></p>
<p>Ref - Prometheus:<br>
<a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md#include-servicemonitors">https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md#include-servicemonitors</a><br>
<a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/proposals/202212-scrape-config.md">https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/proposals/202212-scrape-config.md</a><br>
<a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/additional-scrape-config.md">https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/additional-scrape-config.md</a><br>
<a href="https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation">https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation</a><br>
<a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.ServiceMonitor">https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.ServiceMonitor</a><br>
<a href="https://aws.amazon.com/blogs/mt/how-to-reduce-istio-sidecar-metric-cardinality-with-amazon-managed-service-for-prometheus/">https://aws.amazon.com/blogs/mt/how-to-reduce-istio-sidecar-metric-cardinality-with-amazon-managed-service-for-prometheus/</a></p>
<p>Send traffic to app endpoint:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token keyword keyword-while">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword keyword-do">do</span> <span class="token function">curl</span> <span class="token number">10.0</span>.0.240:80 <span class="token operator">&gt;</span> /dev/null<span class="token punctuation">;</span> <span class="token function">sleep</span> <span class="token number">3</span><span class="token punctuation">;</span> <span class="token keyword keyword-done">done</span>
</code></pre><p>Useful Resources for expansions in future projects:<br>
<a href="https://www.eksworkshop.com/docs/automation/gitops/">https://www.eksworkshop.com/docs/automation/gitops/</a><br>
<a href="https://sayantansamanta098.medium.com/github-actions-amazon-eks-ci-cd-pipeline-2b32d9bd761f">https://sayantansamanta098.medium.com/github-actions-amazon-eks-ci-cd-pipeline-2b32d9bd761f</a></p>
<hr>
<hr>
<h1 id="kubernetes-on-linux">Kubernetes on Linux </h1>
<p>We install kubernetes cluster + networking add-on (Calico) + Load Balancer (MetalLB) on Ubuntu 24.04</p>
<p><a href="https://hbayraktar.medium.com/how-to-install-kubernetes-cluster-on-ubuntu-22-04-step-by-step-guide-7dbf7e8f5f99">Ref</a></p>
<p>Step 1: Update and Upgrade Ubuntu (all nodes)</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">apt</span> update <span class="token operator">&amp;&amp;</span> <span class="token function">sudo</span> <span class="token function">apt</span> upgrade
</code></pre><p>Step 2: Disable Swap (all nodes)</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> swapoff <span class="token parameter variable">-a</span>
$ <span class="token function">sudo</span> <span class="token function">sed</span> <span class="token parameter variable">-i</span> <span class="token string">'/ swap / s/^\(.*\)$/#\1/g'</span> /etc/fstab
</code></pre><p>Or you can manually comment out the line for swap setting at <code>/etc/fstab</code>.</p>
<p>Step 3: Add Kernel Parameters (all nodes)<br>
Load the required kernel modules on all nodes:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">tee</span> /etc/modules-load.d/containerd.conf <span class="token operator">&lt;&lt;</span><span class="token string">EOF
overlay
br_netfilter
EOF</span>
$ <span class="token function">sudo</span> modprobe overlay
$ <span class="token function">sudo</span> modprobe br_netfilter
</code></pre><p>Configure the critical kernel parameters for Kubernetes:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">tee</span> /etc/sysctl.d/kubernetes.conf <span class="token operator">&lt;&lt;</span><span class="token string">EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF</span>
</code></pre><p>reload the changes:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">sysctl</span> <span class="token parameter variable">--system</span>
</code></pre><p>Step 4: Install Containerd Runtime (all nodes)<br>
We are using the containerd runtime. Install containerd and its dependencies:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> <span class="token function">curl</span> gnupg2 software-properties-common apt-transport-https ca-certificates
</code></pre><p>Enable the Docker repository:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">curl</span> <span class="token parameter variable">-fsSL</span> https://download.docker.com/linux/ubuntu/gpg <span class="token operator">|</span> <span class="token function">sudo</span> gpg <span class="token parameter variable">--dearmour</span> <span class="token parameter variable">-o</span> /etc/apt/trusted.gpg.d/docker.gpg
$ <span class="token function">sudo</span> add-apt-repository <span class="token string">"deb [arch=amd64] https://download.docker.com/linux/ubuntu <span class="token variable"><span class="token variable">$(</span>lsb_release <span class="token parameter variable">-cs</span><span class="token variable">)</span></span> stable"</span>
</code></pre><p>Update the package list and install containerd:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">apt</span> update
$ <span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> containerd.io
</code></pre><p>Configure containerd to start using systemd as cgroup:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ containerd config default <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">tee</span> /etc/containerd/config.toml <span class="token operator">&gt;</span>/dev/null <span class="token operator"><span class="token file-descriptor important">2</span>&gt;</span><span class="token file-descriptor important">&amp;1</span>
$ <span class="token function">sudo</span> <span class="token function">sed</span> <span class="token parameter variable">-i</span> <span class="token string">'s/SystemdCgroup \= false/SystemdCgroup \= true/g'</span> /etc/containerd/config.toml
</code></pre><p>Restart and enable the containerd service:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> systemctl restart containerd
$ <span class="token function">sudo</span> systemctl <span class="token builtin class-name">enable</span> containerd
</code></pre><p>Step 5: Add Apt Repository for Kubernetes (all nodes)<br>
Kubernetes packages are not available in the default Ubuntu 22.04 repositories. Add the Kubernetes repositories:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">-fsSL</span> https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key <span class="token operator">|</span> $ <span class="token function">sudo</span> gpg <span class="token parameter variable">--dearmor</span> <span class="token parameter variable">-o</span> /etc/apt/keyrings/kubernetes-apt-keyring.gpg
$ <span class="token function">sudo</span> <span class="token function">chmod</span> <span class="token number">644</span> /etc/apt/keyrings/kubernetes-apt-keyring.gpg
<span class="token builtin class-name">echo</span> <span class="token string">'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /'</span> <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">tee</span> /etc/apt/sources.list.d/kubernetes.list
$ <span class="token function">sudo</span> <span class="token function">chmod</span> <span class="token number">644</span> /etc/apt/sources.list.d/kubernetes.list
</code></pre><p><a href="https://v1-30.docs.kubernetes.io/docs/tasks/tools/install-kubectl-linux/">Ref</a><br>
Step 6: Install Kubectl, Kubeadm, and Kubelet (all nodes)</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> <span class="token function">apt</span> update
$ <span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> kubelet kubeadm kubectl
$ <span class="token function">sudo</span> apt-mark hold kubelet kubeadm kubectl
</code></pre><p>—————————MASTER——————————</p>
<p>Step 7: Initialize Kubernetes Cluster with Kubeadm (master node)</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> kubeadm init
</code></pre><p>After the initialization is complete make a note of the kubeadm join command for future reference. Run the following commands on the master node:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> <span class="token environment constant">$HOME</span>/.kube
$ <span class="token function">sudo</span> <span class="token function">cp</span> <span class="token parameter variable">-i</span> /etc/kubernetes/admin.conf <span class="token environment constant">$HOME</span>/.kube/config
$ <span class="token function">sudo</span> <span class="token function">chown</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">id</span> <span class="token parameter variable">-u</span><span class="token variable">)</span></span><span class="token builtin class-name">:</span><span class="token variable"><span class="token variable">$(</span><span class="token function">id</span> <span class="token parameter variable">-g</span><span class="token variable">)</span></span> <span class="token environment constant">$HOME</span>/.kube/config
</code></pre><p>Now, you just need to add worker nodes by running the command suggested by Kubernetes after you initialized the cluster; something like this:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubeadm <span class="token function">join</span> <span class="token number">146.190</span>.135.86:6443 <span class="token parameter variable">--token</span> f1h95l.u4nkex9cw8d0g63w  --discovery-token-ca-cert-hash sha256:6d15f2a79bdb38d1666af50c85f060b9fadc73f13c932e0e2a9eeef08f51f91a
</code></pre><p>Your token and hash is generated by kubeadm. These tokens have expiry date and should be regenerated if you wanted to add more nodes after tokens expire.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubeadm token create ——print—join—command
</code></pre><p>Step :9 Install Kubernetes Network Plugin (master node)<br>
To enable communication between pods in the cluster, you need a network plugin. Install the Calico network plugin:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml
</code></pre><p>Step 10: Verify the cluster and test (master node)</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get pods <span class="token parameter variable">-n</span> kube-system
$ kubectl get nodes
</code></pre><p>Now your cluster is ready to deploy your pods!</p>
<h4 id="metallb-installation">MetalLB Installation </h4>
<p><a href="https://metallb.universe.tf/installation/">Ref</a></p>
<p><strong>MetalLB</strong> provides a network load-balancer implementation for Kubernetes clusters that do not run on a supported cloud provider, effectively allowing the usage of LoadBalancer Services within any cluster. In the case of bare metal installation of Kubernetes on premise, one needs a load balancer to make applications easily available out of Kubernetes cluster through a single endpoint. MetalLB is one of the popular options that works inside Kubernetes cluster as opposed to cloud LB that operate outside. Then ingress-nginx will be installed as another layer to for routing traffic based on domain or path. It is a good idea to install MetalLB.</p>
<p>Preparation<br>
If you are using kube-proxy in IPVS mode, starting from Kubernetes v1.14.2, you need to enable strict ARP mode. By default, Kube-router enables strict ARP, so this feature is not required if you are using Kube-router as a service proxy.</p>
<p>Before applying strict ARP mode, check the current mode.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token comment"># see what changes would be made, returns nonzero returncode if different</span>
$ kubectl get configmap kube-proxy <span class="token parameter variable">-n</span> kube-system <span class="token parameter variable">-o</span> yaml <span class="token operator">|</span> <span class="token function">grep</span> strictARP
</code></pre><p>If <code>strictARP: false</code> is outputted, run the following to change it to strictARP: true. (If strictARP: true is already outputted, you do not need to execute the following command).</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code><span class="token comment"># actually apply the changes, returns nonzero returncode on errors only</span>
$ kubectl get configmap kube-proxy <span class="token parameter variable">-n</span> kube-system <span class="token parameter variable">-o</span> yaml <span class="token operator">|</span> <span class="token punctuation">\</span>
$ <span class="token function">sed</span> <span class="token parameter variable">-e</span> <span class="token string">"s/strictARP: false/strictARP: true/"</span> <span class="token operator">|</span> <span class="token punctuation">\</span>
$ kubectl apply <span class="token parameter variable">-f</span> - <span class="token parameter variable">-n</span> kube-system
</code></pre><h5 id="installation---manifest">Installation - Manifest </h5>
<p>To install MetalLB, apply the manifest:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml
</code></pre><p>Wait until both pods in the metallb-system namespace are Running. Check:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get pod <span class="token parameter variable">-n</span> metallb-system
</code></pre><h5 id="configuration">Configuration </h5>
<p>Setting up the load balancing policy of MetalLB can be done by deploying a configmap containing the related configuration information. There are two modes that can be configured in MetalLB:</p>
<ul>
<li>Layer 2 Mode</li>
<li>BGP Mode</li>
</ul>
<p>Here we will proceed with Layer 2 mode. Define The IPs To Assign To The Load Balancer Services:</p>
<p>Create a file called metallb-config.yaml and fill it as follows:</p>
<pre data-role="codeBlock" data-info="yaml" class="language-yaml yaml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> metallb.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> IPAddressPool
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> first<span class="token punctuation">-</span>pool
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> metallb<span class="token punctuation">-</span>system
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">addresses</span><span class="token punctuation">:</span>
  <span class="token comment"># The ip address range should be from the same network that the nodes belong to.</span>
  <span class="token punctuation">-</span> 192.168.1.240<span class="token punctuation">-</span>192.168.1.250
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> metallb.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> L2Advertisement
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> metallb<span class="token punctuation">-</span>system
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ipAddressPools</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> first<span class="token punctuation">-</span>pool
</code></pre><p>Apply the changes:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl create <span class="token parameter variable">-f</span> metallb-config.yaml
</code></pre><p>Test MetalLB:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl create deploy my-nginx —-image<span class="token operator">=</span>nginx
$ kubectl expose deploy my-nginx <span class="token parameter variable">--port</span> <span class="token number">80</span> <span class="token parameter variable">--type</span> LoadBalancer
$ kubectl get all
$ <span class="token function">curl</span> http://<span class="token variable">$EXTERNAL_IP</span>
</code></pre><p>This show show you the nginx welcome page.</p>
<h4 id="install-helm">Install Helm </h4>
<p><a href="https://helm.sh/docs/intro/install/">Installing Helm</a><br>
Helm&nbsp;is a package management for Kubernetes</p>
<p>From Apt (Debian/Ubuntu)<br>
Members of the Helm community have contributed a Helm package for Apt. This package is generally up to date.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">curl</span> https://baltocdn.com/helm/signing.asc <span class="token operator">|</span> gpg <span class="token parameter variable">--dearmor</span> <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">tee</span> /usr/share/keyrings/helm.gpg <span class="token operator">&gt;</span> /dev/null

<span class="token builtin class-name">echo</span> <span class="token string">"deb [arch=<span class="token variable"><span class="token variable">$(</span>dpkg --print-architecture<span class="token variable">)</span></span> signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main"</span> <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">tee</span> /etc/apt/sources.list.d/helm-stable-debian.list

$ <span class="token function">sudo</span> <span class="token function">apt</span> update

$ <span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> helm
</code></pre><h4 id="useful-commands-for-kubernetes">Useful commands for Kubernetes: </h4>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl get pods <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span> <span class="token parameter variable">-o</span> wide
$ kubectl get svc  <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span> <span class="token parameter variable">-o</span> wide
$ kubectl logs <span class="token operator">&lt;</span>pod name<span class="token operator">&gt;</span> n <span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span>
$ kubectl get pods —all-namespaces
$ kubectl delete pod <span class="token operator">&lt;</span>pod name<span class="token operator">&gt;</span> <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span>
$ kubectl delete svc <span class="token operator">&lt;</span>service name<span class="token operator">&gt;</span> <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span>
<span class="token comment">#delete forcefully</span>
$ kubectl delete pod <span class="token operator">&lt;</span>pod name<span class="token operator">&gt;</span> --grace-period<span class="token operator">=</span><span class="token number">0</span> <span class="token parameter variable">--force</span> <span class="token parameter variable">--namespace</span> <span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span>
$ kubectl create <span class="token parameter variable">-f</span> <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>.yaml
$ kubectl apply <span class="token parameter variable">-f</span> <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>.yaml
$ kubectl delete <span class="token parameter variable">-f</span> <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>.yaml
<span class="token comment"># print all resources in a ns</span>
$ kubectl <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span> get all
$ kubectl get ns
<span class="token comment"># quickly deploy and expose a pod given an image </span>
$ kubectl create deploy mynginx <span class="token parameter variable">--image</span> nginx
$ kubectl expose deploy mynginx <span class="token parameter variable">--port</span><span class="token operator">=</span><span class="token number">80</span> —type<span class="token operator">=</span>LoadBalancer
</code></pre><p>The command to convert a service type to a NodePort (or LoadBalancer) service type</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl patch svc <span class="token operator">&lt;</span>service-name<span class="token operator">&gt;</span> <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span> <span class="token parameter variable">-p</span> <span class="token string">'{"spec": {"type": “NodePort”}}'</span>
</code></pre><p>Very useful command used on worker nodes to see the logs of changes in the node</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ journalctl <span class="token parameter variable">-f</span> <span class="token parameter variable">-u</span> kubelet
</code></pre><p>Reslove a domain into an ip+port</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">--resolve</span> hello-world.info:80:10.0.2.8 <span class="token parameter variable">-i</span> http://hello-world.info
</code></pre><p>Restart the cluster:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">sudo</span> swapoff <span class="token parameter variable">-a</span>
$ <span class="token function">sudo</span> systemctl restart containerd
$ <span class="token function">sudo</span> systemctl restart kubelet.service 
</code></pre><p>Copy files a VM - port forwarding, Nat networking</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ <span class="token function">scp</span> <span class="token parameter variable">-P</span> <span class="token number">2220</span> <span class="token parameter variable">-r</span> ~/Downloads/k8s-config generic@10.0.0.67:.
</code></pre><p>Delete pod:</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl delete pod <span class="token operator">&lt;</span>PODNAME<span class="token operator">&gt;</span> --grace-period<span class="token operator">=</span><span class="token number">0</span> <span class="token parameter variable">--force</span> <span class="token parameter variable">--namespace</span> <span class="token operator">&lt;</span>NAMESPACE<span class="token operator">&gt;</span>

</code></pre><h5 id="kubectl-exec-syntax">kubectl exec Syntax </h5>
<p>You can use <code>kubectl exec</code> to connect to a running container and then execute single commands. Connecting to a container is useful to view logs, inspect processes, mount points, environment variables, and package versions, amongst other things. <code>kubectl exec</code> will give you full shell access to the container, so modifying it and installing packages that are not part of the container image is possible but is not recommended unless for temporary troubleshooting purposes. If extra modifications or packages are required permanently for the container, the image should be modified, and the new version should be deployed to maintain immutability and reproducibility.</p>
<p>Let’s take a look at the syntax of the kubectl exec command with an example.</p>
<pre data-role="codeBlock" data-info="sh" class="language-bash sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">--stdin</span> <span class="token parameter variable">--tty</span> aks-helloworld-one-56c7b8d79d-xqx5t -- /bin/bash
<span class="token comment">#Or</span>
$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-ti</span> <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span> pingtest-7b5d44b647-kz5lc -- <span class="token function">sh</span>
</code></pre><h3 id="how-to-find-unused-ip-address-on-my-network">How to find unused IP address on my network? </h3>
<p>Probably the best way is to use <strong>NMAP</strong> (<a href="http://nmap.org/">http://nmap.org/</a>) in ARP Ping scan mode. The usage will be something like <code>nmap -sP -PR 192.168.0.*</code> (or whatever your network is).</p>
<p>The advantage of this approach is that it uses the Address Resolution Protocol to detect if IP addresses are assigned to machines. Any machine that wants to be found on a network needs to answer the ARP, so this approach works where ping scans, broadcast pings and port scans don't (due to firewalls, OS policy, etc.).</p>

      </div>
      
      
     <button id="backToHomeBtn" onclick="goToHome()">Back to Profile</button>
    
  </body>
  <script>
   function goToHome() {
       window.location.href = 'https://Yas2020.github.io/#portfolio';  // Replace with your home page URL
   }
 </script>
     
  </html>