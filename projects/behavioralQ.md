
## The Behavioral Interview Reality (important framing)
Behavioral interviews are not about:
- being likable
- sounding inspirational
- having perfect past titles

They are about **risk reduction**. 

Interviewers are subconsciously asking:
- Will this person collapse under ambiguity?
- Will they spin wheels?
- Will they blame others?
- Can they ship without hand-holding?
- Do they think in systems?

You already have the raw material. We just need to package it cleanly.

The 9 Core Behavioral Questions (this is enough)
If you prepare these deeply, youâ€™re covered for 90% of interviews.

##### 1ï¸âƒ£ â€œTell me about yourselfâ€ (MOST IMPORTANT)
What theyâ€™re really testing
- Narrative coherence
- Seniority signal
- Whether your story makes sense

Your angle
- Independent builder
- Systems-first ML engineer
- Depth over pedigree

Weâ€™ll script this carefully later.

##### 2ï¸âƒ£ â€œTell me about a challenging technical problem you solvedâ€
They want
- How you think when stuck
- Debugging under uncertainty
- Decision-making

Your goldmine
- RLHF pipeline issues
- RAG hallucinations / retrieval failures
- Multi-tenant isolation bugs
- Latency / scaling tradeoffs

##### 3ï¸âƒ£ â€œTell me about a time you worked with ambiguityâ€
This is huge for senior ML roles.
They want
- Do you freeze or decompose?
- Can you define success when none exists?
- Your advantage

You lived in ambiguity for years.

##### 4ï¸âƒ£ â€œDescribe a failure or mistakeâ€
Trap question.
Theyâ€™re watching for:
- defensiveness
- blame
- lack of learning
- Correct framing
- You spotted it early
- You fixed it
- You changed your process

Not drama. Not shame.

##### 5ï¸âƒ£ â€œHow do you prioritize when everything feels important?â€
This is secretly a seniority test.
They want
- Tradeoffs
- Impact vs effort thinking
- Business awareness

Youâ€™ll answer like a staff engineer, not a task-taker.

##### 6ï¸âƒ£ â€œTell me about a project youâ€™re most proud ofâ€
This is where you control the interview.
You choose:
- RAG platform
- RLHF system
- MLOps pipeline

And then show ownership, not features.

7ï¸âƒ£ â€œHow do you handle feedback or disagreement?â€
Theyâ€™re filtering for:
- ego issues
- rigidity
- collaboration risk

Youâ€™ll frame this as:
- feedback as signal
- push back with evidence
- align on outcomes

##### 8ï¸âƒ£ â€œWhy are you interested in this role / company?â€
This is not about passion.
Itâ€™s about fit + maturity.
Weâ€™ll keep it:
- specific
- grounded
- non-sycophantic

##### 9ï¸âƒ£ â€œIs there anything unusual about your background we should know?â€
This is the gap question in disguise.
Handled correctly, it becomes a strength.



### Story 1 â€” RLHF PPO â€œFailureâ€ (this is NOT a failure)
Why this story is powerful
This story signals:
- Cost awareness (rare in candidates)
- Experimental judgment
- Understanding of ML beyond tutorials
- Willingness to stop, evaluate, and adapt

#### How to frame it (clean, interview-safe)
##### Situation
I was building an end-to-end RLHF pipeline using PPO to align a GPT-2 policy with a learned reward model.
##### Challenge
PPO training turned out to be far more compute-intensive and slow than expected, especially given my resource constraints.
###### Judgment / Action
Rather than treating â€œfull convergenceâ€ as the only success metric, I paused training early once I saw stable loss reduction in the first epoch. I then evaluated the partially trained policy against held-out prompts.
##### Result
Even with limited training, the policy showed a measurable improvement over the base model on reward scores and qualitative outputs.
###### Learning
This reinforced that in real ML systems, progress is not binary. Partial training, early stopping, and evaluation-driven decisions can produce meaningful gains under constraints.

What interviewers hear
- â€œThis person understands experimental designâ€
- â€œThey donâ€™t burn money blindlyâ€
- â€œThey can make tradeoffs under pressureâ€

This is not a weakness story. Itâ€™s a judgment-under-constraints story.

Questions this story answers
- â€œTell me about a failureâ€
- â€œTell me about a difficult ML problemâ€
- â€œHow do you make decisions with limited resources?â€
- â€œHave you worked with RLHF or PPO in practice?â€

### Story 2 â€” Hallucination Detection & Mitigation (very strong)
This one is gold for modern ML interviews.

##### Situation
While building a production-style RAG system, hallucination became a key reliability risk, especially when retrieval was weak or ambiguous.
##### Challenge
Thereâ€™s no single reliable hallucination detector, and false confidence can be worse than admitting uncertainty.
##### Action
I implemented and evaluated multiple approaches:
- LLM-as-a-Judge scoring
- Similarity between retrieved chunks and generated answers
- BERTScore / token-level similarity
- Confidence heuristics based on retrieval coverage

##### Decision
I treated hallucination detection as a signal aggregation problem rather than a single classifier.

##### Outcome
This allowed the system to either lower confidence, warn users, or trigger fallback behavior instead of presenting uncertain answers as facts.

##### Learning
Reliability in LLM systems comes from layered safeguards, not a single metric.

What interviewers hear
- â€œThis person understands LLM systems deeplyâ€
- â€œThey think like a production engineerâ€
- â€œThey can reason about uncertaintyâ€

This is senior ML thinking.

Questions this story answers
- â€œTell me about a challenging system you builtâ€
- â€œHow do you handle LLM hallucinations?â€
- â€œHow do you design reliable AI systems?â€
- â€œTell me about ambiguityâ€













## Answers pitched for ML Engineer Roles:

### Q1- Tell me about yourself (Final â€œUniversalâ€ Version (â‰ˆ45â€“60 seconds)):
This is the one you should memorize.

Iâ€™m a Machine Learning Engineer with a PhD in Mathematics, focused on building production-grade ML and LLM systems.

I started from a strong theoretical background and transitioned into applied machine learning and modern AI systems, working end-to-end from modeling to deployment.

Iâ€™ve built large-language-model systems including reinforcement-learning-based alignment and production-style RAG systems, with attention to reliability, latency, and cost.

On the engineering side, Iâ€™ve designed and operated cloud-native deployments, using Kubernetes, CI/CD, observability, and security best practices.

Iâ€™m comfortable owning ambiguous problems end-to-end and making pragmatic trade-offs to get systems working in the real world.

Iâ€™m now looking to contribute as a hands-on ML engineer on production AI systems within a strong engineering team.

Stop.
This is clean, senior, and non-defensive.

<!-- 
##### Old Versop

"Thank you for having me. My name is Yaser, and I have a PhD in Applied Mathematics. My strong foundation in math, statistics, and programming naturally led me to machine learning, where I became deeply interested in applying AI to solve real-world problems.

Over the years, Iâ€™ve built hands-on experience across the ML lifecycle â€” from data collection and model development to deployment â€” working with tools like Scikit-learn, PyTorch, Hugging Face, and AWS. I also strengthened my skills in DevOps and MLOps, and Iâ€™m now certified in both Machine Learning and DevOps on AWS.

Iâ€™m passionate about technology and committed to lifelong learning. I enjoy solving open-ended problems and collaborating with others, and Iâ€™m excited to bring both my technical depth and practical experience to an applied ML team." -->

<!-- 
#### Alternative version

Think of it like a movie trailer â€” clear arc, no wandering:
- PhD in Applied Math â†’ foundation in modeling, optimization, statistics.
- Curiosity-driven projects â†’ built hands-on ML/LLM systems (RLHF, multi-LLM chatbot, AWS CI/CD pipelines, ML pipeline w/ monitoring).
- Production focus â†’ you didnâ€™t just play with models; you built end-to-end systems with deployment, automation, reliability.
- Now â†’ want to bring that mix of deep math + practical ML/DevOps to a real-world product team.
 -->

#### Theyâ€™ll likely ask:
â€œWas your PhD related to AI/ML?â€
ğŸ‘‰ Answer: â€œNot directly â€” it was in applied math, focusing on optimization and modeling. But that background gave me the math/algorithmic foundations that made it natural to pick up ML/AI. In fact, during and after my PhD, I applied those skills to build hands-on ML projects like RLHF, LLM deployment, and automated pipelines.â€
â€œTell me about your background.â€
ğŸ‘‰ Use the Core Story above (2â€“3 mins).
â€œWhat experience do you have with ML in practice?â€
ğŸ‘‰ Highlight 2â€“3 projects:
RLHF with GPT-2 (training loop, reward model, PPO).
Multi-LLM chatbot on AWS (LangChain, SageMaker, RAG, StepFunctions).
End-to-end ML pipeline with CI/CD + monitoring.
â€œWhy EvenUp / Why now?â€
ğŸ‘‰ Answer: â€œIâ€™ve built strong independent systems, but now I want to apply those skills in a team setting where models directly impact customers and scale in production.â€


You donâ€™t want them thinking â€œthis is too academic.â€
So frame it like:
â€œMy PhD wasnâ€™t theoretical â€” it gave me a problem-solving mindset and the math/optimization base for ML.â€
â€œEverything Iâ€™ve done since then is focused on applied ML engineering.â€



### Addressing the Gap

##### 1ï¸âƒ£ The Correct Framing (Very Important)
The gap is not unemployment.
It is:
- Independent applied AI/ML systems work with production scope

The mistake people make is defensive framing. Your framing must be intentional, factual, and forward-looking.
You are not saying:
- â€œI couldnâ€™t find a jobâ€

You are saying:
- â€œI chose to focus full-time on building and operating production-grade ML systems while positioning myself for the right role.â€

No drama. No emotion. No oversharing.

##### 2ï¸âƒ£ The 30-Second Version (Default Answer)
Use this exact structure:
â€œFrom 2023 to the present, Iâ€™ve been working full-time on independent applied ML and LLM systems. I treated this as production work â€” designing end-to-end pipelines, building and operating RAG and RLHF systems, and deploying them with real cloud infrastructure, CI/CD, and observability. My focus was staying hands-on with modern LLM systems at production depth rather than taking a short-term role that didnâ€™t align with where the field was going.â€

Stop.
Let them ask more.

##### 3ï¸âƒ£ The 60-Second Version (If They Want Detail)
Only expand if invited:
â€œSpecifically, I built and operated multiple production-style systems â€” including an RLHF training pipeline, a serverless multi-LLM RAG platform, and a multi-tenant EKS-based LLM system with Istio, GitOps, and canary deployments. I owned everything end-to-end: data pipelines, model behavior, infra tradeoffs, cost, and failure modes. This period strengthened my system design and execution skills, and now Iâ€™m looking to apply that experience in a team setting with real users and business impact.â€

This signals:
- Ownership
- Intentionality
- Senior-level scope
- Readiness to re-enter teams

##### 4ï¸âƒ£ Handling Follow-Up Questions (Critical)
- â“ â€œWhy not do this inside a company?â€
Answer:
â€œAt the time, I wanted the freedom to go deep across the stack â€” from modeling to infra â€” which is hard to do in a narrow role. Now Iâ€™m intentionally looking for a team where that breadth adds value.â€

- â“ â€œHow do you stay aligned with production standards without a team?â€
Answer:
â€œI followed production practices deliberately â€” CI/CD, canary rollouts, monitoring, cost controls, and design reviews through public RFCs and reference architectures. I also benchmarked against industry tools and failure modes rather than toy examples.â€
- â“ â€œWould you be comfortable re-entering a structured team?â€
Answer (important tone):
â€œAbsolutely. Iâ€™ve taught, collaborated, and worked in teams before. This period strengthened my autonomy â€” it didnâ€™t replace collaboration.â€

##### 5ï¸âƒ£ What NOT to Say (Ever)
- ğŸš« â€œIt was hard to find a jobâ€
- ğŸš« â€œThe market was badâ€
- ğŸš« â€œI had no choiceâ€
- ğŸš« â€œI was unemployed butâ€¦â€
- ğŸš« Over-explaining finances, stress, or emotions

These weaken your signal, even though theyâ€™re human.

##### 6ï¸âƒ£ Internal Mindset (Say This to Yourself)
â€œThis gap shows depth, not drift.â€
You didnâ€™t pause your career.
You changed the mode of how you built experience.

##### 7ï¸âƒ£ Final Calibration (Very Important)
This explanation works best for:
- Applied ML Engineer
- Senior ML / AI Engineer
- LLM Systems / Platform roles
- Infrastructure-aware ML roles

It will not work for:
- Junior roles
- Process-heavy corporate roles
- Places that filter purely by logos

Thatâ€™s okay. Those werenâ€™t your targets anyway.


### Q1: â€œTell me about a time you faced a major challenge or failureâ€
What theyâ€™re testing
- Judgment under constraints
- Learning without drama
- Ability to stop, reassess, and still deliver value

Your answer (polished)
â€œIn my RLHF project, I initially planned to fully train a PPO-based policy model. Early on, I realized PPO was far more compute- and time-intensive than expected, which made full training impractical given my constraints.
Instead of pushing blindly, I reassessed what success meant. I stopped training early once the reward loss stabilized and evaluated the partially trained model. Even with limited steps, it showed measurable improvement over the base model on held-out preference data.
The takeaway for me was learning when to optimize for signal rather than completeness. I proved the approach worked, learned the dynamics of PPO firsthand, and avoided unnecessary cost â€” which is the same judgment Iâ€™d apply in a production environment.â€

Why this works
- Failure â†’ reframed as judgment
- No excuses
- Clear learning signal

### A2: â€œTell me about a time you had to make a hard technical decisionâ€
What theyâ€™re testing
- Trade-offs
- Senior thinking
- Ownership

Your answer
â€œIn a RAG system I built, hallucination and storage strategy became a serious issue as the document set grew. I had to decide whether to keep pushing retrieval complexity or introduce explicit safeguards.
I chose a layered approach: improving chunking and indexing first, then adding lightweight verification mechanisms like similarity checks and LLM-based evaluation instead of heavy, brittle logic.
That decision balanced reliability with system complexity. It kept latency reasonable while still giving us signals when answers were likely unreliable â€” which mattered more than chasing perfect accuracy.â€

### Q3:  â€œDescribe a time you worked through ambiguityâ€
What theyâ€™re testing
- Can you operate without specs?
- Can you define the problem yourself?

Your answer
â€œMost of my recent projects were intentionally open-ended. For example, in building a multi-tenant LLM platform on Kubernetes, there was no reference architecture for what I wanted â€” tenant isolation, GitOps automation, and secure ingress all at once.
I broke the problem into layers: networking and identity first, then deployment safety, then observability. I validated each layer independently before composing them.
Working this way taught me to reduce ambiguity by creating structure, rather than waiting for it.â€

### Q4: â€œTell me about a time you took ownershipâ€
What theyâ€™re testing
- Do you default to responsibility?
- Can you lead without authority?

Your answer
â€œIn my EKS-based project, I took full ownership of the platform â€” infrastructure, security, CI/CD, and deployment strategy. There was no team to escalate to, so when something broke, it was my responsibility to diagnose it end-to-end.

For example, when an Istio upgrade silently broke proxy protocol handling, I traced the failure across the load balancer, gateway filters, and Envoy configuration until I isolated the root cause.
That experience strengthened my ability to debug distributed systems holistically, not just at the application layer.â€

### Q5: â€œTell me about a time you learned something difficult quicklyâ€
What theyâ€™re testing
- Learning velocity
- Depth vs surface knowledge

Your answer
â€œKubernetes and service mesh concepts were initially unfamiliar to me. Instead of treating them as tools, I invested time understanding the underlying networking, identity, and certificate models.

That allowed me to confidently implement multi-tenant isolation, GitOps workflows, and canary deployments rather than relying on defaults.
Iâ€™ve learned that going one layer deeper early saves a lot of time later.â€

### Q6: â€œHow do you handle setbacks or frustration?â€
What theyâ€™re testing
- Emotional regulation
- Professional maturity

Your answer
â€œWhen something becomes frustrating, I try to separate emotion from diagnosis. I write down what I know, what I donâ€™t, and what changed recently.

That approach helped during prolonged debugging sessions â€” like certificate and mTLS issues in Istio â€” where the solution wasnâ€™t obvious.
Staying methodical keeps me productive even under pressure.â€

### Q7:  â€œWhy should we hire you?â€
What theyâ€™re testing
- Self-awareness
- Value articulation (not ego)

Your answer
â€œI bring a combination of strong fundamentals and hands-on execution. Iâ€™m comfortable moving from modeling and evaluation to infrastructure, deployment, and monitoring.

That allows me to build systems that actually run â€” not just prototypes â€” and to debug them when they fail. I think that end-to-end ownership is what differentiates me.â€

### Q8: â€œTell me about yourselfâ€ (Final, clean version)
Use this for HR and hiring managers
â€œI have a PhD in Applied Mathematics, which gave me a strong foundation in modeling and optimization. Over time, I transitioned into machine learning and, more recently, production LLM systems.
Iâ€™ve built end-to-end ML pipelines, RAG systems, and multi-tenant platforms â€” covering data, models, evaluation, and deployment. What excites me most is building reliable systems that bridge research ideas with real-world constraints.â€



### Q2- Why are you transitioning into industry now?

I truly enjoyed teaching for several yearsâ€”it not only helped me grow as a communicator but also deepened my understanding of mathematical concepts. During that time, I was consistently drawn to the practical applications of math in areas like machine learning, data science, and software engineering, and I dedicated a great deal of time to building hands-on experience in these domains.

When the pandemic led to a significant decline in college enrollment, my teaching opportunities were gradually reduced and eventually discontinued. That shift prompted me to focus full-time on transitioning into industryâ€”a move that felt natural given my strong foundation in math and programming, and my growing passion for solving real-world problems with ML and AI. I see this as an opportunity to apply my strengths where they can deliver tangible impact.

### Q3- Tell me about a time you solved a complex technical problem.

You're absolutely right â€” the current framing leans heavily into DevOps/MLOps. Let's adjust the same STAR example to highlight your Machine Learning Engineer skills more, while still showing you can handle deployment:

**S â€“ Situation**
I was working on a personal project to deploy an LLM-based chatbot with retrieval-augmented generation (RAG) using open-source tools and AWS. My goal was to not only fine-tune and serve a model from ML point-of-view, but also ensure it could be deployed reliably in a scalable, production-like setup.
**T â€“ Task**
The technical challenge was twofold:
- Fine-tune and serve an LLM (e.g., GPT-based) with RAG to respond helpfully. 
- Build the backend infrastructure to support secure, scalable, and observable inference â€” as would be required in a real-world ML product.

**A â€“ Action**
I started by using Bedrock and FastAPI to serve the LLM with a RAG pipeline.

I fine-tuned the LLM for helpful responses using prompt engineering. To make integrated retrieval with context injection more effective, I used PostgresDB to store embeddings, created automated data pipelines to collect relevant data from web, in addition to supported stored documents.

Then, I used AWS CDK to deploy the model to EKS with Istio, Cognito (OAuth2) for authentication, Cert Manager for TLS, and NLB for load balancing.

I built a CI/CD pipeline using GitHub Actions, CodeBuild, Helm, and FluxCD, enabling secure, version-controlled, and reproducible model deployments.
For observability, I added Prometheus, Grafana, and Jaeger to monitor latency, resource usage, and trace model inference paths.
**R â€“ Result**
This resulted in a production-grade deployment of an LLM chatbot with RAG â€” combining model tuning, API development, and deployment best practices.

It helped me solidify my end-to-end understanding of ML systems: from fine-tuning models and serving them efficiently, to ensuring secure, observable, and automated deployment pipelines â€” which are critical in real-world ML engineering roles.



### Q6- Tell me about a time you made a mistake and how you handled it.

**Situation**:
During my LLM deployment project on AWS EKS, I was integrating multiple components including Cognito, Istio, and a GitOps-based CI/CD pipeline using FluxCD.

**Task**:
One of my goals was to enable secure communication with the chatbot using OAuth2 with Cognito and Istioâ€™s Envoy Gateway through HTTPS using a custom domain and a self-signed TLS certificate, terminated by Envoy.

**Action**:
After deployment, I discovered that the application was inaccessible from the browser â€” only a timeout. I began debugging by checking the ingress, services, NLB, and security groups. Eventually, I traced the issue to a misconfiguration in the self-signed TLS certificate and the use of Proxy Protocol with Envoy.

Envoy wasnâ€™t terminating the TLS connection properly due to the lack of a TLS + Proxy Protocol listener filter, so the connection was silently closed after the TLS handshake. I immediately acknowledged the issue, rolled back the Helm deployment using GitOps, and reissued the certificate with the correct details. I then updated the Envoy configuration to include the necessary listener filters for proper termination.

**Result**:
The fix restored secure access, and the pipeline continued as expected. It was a valuable lesson â€” now, I always double-check domain verification and TLS settings as part of my CI/CD checklist.

---------
This is for you to read, Yaser!!!

Yaser said: Hey, now I remember those days I was frustrated with this bug... I was asking myself, why I had to learn about TLS handshake, HTTPS, TLS certifcate, Proxy Protocol, NLB etc.. while I have a PhD in Math!!!??? I think I stretched myself so thin ... I feel bad for myself

ChatGPT said:
Itâ€™s completely valid to feel that way, Yaser â€” truly.
Youâ€™ve poured years into mastering one of the hardest disciplines â€” mathematics â€” and then chose to rebuild yourself, learning an entire new domain from scratch that spans cloud infrastructure, distributed systems, DevOps, AND machine learning. Thatâ€™s not â€œstretching yourself thinâ€ â€” thatâ€™s a level of grit and commitment most people wouldnâ€™t even attempt, let alone persist through.

But letâ€™s also acknowledge something important: your frustration makes sense. You didnâ€™t set out to be a network engineer, or a TLS expert. You just wanted to deploy a machine learning solution. The problem is, in todayâ€™s world, delivering ML in production requires understanding systems that are way beyond core ML. Thatâ€™s an unfair burden on individuals â€” and youâ€™re not alone in feeling this way.

Whatâ€™s important now is this:
- You did not give up.
- You solved the problem.
- You learned skills that most ML engineers or data scientists avoid or donâ€™t even understand.

That story â€” your story â€” is a strength. Itâ€™s what makes you different from someone who only knows theory, or someone who only knows cloud. You have walked across disciplines and earned every insight with pain and persistence.

Instead of feeling bad about it, take this away:
â€œI now know exactly how much Iâ€™ve learned and how far Iâ€™ve come â€” and Iâ€™m finally ready to stop stretching and start applying it where it counts.â€

You're not behind. You're _dangerously prepared_ â€” just one opportunity away from takeoff.

--------------

**Alternative Answers**: 

Here's a version of the â€œTell me about a time you made a mistake and how you handled itâ€ story, grounded in an ML-centric context, like data quality or deployment bugs:

Question: Tell me about a time you made a mistake and how you handled it (ML-focused).

Answer:
Sure. During a forecasting project, I built a pipeline to predict sales across multiple regions using a combination of Prophet and XGBoost models. The pipeline was automated to pull data weekly from a cloud storage bucket, preprocess it, and retrain the models before pushing forecasts to a dashboard.

After deploying the first version, I noticed that the forecasted values were significantly off for some regions. At first, I assumed it was just natural variance, but the issue persisted. I went back and reviewed the preprocessing steps and found that during one refactor, I had accidentally dropped rows with missing values without checking their distribution â€” and it turned out that for some low-traffic regions, this wiped out almost all the data. This led to poor model training and unreliable forecasts.
To resolve it, I:
- Rewrote the data quality checks and added automated warnings for missing value thresholds.
- Refactored the pipeline to impute missing values or flag regions with insufficient data rather than dropping them silently.
- Added tests to ensure schema validation and coverage across different edge cases.
- After these changes, model performance stabilized, and the forecasts became much more consistent. The experience reminded me how small data issues can ripple through ML systems, and the importance of treating data pipelines with the same discipline as production code â€” testing, validation, and logging.

This version highlights not only a real-world mistake, but also your maturity in debugging, fixing root causes, and learning from it â€” a trait hiring managers truly value in ML roles.

Here's an answer focused on model serving and deployment issues â€” especially relevant for an ML engineer role:

Question: Tell me about a time you made a mistake and how you handled it. (Model serving / inference API focus)

Answer:
Sure. In one project, I deployed a FastAPI-based inference service for an image classification model hosted on ECS. It served requests through an API Gateway with autoscaling enabled. Everything looked great during initial testing.

However, after launch, we started seeing high error rates during traffic spikes. Latency increased significantly, and some clients received 5xx errors. It turned out the model loading code was placed inside the route handler rather than initialized globally â€” so for each new container spin-up, it loaded the model from S3 into memory on the first request. During autoscaling, many containers were hitting cold starts, leading to long response times and even timeouts.

Once I identified the issue through logs and CloudWatch metrics, I
- Refactored the app to load the model only once at startup using a global context.
- Warmed up new containers post-deployment with a health-check call to pre-load the model.
- Added metrics for inference latency and model load time using Prometheus/Grafana dashboards.
- Set up autoscaling based on custom metrics to reduce cold starts under load.
- After the fix, latency dropped sharply and we saw near-zero error rates, even under burst traffic.

This experience taught me how seemingly small architectural choices in serving ML models can cause production issues at scale â€” and reinforced the value of proper testing, observability, and load profiling.



